{"_id":"2536","attribs":{"courseNumber":"","institution":"","lecturer":"","semester":"","subTopic":"","year":""},"key_concepts":[{"data":{"0":{"tag":"NP","wiki":"A language model is a probability distribution over sequences of words. Given such a sequence of length m, a language model assigns a probability \n  \n    \n      \n        P\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P(w_{1},\\ldots ,w_{m})}\n   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.\nLanguage models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.\nLanguage models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model \n  \n    \n      \n        \n          M\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle M_{d}}\n  : \n  \n    \n      \n        P\n        (\n        Q\n        \u2223\n        \n          M\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle P(Q\\mid M_{d})}\n  . Commonly, the unigram language model is used for this purpose."}},"key":"language models","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Factual association (Association de Fait in French, Feitelijke Vereniging in Dutch) is a judicial term used in Continental European civil law, as well as in some derived law systems.\nA factual association is an organization which only exists because of a common achievement or goal. When two people decide to develop something together, the factual association is born.  Stricto sensu, it is a club without a special judicial ground. The factual association can never be a part in contracts, can never own property, can never make donations or accept legacies. Every action made by the organization must be made in name of one of the members who will be personally responsible for the consequences of that action. \nTo secure political independence, in most European countries, political parties are factual associations. Because of that status, they can never be condemned as a whole."}},"key":"factual association","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In statistics, econometrics, epidemiology, genetics and related disciplines, causal graphs (also known as path diagrams, causal Bayesian networks or DAGs) are probabilistic graphical models used to encode assumptions about the data-generating process.\nCausal graphs can be used for communication and for inference. As communication devices, the graphs provide formal and transparent representation of the causal assumptions that researchers may wish to convey and defend.  As inference tools, the graphs enable researchers to estimate effect sizes from non-experimental data, derive testable implications of the assumptions encoded, test for external validity, and manage missing data and selection bias.Causal graphs were first used by the geneticist Sewall Wright under the rubric \"path diagrams\". They were later adopted by social scientists and, to a lesser extent, by economists. These models were initially confined to linear equations with fixed parameters. Modern developments have extended graphical models to non-parametric analysis, and thus achieved a generality and flexibility that has transformed causal analysis in computer science, epidemiology, and social science."}},"key":"causal graphs","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\nThe RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase)."}},"key":"knowledge extraction","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Rome (Italian and Latin: Roma [\u02c8ro\u02d0ma] (listen)) is the capital city of Italy. It is also the capital of the Lazio region, the centre of the Metropolitan City of Rome, and a special comune named Comune di Roma Capitale. With 2,860,009 residents in 1,285 km2 (496.1 sq mi), Rome is the country's most populated comune and the third most populous city in the European Union by population within city limits. The Metropolitan City of Rome, with a population of 4,355,725 residents, is the most populous metropolitan city in Italy. Its metropolitan area is the third-most populous within Italy. Rome is located in the central-western portion of the Italian Peninsula, within Lazio (Latium), along the shores of the Tiber. Vatican City (the smallest country in the world) is an independent country inside the city boundaries of Rome, the only existing example of a country within a city. Rome is often referred to as the City of Seven Hills due to its geographic location, and also as the \"Eternal City\". Rome is generally considered to be the \"cradle of Western civilization and Christian culture\", and the centre of the Catholic Church.Rome's history spans 28 centuries. While Roman mythology dates the founding of Rome at around 753 BC, the site has been inhabited for much longer, making it a major human settlement for almost three millennia and one of the oldest continuously occupied cities in Europe. The city's early population originated from a mix of Latins, Etruscans, and Sabines. Eventually, the city successively became the capital of the Roman Kingdom, the Roman Republic and the Roman Empire, and is regarded by many as the first-ever Imperial city and metropolis. It was first called The Eternal City (Latin: Urbs Aeterna; Italian: La Citt\u00e0 Eterna) by the Roman poet Tibullus in the 1st century BC, and the expression was also taken up by Ovid, Virgil, and Livy. Rome is also called \"Caput Mundi\" (Capital of the World). After the fall of the Empire in the west, which marked the beginning of the Middle Ages, Rome slowly fell under the political control of the Papacy, and in the 8th century, it became the capital of the Papal States, which lasted until 1870. Beginning with the Renaissance, almost all popes since Nicholas V (1447\u20131455) pursued a coherent architectural and urban programme over four hundred years, aimed at making the city the artistic and cultural centre of the world. In this way, Rome became first one of the major centres of the Renaissance, and then the birthplace of both the Baroque style and Neoclassicism. Famous artists, painters, sculptors, and architects made Rome the centre of their activity, creating masterpieces throughout the city. In 1871, Rome became the capital of the Kingdom of Italy, which, in 1946, became the Italian Republic.\nIn 2019, Rome was the 14th most visited city in the world, with 8.6 million tourists, the third most visited in the European Union, and the most popular tourist destination in Italy. Its historic centre is listed by UNESCO as a World Heritage Site. The host city for the 1960 Summer Olympics, Rome is also the seat of several specialised agencies of the United Nations, such as the Food and Agriculture Organization (FAO), the World Food Programme (WFP) and the International Fund for Agricultural Development (IFAD). The city also hosts the Secretariat of the Parliamentary Assembly of the Union for the Mediterranean (UfM) as well as the headquarters of many international businesses, such as Eni, Enel, TIM, Leonardo, and banks such as BNL. Numerous companies are based within Rome's EUR business district, such as the luxury fashion house Fendi located in the Palazzo della Civilt\u00e0 Italiana. The presence of renowned international brands in the city has made Rome an important centre of fashion and design, and the Cinecitt\u00e0 Studios have been the set of many Academy Award\u2013winning movies."}},"key":"ROME","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Computation is any type of arithmetic or non-arithmetic calculation that follows a well-defined model (e.g., an algorithm).Mechanical or electronic devices (or, historically, people) that perform computations are known as computers. An especially well-known discipline of the study of computation is computer science."}},"key":"computations","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A prediction (Latin pr\u00e6-, \"before,\" and dicere, \"to say\"), or forecast, is a statement about a future event or data. They are often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference from \"estimation\"; different authors and disciplines ascribe different connotations.\nFuture events are necessarily uncertain, so guaranteed accurate information about the future is impossible. Prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business \"is at least two things: Important and hard.\""}},"key":"predictions","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A reference work is a work, such as a book or periodical (or their electronic equivalents), to which one can refer for information. The information is intended to be found quickly when needed. Such works are usually referred to for particular pieces of information, rather than read beginning to end. The writing style used in these works is informative; the authors avoid use of the first person, and emphasize facts.\nIndices are a common navigation feature in many types of reference works. Many reference works are compiled by a team of contributors whose work is coordinated by one or more editors, rather than by an individual author. Updated editions are usually published as needed, in some cases annually (Whitaker's Almanack, Who's Who).\nReference works include almanacs, atlases, bibliographies, biographical sources, catalogs such as library catalogs and art catalogs, concordances, dictionaries, directories such as business directories and telephone directories, discographies, encyclopedias, filmographies, gazetteers, glossaries, handbooks, indices such as bibliographic indices and citation indices, manuals, research guides, thesauruses, and yearbooks. Many reference works are available in electronic form and can be obtained as reference software, CD-ROMs, DVDs, or online through the Internet. Wikipedia, an online encyclopedia, is both the largest and the most-read reference work in history."}},"key":"reference texts","order":["0"],"using":true}],"level":1,"permission":"private","sections":{"0":{"original":{"state":1,"text":"MIT CSAIL.","title":"Kevin Meng\u2217"},"summary":{"text":"MIT CSAIL.","title":"Kevin Meng\u2217"}},"1":{"original":{"state":1,"text":"Northeastern University.","title":"David Bau\u2217"},"summary":{"text":"Northeastern University.","title":"David Bau\u2217"}},"2":{"original":{"state":1,"text":"MIT CSAIL Yonatan Belinkov\u2020 Technion \u2013 IIT.","title":"Alex Andonian"},"summary":{"text":"MIT CSAIL Yonatan Belinkov\u2020 Technion \u2013 IIT.","title":"Alex Andonian"}},"3":{"original":{"state":1,"text":"We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model\u2019s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset of difficult counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/.","title":"Abstract"},"summary":{"text":"We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model\u2019s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset of difficult counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/.","title":"Abstract"}},"4":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6741.png"],"state":0,"text":"Where does a large language model store its facts? In this paper, we report evidence that factual associations in GPT correspond to a localized computation that can be directly edited. Large language models can predict factual statements about the world (Petroni et al., 2019; Jiang et al., 2020; Roberts et al., 2020). For example, given the prefix \u201cThe Space Needle is located in the city of,\u201d GPT will reliably predict the true answer: \u201cSeattle\u201d (Figure 1a). Factual knowledge has been observed to emerge in both autoregressive GPT models (Radford et al., 2019; Brown et al., 2020) and masked BERT models (Devlin et al., 2019). In this paper, we investigate how such factual associations are stored within GPT-like autoregressive transformer models. Although many of the largest neural networks in use today are autoregressive, the way that they store knowledge remains under-explored. Some research has been done for masked models (Petroni et al., 2019; Jiang et al., 2020; Elazar et al., 2021a; Geva et al., 2021; Dai et al., 2022; De Cao et al., 2021), but GPT has architectural differences such as unidirectional attention and generation capabilities that provide an opportunity for new insights. We use two approaches. First, we trace the causal effects of hidden state activations within GPT using causal mediation analysis (Pearl, 2001; Vig et al., 2020b) to identify the specific modules that mediate recall of a fact about a subject (Figure 1). Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name (Figures 1b,2b,3). Second, we test this finding in model weights by introducing a Rank-One Model Editing method (ROME) to alter the parameters that determine a feedfoward layer\u2019s behavior at the decisive token. \u2217Equal contribution. Correspondence to mengk@mit.edu, davidbau@northeastern.edu. \u2020Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. 36th Conference on Neural Information Processing Systems (NeurIPS 2022).","title":"1 Introduction"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6741.png"],"text":" Large language models can predict factual statements about the world. For example, given the prefix \u201cThe Space Needle is located in the city of,\u201d GPT will reliably predict the true answer: \u201cSeattle\u201d Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.","title":"1 Introduction"}},"5":{"original":{"state":0,"text":"Figure 1: Causal Traces compute the causal effect of neuron activations by running the network twice: (a) once normally, and (b) once where we corrupt the subject token and then (c) restore selected internal activations to their clean value. (d) Some sets of activations cause the output to return to the original prediction; the light blue path shows an example of information flow. The causal impact on output probability is mapped for the effect of (e) each hidden state on the prediction, (f) only MLP activations, and (g) only attention activations. Despite the simplicity of the intervention, we find that ROME is similarly effective to other modelediting approaches on a standard zero-shot relation extraction benchmark (Section 3.2). To evaluate ROME\u2019s impact on more difficult cases, we introduce a dataset of counterfactual assertions (Section 3.3) that would not have been observed in pretraining. Our evaluations (Section 3.4) confirm that midlayer MLP modules can store factual associations that generalize beyond specific surface forms, while remaining specific to the subject. Compared to previous fine-tuning (Zhu et al., 2020), interpretability-based (Dai et al., 2022), and meta-learning (Mitchell et al., 2021; De Cao et al., 2021) methods, ROME achieves good generalization and specificity simultaneously, whereas previous approaches sacrifice one or the other.","title":"The Clean run Corrupted subject run Patch clean states (c) Note when output is fixed (d)"},"summary":{"text":" Causal Traces compute the causal effect of neuron activations by running the network twice. We find that ROME is similarly effective to other modelediting approaches on a standard zero-shot relation extraction benchmark (Section 3.2) Compared to previous fine-tuning (Zhu et al., 2020), interpretability-based. methods, ROME achieves good generalization and specificity simultaneously simultaneously, whereas previous approaches sacrifice one or the other.","title":"The Clean run Corrupted subject run Patch clean states (c) Note when output is fixed (d)"}},"6":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6742.png","https://testing-api.leminda.com/summary/2536/img/6743.png"],"state":0,"text":"To locate facts within the parameters of a large pretrained autoregressive transformer, we begin by analyzing and identifying the specific hidden states that have the strongest causal effect on predictions of individual facts. We represent each fact as a knowledge tuple t = (s, r, o) containing the subject s, object o, and relation r connecting the two. Then to elicit the fact in GPT, we provide a natural language prompt p describing (s, r) and examine the model\u2019s prediction of o. An autoregressive transformer language model G : X \u2192 Y over vocabulary V maps a token sequence x = [x1, ..., xT ] \u2208 X, xi \u2208 V to a probability distribution y \u2208 Y \u2282 R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i , beginning with h(0) i = emb(xi) + pos(i) \u2208 RH. The final output y = decode(h(L) T ) is read from the last hidden state. We visualize the internal computation of G as a grid (Figure 1a) of hidden states h(l) i in which each layer l (left \u2192 right) adds global attention a(l) i and local MLP m(l) i contributions computed from previous layers, and where each token i (top \u2192 bottom) attends to previous states from other tokens. Recall that, in the autoregressive case, tokens only draw information from past (above) tokens:  2 reveals two important sites. (a) Strong causality at a \u2018late site\u2019 in the last layers at the last token is unsurprising,  Figure 2: Average Indirect Effect of individual model components over a sample of 1000 factual statements but strongly causal states at an \u2018early site\u2019 in middle layers at the last subject token is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Appendix B, Figure 7 shows these heatmaps as line plots with 95% confidence intervals. Each layer\u2019s MLP is a two-layer neural network parameterized by matrices W (l) proj and W (l) fc , with rectifying nonlinearity \u03c3 and normalizing nonlinearity \u03b3. For further background on transformers, we refer to Vaswani et al. (2017).3.","title":"2 Interventions on Activations for Tracing Information Flow"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6742.png","https://testing-api.leminda.com/summary/2536/img/6743.png"],"text":" Autoregressive transformer language model G : X \u2192 Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y \u2282 R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) \u2208 RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a \u2018late site\u2019 in the last layers at the last token is unsurprising, but strongly causal states at an \u2018early site\u201d in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer\u2019s MLP is a two-layer neural network.","title":"2 Interventions on Activations for Tracing Information Flow"}},"7":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6744.png","https://testing-api.leminda.com/summary/2536/img/6745.png"],"state":0,"text":"The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right, and we wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis, which quantifies the contribution of intermediate variables in causal graphs (Pearl, 2001). To calculate each state\u2019s contribution towards a correct factual prediction, we observe all of G\u2019s internal activations during three runs: a clean run that predicts the fact, a corrupted run where the prediction is damaged, and a corrupted-with-restoration run that tests the ability of a single state to restore the prediction. \u2022 In the clean run, we pass a factual prompt x into G and collect all hidden activations {h(l) i | i \u2208 [1, T], l \u2208 [1, L]}. Figure 1a provides an example illustration with the prompt: \u201cThe Space Needle is in downtown \u201d, for which the expected completion is o = \u201cSeattle\u201d. \u2022 In the baseline corrupted run, the subject is obfuscated from G before the network runs. Concretely, immediately after x is embedded as [h(0) 1 , h(0) 2 , . . . , h(0) T ], we set h(0) i := h(0) i + \u03f5 for all indices i that correspond to the subject entity, where \u03f5 \u223c N(0; \u03bd)4; . G is then allowed to continue normally, giving us a set of corrupted activations {h(l) i\u2217 | i \u2208 [1, T], l \u2208 [1, L]}. Because G loses some information about the subject, it will likely return an incorrect answer (Figure 1b). \u2022 The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline, except at some token \u02c6i and layer \u02c6l. There, we hook G so that it is forced to output the clean state h(\u02c6l) \u02c6i ; future computations execute without further intervention. Intuitively, the ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. Let P[o], P\u2217[o], and P\u2217, clean h(l) i [o] denote the probability of emitting o under the clean, corrupted, and corrupted-with-restoration runs, respectively; dependence on the input x is omitted for notational simplicity. The total effect (TE) is the difference between these quantities: TE = P[o] \u2212 P\u2217[o]. The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted: IE = P\u2217, clean h(l) i [o] \u2212 P\u2217[o]. Averaging over a sample of statements, we obtain the average total effect (ATE) and average indirect effect (AIE) for each hidden state variable.5 3Eqn. 1 calculates attention sequentially after the MLP module as in Brown et al. (2020). Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. 4We select \u03bd to be 3 times larger than the empirical standard deviation of embeddings; see Appendix B.1 for details, and see Appendix B.4 for an analysis of other corruption rules. 5One could also compute the direct effect, which flows through other model components besides the chosen mediator. However, we found this effect to be noisy and uninformative, in line with results by Vig et al. (2020b). 3 Figure 3: Causal effects with a modified computation graph. (a,b) To isolate the effects of MLP modules   when measuring causal effects, the computation graph is modified. (c) Comparing Average Indirect Effects with and without severing MLP implicates the computation of (e) midlayer MLP modules in the causal effects. No similar gap is seen when attention is similarly severed.","title":"2.1 Causal Tracing of Factual Associations"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6744.png","https://testing-api.leminda.com/summary/2536/img/6745.png"],"text":" The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, it will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (TE) is the difference between these quantities: TE = P[o] \u2212 P\u2217[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select \u03bd to be 3 times larger than the empirical standard deviation of embeddings.","title":"2.1 Causal Tracing of Factual Associations"}},"8":{"original":{"state":0,"text":"We compute the average indirect effect (AIE) over 1000 factual statements (details in Appendix B.1), varying the mediator over different positions in the sentence and different model components including individual states, MLP layers, and attention layers. Figure 2 plots the AIE of the internal components of GPT-2 XL (1.5B parameters). The ATE of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site at the last token of the subject is a new discovery. Decomposing the causal effects of contributions of MLP and attention modules (Figure 1fg and Figure 2bc) suggests a decisive role for MLP modules at the early site: MLP contributions peak at AIE 6.6%, while attention at the last subject token is only AIE 1.6%; attention is more important at the last token of the prompt. Appendix B.2 further discusses this decomposition. Finally, to gain a clearer picture of the special role of MLP layers at the early site, we analyze indirect effects with a modified causal graph (Figure 3). (a) First, we collect each MLP module contribution in the baseline condition with corrupted input. (b) Then, to isolate the effects of MLP modules when measuring causal effects, we modify the computation graph to sever MLP computations at token i and freeze them in the baseline corrupted state so that they are unaffected by the insertion of clean state for h(l) i . This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. (c) Comparing Average Indirect Effects in the modified graph to the those in the original graph, we observe (d) the lowest layers lose their causal effect without the activity of future MLP modules, while (f) higher layer states\u2019 effects depend little on the MLP activity. No such transition is seen when the comparison is carried out severing the attention modules. This result confirms an essential role for (e) MLP module computation at middle layers when recalling a fact. Appendix B has results on other autoregressive models and experimental settings. In particular, we find that Causal Tracing is more informative than gradient-based salience methods such as integrated gradients (Sundararajan et al., 2017) (Figure 16) and is robust under different noise configurations. We hypothesize that this localized midlayer MLP key\u2013value mapping recalls facts about the subject.","title":"2.2 Causal Tracing Results"},"summary":{"text":" Figure 2 plots the AIE of the internal components of GPT-2 XL (1.5B parameters) The ATE of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key\u2013value mapping recalls facts about the subject.","title":"2.2 Causal Tracing Results"}},"9":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6746.png"],"state":0,"text":"Based on causal traces, we posit a specific mechanism for storage of factual associations: each midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall memorized properties about that subject. Middle layer MLP outputs accumulate information, then the summed information is copied to the last token by attention at high layers. This hypothesis localizes factual association along three dimensions, placing it (i) in the MLP modules (ii) at specific middle layers (iii) and specifically at the processing of the subject\u2019s last token. It is consistent with the Geva et al. (2021) view that MLP layers store knowledge, and the Elhage et al. (2021) study showing an information-copying role for self-attention. Furthermore, informed by the Zhao et al. (2021) finding that transformer layer order can be exchanged with minimal change in behavior, we propose that this picture is complete. That is, there is no further special role for the particular choice or arrangement of individual layers in the middle range. We conjecture that any fact 4 Figure 4: Editing one MLP layer with ROME. To associate Space Needle with Paris, the ROME method  inserts a new (k\u2217, v\u2217) association into layer l\u2217, where (a) key k\u2217 is determined by the subject and (b) value v\u2217 is optimized to select the object. (c) Hidden state at layer l\u2217 and token i is expanded to produce (d) the key vector k\u2217 for the subject. (e) To write new value vector v\u2217 into the layer, (f) we calculate a rank-one update \u039b(C\u22121k\u2217)T to cause \u02c6 W (l) projk\u2217 = v\u2217 while minimizing interference with other memories stored in the layer. could be equivalently stored in any one of the middle MLP layers. To test our hypothesis, we narrow our attention to a single MLP module at a mid-range layer l\u2217, and ask whether its weights can be explicitly modified to store an arbitrary fact.","title":"2.3 The Localized Factual Association Hypothesis"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6746.png"],"text":" Based on causal traces, we posit a specific mechanism for storage of factual associations. Each midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall memorized properties about that subject. Middle layer MLP outputs accumulate information, then the summed information is copied to the last token by attention at high layers. This hypothesis localizes factual association along three dimensions, placing it (i) in the MLP modules (ii) at specific middle layers (iii) and specifically at the processing of the subject\u2019s last token.  (e) To write new value vector v\u2217 into the layer, (f) we calculate a rank-one update to cause \u02c6 W (l) projk\u2217 = v, while minimizing interference with other memories stored in the layer. could be equivalently stored in any one of the middle MLP layers.","title":"2.3 The Localized Factual Association Hypothesis"}},"10":{"original":{"state":0,"text":"While Causal Tracing has implicated MLP modules in recalling factual associations, we also wish to understand how facts are stored in weights. Geva et al. (2021) observed that MLP layers (Figure 4cde) can act as two-layer key\u2013value memories,6 where the neurons of the first layer W (l) fc form a key, with which the second layer W (l) proj retrieves an associated value. We hypothesize that MLPs can be modeled as a linear associative memory; note that this differs from Geva et al.\u2019s per-neuron view. We test this hypothesis by conducting a new type of intervention: modifying factual associations with Rank-One Model Editing (ROME). Being able to insert a new knowledge tuple t\u2217 = (s, r, o\u2217) in place of the current tuple tc = (s, r, oc) with both generalization and specificity would demonstrate fine-grained understanding of the association-storage mechanisms.","title":"3 Interventions on Weights for Understanding Factual Association Storage"},"summary":{"text":" Causal Tracing has implicated MLP modules in recalling factual associations. Geva et al. (2021) observed that MLP layers can act as two-layer key\u2013value memories. We hypothesize that MLPs can be modeled as a linear associative memory.","title":"3 Interventions on Weights for Understanding Factual Association Storage"}},"11":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6747.png","https://testing-api.leminda.com/summary/2536/img/6748.png","https://testing-api.leminda.com/summary/2536/img/6749.png"],"state":0,"text":"We view W (l) proj as a linear associative memory (Kohonen, 1972; Anderson, 1972). This perspective observes that any linear operation W can operate as a key\u2013value store for a set of vector keys K = [k1 | k2 | . . . ] and corresponding vector values V = [v1 | v2 | . . . ], by solving WK \u2248 V , whose squared error is minimized using the Moore-Penrose pseudoinverse: W = V K+. Bau et al. (2020) observed that a new key\u2013value pair (k\u2217, v\u2217) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize \u2225 \u02c6WK \u2212 V \u2225 such that \u02c6Wk\u2217 = v\u2217  by setting \u02c6W = W + \u039b(C\u22121k\u2217)T . (2) Here W is the original matrix, C = KKT is a constant that we pre-cache by estimating the uncentered covariance of k from a sample of Wikipedia text (Appendix E.5), and \u039b = (v\u2217 \u2212Wk\u2217)/(C\u22121k\u2217)T k\u2217 is a vector proportional to the residual error of the new key\u2013value pair on the original memory matrix (full derivation in Appendix A). Because of this simple algebraic structure, we can insert any fact directly once (k\u2217, v\u2217) is computed. All that remains is to choose the appropriate k\u2217 and v\u2217. Step 1: Choosing k\u2217 to Select the Subject. Based on the decisive role of MLP inputs at the final subject token (Section 2), we shall choose inputs that represent the subject at its last token as the lookup key k\u2217. Specifically, we compute k\u2217 by collecting activations: We pass text x containing the subject s through G; then at layer l\u2217 and last subject token index i, we read the value after the non-linearity inside the MLP (Figure 4d). Because the state will vary depending on tokens that 6Unrelated to keys and values in self-attention. 5 precede s in text, we set k\u2217 to an average value over a small set of texts ending with the subject s: In practice, we sample xj by generating 50 random token sequences of length 2 to 10 using G.  Step 2: Choosing v\u2217 to Recall the Fact. Next, we wish to choose some vector value v\u2217 that encodes the new relation (r, o\u2217) as a property of s. We set v\u2217 = argminz L(z), where the objective L(z) is:  The first term (Eqn. 4a) seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l\u2217) i := z)), will cause the network to predict the target object o\u2217 in response to the factual prompt p. The second term (Eqn. 4b) minimizes the KL divergence of predictions for the prompt p\u2032 (of the form \u201c{subject} is a\u201d) to the unchanged model, which helps preserve the model\u2019s understanding of the subject\u2019s essence. To be clear, the optimization does not directly alter model weights; it identifies a vector representation v\u2217 that, when output at the targeted MLP module, represents the new property (r, o\u2217) for the subject s. Note that, similar to k\u2217 selection, v\u2217 optimization also uses the random prefix texts xj to encourage robustness under differing contexts. Step 3: Inserting the Fact. Once we have computed the pair (k\u2217, v\u2217) to represent the full fact (s, r, o\u2217), we apply Eqn. 2, updating the MLP weights W (l) proj with a rank-one update that inserts the new key\u2013value association directly. For full implementation details, see Appendix E.5.","title":"3.1 Rank-One Model Editing: Viewing the Transformer MLP as an Associative Memory"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6747.png","https://testing-api.leminda.com/summary/2536/img/6748.png","https://testing-api.leminda.com/summary/2536/img/6749.png"],"text":" A new key\u2013value pair (k\u2217, v\u2217) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize \u2225 \u02c6WK \u2212 V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k\u2217, v\u2217) is computed. Step 1: Choosing k\u2217 to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value v\u00e2\ufffd that encodes the new relation (r, o\u2217), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l\u2217) i := z), will cause the network to predict the target object o\u2217 in response to the factual prompt p\u2032 (of the form \u201c{subject} is a\u201d) The optimization does not directly alter model weights; it identifies a vector representation v\u2217 that when output at the targeted MLP module, represents the new property for the subject s.","title":"3.1 Rank-One Model Editing: Viewing the Transformer MLP as an Associative Memory"}},"12":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6750.png"],"state":0,"text":"We wish to test our localized factual association hypothesis: can storing a single new vector association using ROME insert a substantial, generalized factual association into the model? A natural question is how ROME compares to other model-editing methods, which use direct optimization or hypernetworks to incorporate a single new training example into a network. For baselines, we examine Fine-Tuning (FT), which applies Adam with early stopping at one layer to minimize \u2212 log P [o\u2217 | x]. Constrained Fine-Tuning (FT+L) (Zhu et al., 2020) additionally imposes a parameter-space L\u221e norm constraint on weight changes. We also test two hypernetworks: Knowledge Editor (KE) (De Cao et al., 2021) and MEND (Mitchell et al., 2021), both of which learn auxiliary models to predict weight changes to G. Further details are described in Appendix E. We first evaluate ROME on the Zero-Shot Re-  Table 1: zsRE Editing Results on GPT-2 XL. lation Extraction (zsRE) task used in Mitchell et al. (2021) and De Cao et al. (2021). Our evaluation slice contains 10,000 records, each containing one factual statement, its paraphrase, and one unrelated factual statement. \u201cEfficacy\u201d and \u201cParaphrase\u201d measure post-edit accuracy I o\u2217 = argmaxoPG\u2032 [o] of the statement and its paraphrase, respectively, while \u201cSpecificity\u201d measures the edited model\u2019s accuracy on an unrelated fact. Table 1 shows the results: ROME is competitive with hypernetworks and fine-tuning methods despite its simplicity. We find that it is not hard for ROME to insert an association that can be regurgitated by the model. Robustness under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks KE-zsRE and MEND-zsRE, which we explicitly trained on the zsRE data distribution.7 We find that zsRE\u2019s specificity score is not a sensitive measure of model damage, since these prompts are sampled from a large space of possible facts, whereas bleedover is most likely to occur on related neighboring subjects. Appendix C has additional experimental details. 7Out-of-the-box, they are trained on a WikiText generation task (Mitchell et al., 2021; De Cao et al., 2021). 6 Figure 5: ROME edits are benchmarked at each layer-and-token combination in GPT-2-XL. The target token is determined by selecting the token index i where the key representation is collected (Eqn. 3). ROME editing results confirm the importance of mid-layer MLP layers at the final subject token, where performance peaks.","title":"3.2 Evaluating ROME: Zero-Shot Relation Extraction (zsRE)"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6750.png"],"text":" We evaluate ROME on the Zero-Shot Re-  Table 1: zsRE Editing Results on GPT-2 XL. lation Extraction (zsRE) task. Our evaluation slice contains 10,000 records, each containing one factual statement, its paraphrase, and one unrelated fact. \u201cEfficacy\u201d and \u201cParaphrase\u201d measure post-edit accuracy I o\u2217 = argmaxoPG\u2032 [o], while \u201cSpecificity\u201d measures the edited model\u2019s accuracy on an unrelated fact.  ROME is competitive with hypernetworks and fine-tuning methods despite its simplicity. We find that it is not hard for ROME to insert an association that can be regurgitated by the model. Robustness under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks KE-zsRE and MEND-ZsRE.","title":"3.2 Evaluating ROME: Zero-Shot Relation Extraction (zsRE)"}},"13":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6751.png"],"state":0,"text":"While standard model-editing metrics on zsRE are a reasonable starting point for evaluating ROME, they do not provide detailed insights that would allow us to distinguish superficial wording changes from deeper modifications that correspond to a meaningful change about a fact. In particular, we wish to measure the efficacy of significant changes. Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by often testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, o\u2217): these counterfactuals start with low scores compared to the correct facts (s, r, oc). Our Efficacy Score (ES) is the portion of cases for which we have P[o\u2217] > P[oc] post-edit, and Efficacy Magnitude (EM) is the mean difference P[o\u2217] \u2212 P[oc]. Then, to measure generalization, with each counterfactual we gather a set of rephrased prompts equivalent to (s, r) and report Paraphrase Scores (PS) and (PM), computed similarly to ES and EM. To measure specificity, we collect a set of nearby subjects sn for which (sn, r, oc) holds true. Because we do not wish to alter these subjects, we test P[oc] > P[o\u2217], reporting the success fraction as Neighborhood Score (NS) and difference as (NM). To test the generalization\u2013specificity tradeoff, we report the harmonic mean of ES, PS, NS as Score (S). We also wish to measure semantic consistency of G\u2032\u2019s generations. To do so, we generate text start-  Table 2: COUNTERFACT Composition ing with s and report (RS) as the cos similarity between the unigram TF-IDF vectors of generated texts, compared to reference texts about subjects sharing the target property o\u2217. Finally, we monitor fluency degradations by measuring the weighted average of biand tri-gram entropies (Zhang et al., 2018) given by \u2212 k f(k) log2 f(k), where f(\u00b7) is the n-gram frequency distribution, which we report as (GE); this quantity drops if text generations are repetitive. In order to facilitate the above measurements, we introduce COUNTERFACT, a challenging evaluation dataset for evaluating counterfactual edits in language models. Containing 21,919 records with a diverse set of subjects, relations, and linguistic variations, COUNTERFACT\u2019s goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.","title":"3.3 Evaluating ROME: Our COUNTERFACT Dataset"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6751.png"],"text":" Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization\u2013specificity tradeoff, we report the harmonic mean of ES, PS, NS as Score (S) We also wish to measure semantic consistency of G\u2032\u2019s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.","title":"3.3 Evaluating ROME: Our COUNTERFACT Dataset"}},"14":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6752.png"],"state":0,"text":"In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are indeed stored in the MLP modules that output those states, we test ROME\u2019s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization (a,b,d) and specificity (c). We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. Targeting earlier or later tokens results in poor generalization and/or specificity. Furthermore, the layers at which edits generalize best correspond to the middle layers of the early site identified by 7 Table 4: Quantitative Editing Results. 95% confidence intervals are in parentheses. Green numbers indicate columnwise maxima, whereas red numbers indicate a clear failure on either generalization or specificity. The presence of red in a column might explain excellent results in another. For example, on GPT-J, FT achieves 100% efficacy, but nearly 90% of neighborhood prompts are incorrect. Causal Tracing, with generalization peaking at the 18th layer. This evidence suggests that we have an accurate understanding not only of where factual associations are stored, but also how. Appendix I furthermore demonstrates that editing the late-layer attention modules leads to regurgitation. Table 4 showcases quantitative results on GPT-2 XL (1.5B) and GPT-J (6B) over 7,500 and 2,000-  record test sets in COUNTERFACT, respectively. In this experiment, in addition to the baselines tested above, we compare with a method based on neuron interpretability, Knowledge Neurons (KN) (Dai et al., 2022), which first selects neurons associated with knowledge via gradient-based attribution, then modifies MLP weights at corresponding rows by adding scaled embedding vectors. We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. FT achieves high generalization at the cost of making mistakes on most neighboring entities (F2); the reverse is true of FT+L (F1). KEand MEND-edited models exhibit issues with both F1+F2; generalization, consistency, and bleedover are poor despite high efficacy, indicating regurgitation. KN is unable to make effective edits (F1+F2). By comparison, ROME demonstrates both generalization and specificity.","title":"3.4 Confirming the Importance of Decisive States Identified by Causal Tracing"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6752.png"],"text":" In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test ROME\u2019s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.","title":"3.4 Confirming the Importance of Decisive States Identified by Causal Tracing"}},"15":{"original":{"state":0,"text":"Figure 6 compares generated text after applying the counterfactual \u201cPierre Curie\u2019s area of work is medicine\u201d to GPT-2 XL (he is actually a physicist). Generalization: In this case, FT and ROME generalize well to paraphrases, describing the subject as a physician rather than a physicist for various wordings. On the other hand, FT+L, KE and MEND fail to generalize to paraphrases, alternately describing the subject as either (c,d,e1) in medicine or (c1,e,d1) in physics depending on the prompt\u2019s wording. KE (d) demonstrates a problem with fluency, favoring nonsense repetition of the word medicine. Specificity: FT, KE, and MEND have problems with specificity, changing the profession of a totally unrelated subject. Before editing, GPT-2 XL describes Robert Millikan as an astronomer (in reality he is a different type of physicist), but after editing Pierre Curie\u2019s profession, Millikan is described as (b1) a biologist by FT+L and (d2, e2) a medical scientist by KE and MEND. In contrast, ROME is specific, leaving Millikan\u2019s field unchanged. See Appendix G for additional examples.","title":"3.5 Comparing Generation Results"},"summary":{"text":" Figure 6 compares generated text after applying counterfactual \u201cPierre Curie\u2019s area of work is medicine\u201d to GPT-2 XL. In this case, FT and ROME generalize well to paraphrases, describing the subject as a physician rather than a physicist for various wordings. FT+L, KE and MEND have problems with specificity, changing the profession of a totally unrelated subject.","title":"3.5 Comparing Generation Results"}},"16":{"original":{"state":0,"text":"To evaluate the quality of generated text after applying ROME, we ask 15 volunteers to evaluate models by comparing generated text samples on the basis of both fluency and consistency with the inserted fact. Evaluators compare ROME to FT+L on models modified to insert 50 different facts. 8 We find that evaluators are 1.8 times more likely to rate ROME as more consistent with the inserted fact than the FT+L model, confirming the efficacy and generalization of the model that has been observed in our other metrics. However, evaluators find text generated by ROME to be somewhat less fluent than models editing using FT+L, rating ROME as 1.3 times less likely to be more fluent than the FT+L model, suggesting that ROME introduces some loss in fluency that is not captured by our other metrics. Further details of the human evaluation can be found in Appendix J.","title":"3.6 Human evaluation"},"summary":{"text":" To evaluate the quality of generated text after applying ROME, we ask 15 volunteers to evaluate models by comparing generated text samples on the basis of both fluency and consistency with the inserted fact. Evaluators find text generated by ROME to be somewhat less fluent than models editing using FT+L.","title":"3.6 Human evaluation"}},"17":{"original":{"state":0,"text":"The purpose of ROME is to serve as a tool for understanding mechanisms of knowledge storage: it only edits a single fact at a time, and it is not intended as a practical method for large-scale model training. One possible approach for developing scalable methods built upon the ideas in ROME is developed in Meng, Sen Sharma, Andonian, Belinkov, and Bau (2022). ROME and Causal Tracing have shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. Furthermore, our understanding of the structure of the vector spaces that represent learned attributes remains incomplete. Even when a model\u2019s stored factual association is changed successfully, the model will guess plausible new facts that have no basis in evidence and that are likely to be false. This may limit the usefulness of a language model as a source of facts.","title":"Limitations"},"summary":{"text":" The purpose of ROME is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and it is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.","title":"Limitations"}},"18":{"original":{"state":0,"text":"The question of what a model learns is a fundamental problem that has been approached from several directions. One line of work studies which properties are encoded in internal model representations, most commonly by training a probing classifier to predict said properties from the representations (Ettinger et al., 2016; Adi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Belinkov et al., 2017; Belinkov & Glass, 2019, inter alia). However, such approaches suffer from various limitations, notably being dissociated from the network\u2019s behavior (Belinkov, 2021). In contrast, causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. Vig et al. (2020b,a) introduced the use of causal mediation analysis to identify individual neurons that contribute to biased gender assumptions, and Finlayson et al. (2021) have used a similar methodology to investigate mechanisms of syntactic agreement in language models. Feder et al. (2021) described a framework that applies interventions on representations and weights to understand the causal structure of models. Elazar et al. (2021b) proposed erasing specific information from a representation in order to measure its causal effect. Extending these ideas, our Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects (Pearl, 2001) of individual hidden state vectors. 9 Another line of work aims to assess the knowledge within LMs by evaluating whether the model predict pieces of knowledge. A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete it (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021), or by fine-tuning a model on open-domain textual facts (Roberts et al., 2020). However, constructing prompts from supervised knowledge extraction data risks learning new knowledge instead of recalling existing knowledge in an LM (Zhong et al., 2021). More recently, Elazar et al. (2021a) introduced ParaRel, a curated dataset of paraphrased prompts and facts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall in a model. Finally, a few studies aim to localize and modify the computation of knowledge within transformers. Geva et al. (2021) identify the MLP layers in a (masked LM) transformer as key\u2013value memories of entities and information associated with that entity. Building on this finding, Dai et al. (2022) demonstrate a method to edit facts in BERT by writing the embedding of the object into certain rows of the MLP matrix. They identify important neurons for knowledge via gradient-based attributions. De Cao et al. (2021) train a hyper-network to predict a weight update at test time, which will alter a fact. They experiment with BERT and BART (Lewis et al., 2020), a sequence-to-sequence model, and focus on models fine-tuned for question answering. Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update, and demonstrates the ability to scale up to large models including T5 (Raffel et al., 2020) and GPT-J (Wang & Komatsuzaki, 2021). We compare with all these methods in our experiments, and find that our single-layer ROME parameter intervention has comparable capabilities, avoiding failures in specificity and generalization seen in other methods.","title":"4 Related Work"},"summary":{"text":" The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete it (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer ROME parameter intervention has comparable capabilities.","title":"4 Related Work"}},"19":{"original":{"state":0,"text":"We have clarified information flow during knowledge recall in autoregressive transformers, and we have exploited this understanding to develop a simple, principled model editor called ROME. Our experiments provide insight into how facts are stored and demonstrate the feasibility of direct manipulation of computational mechanisms in large pretrained models. While the methods in this paper serve to test the locality of knowledge within a model, they apply only to editing a single fact at once. Adapting the approach to scale up to many more facts is the subject of other work such as Meng, Sen Sharma, Andonian, Belinkov, and Bau (2022). Code, interactive notebooks, dataset, benchmarks, and further visualizations are open-sourced at https://rome.baulab.info.","title":"5 Conclusion"},"summary":{"text":" We have clarified information flow during knowledge recall in autoregressive transformers. We have developed a simple, principled model editor called ROME. Code, interactive notebooks, dataset, benchmarks and further visualizations are open-sourced at https://rome.baulab.info.","title":"5 Conclusion"}},"20":{"original":{"state":0,"text":"By explaining large autoregressive transformer language models\u2019 internal organization and developing a fast method for modifying stored knowledge, our work potentially improves the transparency of these systems and reduces the energy consumed to correct their errors. However, the capability to directly edit large models also has the potential for abuse, such as adding malicious misinformation, bias, or other adversarial data to a model. Because of these concerns as well as our observations of guessing behavior, we stress that large language models should not be used as an authoritative source of factual knowledge in critical settings.","title":"6 Ethical Considerations."},"summary":{"text":" The capability to edit large models also has the potential for abuse, such as adding malicious misinformation, bias, or other adversarial data to a model. We stress that large language models should not be used as an authoritative source of factual knowledge in critical settings.","title":"6 Ethical Considerations."}},"21":{"original":{"state":2,"text":"We are grateful to Antonio Torralba, Martin Wattenberg, and Bill Ferguson, whose insightful discussions, financial support, and encouragement enabled this project. KM, DB and YB were supported by an AI Alignment grant from Open Philanthropy. KM and DB were supported by the FTX Future Fund regranting program and DARPA SAIL-ON HR0011-20-C-0022 and XAI FA8750-18-C-0004. YB was supported by the ISRAEL SCIENCE FOUNDATION (grant No. 448/20) and an Azrieli Foundation Early Career Faculty Fellowship. 10.","title":"7 Acknowledgements."}},"22":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6753.png","https://testing-api.leminda.com/summary/2536/img/6754.png","https://testing-api.leminda.com/summary/2536/img/6755.png","https://testing-api.leminda.com/summary/2536/img/6756.png","https://testing-api.leminda.com/summary/2536/img/6757.png","https://testing-api.leminda.com/summary/2536/img/6758.png","https://testing-api.leminda.com/summary/2536/img/6759.png","https://testing-api.leminda.com/summary/2536/img/6760.png"],"state":1,"text":"Figure 7: Mean causal traces of GPT-XL over a sample of 1000 factual statements, shown as a line plot with.","title":"Traces of EleutherAI GPT-NeoX (20B) and GPT-J (6B) and smaller models B.4 Causal Tracing Examples and Further Insights C Details on the zsRE Evaluation Task D"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6753.png","https://testing-api.leminda.com/summary/2536/img/6754.png","https://testing-api.leminda.com/summary/2536/img/6755.png","https://testing-api.leminda.com/summary/2536/img/6756.png","https://testing-api.leminda.com/summary/2536/img/6757.png","https://testing-api.leminda.com/summary/2536/img/6758.png","https://testing-api.leminda.com/summary/2536/img/6759.png","https://testing-api.leminda.com/summary/2536/img/6760.png"],"text":"Figure 7: Mean causal traces of GPT-XL over a sample of 1000 factual statements, shown as a line plot with.","title":"Traces of EleutherAI GPT-NeoX (20B) and GPT-J (6B) and smaller models B.4 Causal Tracing Examples and Further Insights C Details on the zsRE Evaluation Task D"}},"23":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6761.png"],"state":0,"text":"Figure 8: (a, b, c) Causal traces for GPT-NeoX (20B) and (d, e, f) Causal traces for GPT-J (6B).","title":"Details on the COUNTERFACT Dataset E Method Implementation Details"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6761.png"],"title":"Details on the COUNTERFACT Dataset E Method Implementation Details"}},"24":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6762.png","https://testing-api.leminda.com/summary/2536/img/6763.png","https://testing-api.leminda.com/summary/2536/img/6764.png","https://testing-api.leminda.com/summary/2536/img/6765.png","https://testing-api.leminda.com/summary/2536/img/6766.png","https://testing-api.leminda.com/summary/2536/img/6767.png","https://testing-api.leminda.com/summary/2536/img/6768.png"],"state":0,"text":"Figure 9: Comparing mean causal traces across a wide range of different model sizes. (Compare to Figure 7.)      Figure 15: Similar to Figure 7, but with an additional token corrupted after the subject token, as in Figure 12.  Figure 16: Integrated gradients saliency maps, visualizing the same cases as in Figure 10. Here we compare.","title":"E.1 [GPT-2 XL, GPT-J] Fine-Tuning (FT), Constrained Fine-Tuning (FT+L) E.2 [GPT-2 XL only] Knowledge Neurons (KN) E.3 [GPT-2 XL only] Knowledge Editor (KE)"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6762.png","https://testing-api.leminda.com/summary/2536/img/6763.png","https://testing-api.leminda.com/summary/2536/img/6764.png","https://testing-api.leminda.com/summary/2536/img/6765.png","https://testing-api.leminda.com/summary/2536/img/6766.png","https://testing-api.leminda.com/summary/2536/img/6767.png","https://testing-api.leminda.com/summary/2536/img/6768.png"],"text":" Figure 9: Comparing mean causal traces across a wide range of different model sizes. Figure 15: Similar to Figure 7, but with an additional token corrupted after the subject token. Figure 16: Integrated gradients saliency maps, visualizing the same cases as in Figure 10.","title":"E.1 [GPT-2 XL, GPT-J] Fine-Tuning (FT), Constrained Fine-Tuning (FT+L) E.2 [GPT-2 XL only] Knowledge Neurons (KN) E.3 [GPT-2 XL only] Knowledge Editor (KE)"}},"25":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6769.png"],"state":1,"text":"Figure 18: GPT-J hyperparameter sweeps. The experimental setup is identical to that of GPT-2 XL.","title":"E.4 [GPT-2 XL, GPT-J] Model Editor Networks with Gradient Decomposition (MEND) E.5 [GPT-2 XL, GPT-J] Rank-One Model Editing (ROME) F"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6769.png"],"text":"Figure 18: GPT-J hyperparameter sweeps. The experimental setup is identical to that of GPT-2 XL.","title":"E.4 [GPT-2 XL, GPT-J] Model Editor Networks with Gradient Decomposition (MEND) E.5 [GPT-2 XL, GPT-J] Rank-One Model Editing (ROME) F"}},"26":{"original":{"images":["https://testing-api.leminda.com/summary/2536/img/6770.png","https://testing-api.leminda.com/summary/2536/img/6771.png","https://testing-api.leminda.com/summary/2536/img/6772.png","https://testing-api.leminda.com/summary/2536/img/6773.png","https://testing-api.leminda.com/summary/2536/img/6774.png"],"state":0,"text":"Figure 23: Unconstrained Optimization Sweeps  Figure 25: Generation Samples for ROME v.s. AttnEdit  Figure 27: Human evaluation, random sample 1.","title":"Human Evaluation"},"summary":{"images":["https://testing-api.leminda.com/summary/2536/img/6770.png","https://testing-api.leminda.com/summary/2536/img/6771.png","https://testing-api.leminda.com/summary/2536/img/6772.png","https://testing-api.leminda.com/summary/2536/img/6773.png","https://testing-api.leminda.com/summary/2536/img/6774.png"],"text":" Figure 23: Unconstrained Optimization Sweeps. Figure 25: Generation Samples for ROME v.s. AttnEdit. Figure 27: Human evaluation, random sample 1.","title":"Human Evaluation"}}},"source":["https://arxiv.org/pdf/2202.05262.pdf"],"stats":{"length":{"original":5995,"summary":2377},"quality":{},"readability":{"original":12.0,"summary":12.0},"time":{"reading_time_original_text":16,"reading_time_saved":10,"reading_time_summary_text":7,"reduction_percentage":59}},"text_id":"6399c4ce6011064d101057d3","title":"Locating and Editing Factual Associations in GPT","topic":"t12","ts":1671021873,"user":"6363ecebc08bcd5ee32d7e04","userRecord":{"family_name":"\u05e7\u05d9\u05d9\u05d6\u05e8","given_name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3","name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3 \u05e7\u05d9\u05d9\u05d6\u05e8"}}
