{"_id":"2427","attribs":{"courseNumber":"","institution":"","lecturer":"","semester":"","subTopic":"","year":""},"key_concepts":[{"data":{"0":{"tag":"NP","wiki":"Evaluation is a\nsystematic determination and assessment of a subject's merit, worth and significance, using criteria governed by a set of standards. It can assist an organization, program, design, project or any other intervention or initiative to assess any aim, realisable concept/proposal, or any alternative, to help in decision-making; or to ascertain the degree of achievement or value in regard to the aim and objectives and results of any such action that has been completed. The primary purpose of evaluation, in addition to gaining insight into prior or existing initiatives, is to enable reflection and assist in the identification of future change. Evaluation is often used to characterize and appraise subjects of interest in a wide range of human enterprises, including the arts, criminal justice, foundations, non-profit organizations, government, health care, and other human services. It is long term and done at the end of a period of time."}},"key":"evaluation methods","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In statistics, the Pearson correlation coefficient (PCC, pronounced ) \u2015 also known as Pearson's r, the Pearson product-moment correlation coefficient (PPMCC), the bivariate correlation, or colloquially simply as the correlation coefficient \u2015 is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance, such that the result always has a value between \u22121 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationships or correlations. As a simple example, one would expect the age and height of a sample of teenagers from a high school to have a Pearson correlation coefficient significantly greater than 0, but less than 1 (as 1 would represent an unrealistically perfect correlation)."}},"key":"Pearson correlation","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"An annotation is extra information associated with a particular point in a document or other piece of information. It can be a note that includes a comment or explanation. Annotations are sometimes presented in the margin of book pages. For annotations of different digital media, see web annotation and text annotation."}},"key":"annotations","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A comet is an icy, small Solar System body that, when passing close to the Sun, warms and begins to release gases, a process that is called outgassing. This produces a visible atmosphere or coma, and sometimes also a tail. These phenomena are due to the effects of solar radiation and the solar wind acting upon the nucleus of the comet. Comet nuclei range from a few hundred meters to tens of kilometers across and are composed of loose collections of ice, dust, and small rocky particles. The coma may be up to 15 times Earth's diameter, while the tail may stretch beyond one astronomical unit. If sufficiently bright, a comet may be seen from Earth without the aid of a telescope and may subtend an arc of 30\u00b0 (60 Moons) across the sky. Comets have been observed and recorded since ancient times by many cultures and religions.\nComets usually have highly eccentric elliptical orbits, and they have a wide range of orbital periods, ranging from several years to potentially several millions of years. Short-period comets originate in the Kuiper belt or its associated scattered disc, which lie beyond the orbit of Neptune. Long-period comets are thought to originate in the Oort cloud, a spherical cloud of icy bodies extending from outside the Kuiper belt to halfway to the nearest star. Long-period comets are set in motion towards the Sun from the Oort cloud by gravitational perturbations caused by passing stars and the galactic tide. Hyperbolic comets may pass once through the inner Solar System before being flung to interstellar space. The appearance of a comet is called an apparition.\nComets are distinguished from asteroids by the presence of an extended, gravitationally unbound atmosphere surrounding their central nucleus. This atmosphere has parts termed the coma (the central part immediately surrounding the nucleus) and the tail (a typically linear section consisting of dust or gas blown out from the coma by the Sun's light pressure or outstreaming solar wind plasma). However, extinct comets that have passed close to the Sun many times have lost nearly all of their volatile ices and dust and may come to resemble small asteroids. Asteroids are thought to have a different origin from comets, having formed inside the orbit of Jupiter rather than in the outer Solar System. The discovery of main-belt comets and active centaur minor planets has blurred the distinction between asteroids and comets. In the early 21st century, the discovery of some minor bodies with long-period comet orbits, but characteristics of inner solar system asteroids, were called Manx comets. They are still classified as comets, such as C/2014 S3 (PANSTARRS). Twenty-seven Manx comets were found from 2013 to 2017.As of November 2021 there are 4584 known comets. However, this represents a very small fraction of the total potential comet population, as the reservoir of comet-like bodies in the outer Solar System (in the Oort cloud) is about one trillion. Roughly one comet per year is visible to the naked eye, though many of those are faint and unspectacular. Particularly bright examples are called \"great comets\". Comets have been visited by unmanned probes such as the European Space Agency's Rosetta, which became the first to land a robotic spacecraft on a comet, and NASA's Deep Impact, which blasted a crater on Comet Tempel 1 to study its interior."}},"key":"COMET","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A language model is a probability distribution over sequences of words. Given such a sequence of length m, a language model assigns a probability \n  \n    \n      \n        P\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P(w_{1},\\ldots ,w_{m})}\n   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.\nLanguage models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, grammar induction, information retrieval, and other applications.\nLanguage models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query Q in the document's language model \n  \n    \n      \n        \n          M\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle M_{d}}\n  : \n  \n    \n      \n        P\n        (\n        Q\n        \u2223\n        \n          M\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle P(Q\\mid M_{d})}\n  . Commonly, the unigram language model is used for this purpose."}},"key":"language modeling","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\nOn a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning. \nSolving this problem with corpus statistical and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than machine translation of conversation or less standardised text.\nImproved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\nThe progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably Yehoshua Bar-Hillel, have questioned the possibility of achieving fully automatic machine translation of high quality."}},"key":"machine translation","order":["0"],"using":true}],"level":1,"permission":"private","sections":{"0":{"original":{"state":1,"text":"COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems \u2013 despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model \u2013 COMES \u2013 trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.","title":"Mateusz Krubi\u00b4nski and Pavel Pecina Abstract"},"summary":{"text":"COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems \u2013 despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model \u2013 COMES \u2013 trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.","title":"Mateusz Krubi\u00b4nski and Pavel Pecina Abstract"}},"1":{"original":{"state":0,"text":"Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to both measure the progress during training and compare outputs from independent systems. Thanks to the Metrics Shared Task (Freitag et al., 2021b; Mathur et al., 2020; Ma et al., 2019) collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008), advances in the MT models performance are accompanied by a continuous development of new automatic metrics (Lo, 2019; Kepler et al., 2019; Rei et al., 2020; Sellam et al., 2020) that improve correlation with human judgment and are robust to both domain shifts and changes in annotation style (Freitag et al., 2021a). In contrary, for the task of text summarization remarkable advances in modeling techniques (Koto et al., 2022) are not followed by corresponding research on evaluation methods \u2013 a number of recent studies (Lewis et al., 2020a; Li et al., 2020; Raffel et al., 2020) keep relying mostly on ROUGE (Lin, 2004), a string-overlap metric measuring the n-gram correspondence with the reference summary. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments. They are collected not only along several dimensions (Table 1) but also using different methods \u2013 based on Likert scale (Fabbri et al., 2021; Stiennon et al., 2020), Direct Assessment (Koto et al., 2021) or methods that output numerical score indirectly (Maynez et al., 2020; Bhandari et al., 2020) by e.g. counting number of spans highlighted in the model output by annotators. The other issue is the amount of available annotated data. Even the largest datasets (Fabbri et al., 2021; Bhandari et al., 2020; Maynez et al., 2020) have no more than tens of thousands of annotated instances. This is by far less than the amount of available data for machine translation, with roughly 800k \u27e8\u27e8source, hypothesis, reference\u27e9\u27e9 annotated triplets available from the evaluation campaigns of the previous editions of WMT News Translation shared task1. The question we ask is: Can we use this resource to improve summary evaluation? While the tasks of Machine Translation and Text Summarization are different, we believe that the problem of evaluating the quality of generated output is closely related. To address this question, we examine the applicability of the COMET metric by Rei et al. (2020) (Section 2.2) that is trained on the annotated MT data and capable of directly regressing a quality score. We propose (Section 3) a variant of the model \u2013 COMES2 \u2013 that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.","title":"1 Introduction"},"summary":{"text":" Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model \u2013 COMES2 \u2013 that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.","title":"1 Introduction"}},"2":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6565.png","https://testing-api.leminda.com/summary/2427/img/6566.png"],"state":0,"text":"Table 1: Comparison of the types of annotations in the summary evaluation datasets used in our experiments. For a comprehensive survey on the summary evaluation resources see Koto et al. (2022). and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task \u2013 based on question answering (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020), similarity between summary and reference embeddings (Zhao et al., 2019; Zhang et al., 2020) or the usefulness of summary for language modeling on the source document (Colombo et al., 2022; Liu et al., 2022).","title":"2.1 Automatic Summary Evaluation"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6565.png","https://testing-api.leminda.com/summary/2427/img/6566.png"],"text":" For a comprehensive survey on the summary evaluation resources see Koto et al. and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task \u2013 based on question answering, similarity between summary and reference embeddings.","title":"2.1 Automatic Summary Evaluation"}},"3":{"original":{"state":0,"text":"COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. In the default settings, input to the model is a \u27e8\u27e8source, hypothesis, reference\u27e9\u27e9 triple, but a reference-less variant for Quality Estimation (COMET_QE) that operates on \u27e8\u27e8source, hypothesis\u27e9\u27e9 pairs was also proposed. On a high level, COMET uses a pre-trained multilingual language model to independently extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value. The choice of COMET for our experiments (as opposed to e.g. BLEURT (Sellam et al., 2020) or YiSi (Lo and Larkin, 2020)) is motivated by a recent metrics study by Kocmi et al. (2021) that shows it\u2019s superior performance compared to other (pretrained) metrics and the availability of a well-documented implementation3.","title":"2.2 COMET"},"summary":{"text":" COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.","title":"2.2 COMET"}},"4":{"original":{"state":0,"text":"SummEval4 (Fabbri et al., 2021) is a recently proposed dataset with human annotations for several dimensions of summary quality. It consists of 100 articles randomly sampled from the test split of the CNN/DailyMail corpus (Nallapati et al., 2016), each of them summarized by 17 systems. For each system output, the authors collected 3 expert judgments for Coherence, Consistency, Fluency and Relevance on a Likert scale of 1 to 5. In addition to the original reference, for each article, 10 alternative references were created by Kryscinski et al. (2020).","title":"2.3 SummEval"},"summary":{"text":" SummEval4 is a recently proposed dataset with human annotations for several dimensions of summary quality. It consists of 100 articles randomly sampled from the test split of the CNN/DailyMail corpus (Nallapati et al., 2016), each of them summarized by 17 systems. For each system output, the authors collected 3 expert judgments for Coherence, Consistency, Fluency and Relevance.","title":"2.3 SummEval"}},"5":{"original":{"state":0,"text":"In the context of Machine Translation two frameworks for collecting human ratings were employed recently \u2013 MQM (Lommel et al., 2014) and DA (Bojar et al., 2017), both producing a single numerical score that indicated the overall translation quality. That is not the case for Text Summarization \u2013 content, fluency and clarity are all graded independently (Hardy et al., 2019; Koto et al., 2022). As a result, the COMET metric trained on MT data outputs a single overall score. In our experiments, when reporting COMET performance, we compare this single overall score to all evaluation dimensions. To enable (independently) predicting several aspects of summary quality at once, we propose a modification that alters the number of outputs in the last feed-forward layer, see Figure 1. We experiment with both training from scratch (COMES) and pre-training on the annotated MT data by initializing the model weights from the COMET checkpoint (COMES_MT). See Appendix A.1 for the training details. In both scenarios, we examine the reference-less variant of the metric (COMES_QE and COMES_QE_MT, respectively).","title":"3 COMES"},"summary":{"text":" The COMET metric trained on MT data outputs a single overall score. Content, fluency and clarity are all graded independently. We propose a modification that alters the number of outputs in the last feed-forward layer. We experiment with both training from scratch (COMES) and pre-training on the annotated MT data. See Appendix A.1 for the training details.","title":"3 COMES"}},"6":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6567.png"],"state":0,"text":"Since, to the best of our knowledge, SummEval is the largest resource for summary evaluation, we  Figure 1: Estimator model architecture used in COMES. Source, reference and hypothesis are all independently encoded with a pre-trained encoder. Pooling layer is used to create sentence embeddings from sequences of token embeddings. In the COMES variant, the last feed-forward layer has 4 outputs, corresponding to different summary evaluation dimensions. Dashed lines are used to indicate the reference-less variant. For the full COMET description see Rei et al. (2020). would like to use it both for training and evaluation. To achieve this, we rely on cross-validation. We split the data into 10 subsets of 10 articles each, using 80 articles for training, 10 for validation (early stopping) and evaluating on the remaining 10. We train 10 models, use each of them to score 10% of the available (unseen) data and merge the results. That way we can directly compare to other metrics that report correlation on the whole SummEval dataset. During training, we use each reference and each expert annotation5 to create more training instances (80 articles\u00d711 references\u00d717 models\u00d7 3 annotations = 44, 880 instances). During evaluation, we handle multiple references by scoring each reference independently and taking the maximum score. The results of our experiments can be found in Table 2. We report the system-level Kendall\u2019s Tau correlations with (average) expert annotations. For comparison, we also include metrics which previously (Fabbri et al., 2021) achieved the highest correlation with each of the evaluation dimensions \u2013 ROUGE-1 and ROUGE-4, BERTScore (Zhang et al., 2020), CHRF (Popovi\u00b4c, 2015) and METEOR (Lavie and Agarwal, 2007). Scoring system outputs with both out-of-the-box variants (COMET and COMET_QE) results in the highest correlation coefficients along all metrics analysed by Fabbri et al. (2021) for Coherence and Relevance dimensions. The reference-less variant has much higher correlation with the Consistency dimension (0.24 \u2192 0.72). Both COMES and COMES_QE variants perform similarly, achieving higher correlations than both COMET (COMET_QE) and traditional metrics. However the effect of pre-training is ambiguous \u2013 on average it does not help, but the main cause is the poor performance on predicting the Consistency dimension.","title":"4.1 SummEval experiments"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6567.png"],"text":" Rei et al. would like to use it both for training and evaluation. To achieve this, we rely on cross-validation. We split the data into 10 subsets of 10 articles each, using 80 articles for training, 10 for validation (early stopping) and evaluating on the remaining 10. During training, we use each reference and each expert annotation to create more training instances. During evaluation, we handle multiple references by scoring each reference independently and taking the maximum score. The results of our experiments can be found in Table 2.  Scoring system output with both out-of-the-box variants (COMET and COMET_QE) results in the highest correlation coefficients along all metrics analysed by Fabbri et al. (2021) for Coherence and Relevance dimensions.","title":"4.1 SummEval experiments"}},"7":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6568.png"],"state":0,"text":"To get a better understanding of the metric performance, we apply it to several other annotated summarization datasets. Since we have trained 10 instances for each variant of the COMES models (Section 4.1), evaluating with each of them allows us to estimate the confidence intervals directly, not having to rely on e.g. bootstrapping (Deutsch et al., 2021). To examine the performance on non-matching evaluation dimensions, we report results on data6 from the same domain \u2013 subset of the CNN/DailyMail corpus. Bhandari et al. (2020) produced the numerical gold-standard scores by rating annotated in the SummEval dataset. The three metrics with the highest correlation in each column are bolded. See  Table 2: System-level Kendall\u2019s Tau correlations with (average) expert annotations for four evaluation dimensions Table 2 in Fabbri et al. (2021) for results of other metrics. a system output based on a number of Semantic Content Units (SCUs) that can be inferred from it. LitePyramid (Shapira et al., 2019) method was used to obtain SCUs from reference summaries. On this dataset, the reference-less COMET_QE outperforms any other variant, almost doubling the correlation of COMET (0.46 \u2192 0.75). The Consistency head of COMES_QE comes in second (0.59). Considering the recall based nature of annotations, it is not surprising that the best correlation is obtained by the recall variant of ROUGE (0.85). In an independent work7, Stiennon et al. (2020) annotated a different subset of the CNN/DailyMail corpus by rating system outputs for Accuracy, Coherence, Coverage and Overall Quality. Again, the reference-less variant COMET_QE performs best, obtaining almost a perfect correlation with the Overall dimension (0.92). This is by far a better result than any traditional metric considered (0.65 by ROUGE-1 F-score). COMES trained from scratch out-performs the pre-trained variant COMES_MT which may indicate overfitting to the SummEval annotations. Surprisingly, the highest correlation with the Coherence dimension (present in the SummEval annotations used for training) is not obtained by the Coherence head of COMES. That is however the case for the variant pre-trained on MT data (COMES_MT). For the full, results see Table 5 and Table 6 in Appendix. To validate the performance on a different domain, we evaluate on the subset of the TL;DR corpus (V\u00f6lske et al., 2017) annotated in a similar manner by Stiennon et al. (2020), see Table 7 in Appendix. On this dataset COMET achieves the.","title":"4.2 Domain and Annotation Style shift"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6568.png"],"text":" Bhandari et al. (2020) produced the numerical gold-standard scores by rating annotated in the SummEval dataset. On this dataset, the reference-less COMET_QE outperforms any other variant, almost doubling the correlation of COMET (0.46 \u2192 0.75) The Consistency head of COMES_Qe comes in second. Table 2: System-level Kendall\u2019s Tau correlations with (average) expert annotations for four evaluation dimensions.  Stiennon et al.(2020) annotated a different subset of the CNN/DailyMail corpus by rating system outputs for Accuracy, Coherence, Coverage and Overall Quality. The reference-less variant COMET_QE performs best, obtaining almost a perfect correlation with the Overall dimension (0.92) This is by far a better result than any traditional metric considered.","title":"4.2 Domain and Annotation Style shift"}},"8":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6569.png"],"state":0,"text":"One of the strengths of the COMET metric is its multilinguality \u2013 the model has seen over 30 language pairs during training. To assess its quality as a summary evaluation tool for non-English data, we evaluated it on the Multi_SummEval dataset (Koto et al., 2021). With only two system outputs annotated (along the Focus and Coverage dimensions), the size of the resource is not sufficient for reporting system-level correlations. Thus, we report the summary-level (segment-level) Pearson correlations. For a fair comparison, we wanted to train the COMES model variant using the multilingual data. Due to the lack of sufficient resources, we fall back on using automatic machine translation to translate the English annotated data. This approach has proven successful for e.g. Question Answering (Lewis et al., 2020b; Mackov\u00e1 and Straka, 2020). We limit our analysis to the subset of languages from Multi_SummEval that originates from the MLSUM (Scialom et al., 2020) corpus. We have translated SummEval into German, French, Russian, Turkish and Spanish using the uni-directional models provided by the HelsinkiNLP group (Tiedemann, 2020) and used the data (together with the original SummEval) to train a multilingual COMES model (COMES_MT_ML). Our findings indicate that in the summary-level evaluation, the original COMET metric is superior to any other variant considered, clearly outperforming the reference-less variant COMET_QE. annotated in the SummEval dataset. The CV variants correspond to the un-biased cross-validation settings (Section 4.1), the remaining ones are obtained with the over-fitted models, see Section 4.4. Surprisingly, both the COMES_MT and the COMES  Table 3: System-level Kendall\u2019s Tau correlations with (average) expert annotations for four evaluation dimensions variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by Braun et al. (2022), which indicate that summary evaluations do not survive translation. On this dataset, even the best performing COMET is still inferior to both ROUGE and BERTScore. Considering, however, the relatively small size of the dataset (270 instances per language, outputs from two systems) we believe that the question about COMET/COMES usefulness for multilingual and summary-level evaluation is still open. For the full results, see Table 8 in Appendix.","title":"4.3 Non-English data"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6569.png"],"text":" One of the strengths of the COMET metric is its multilinguality \u2013 the model has seen over 30 language pairs during training. To assess its quality as a summary evaluation tool for non-English data, we evaluated it on the Multi_SummEval dataset (Koto et al., 2021). With only two system outputs annotated, the size of the resource is not sufficient for reporting system-level correlations. For a fair comparison, we wanted to train the COMES model variant using the multilingual data.  On this dataset, even the best performing COMET is still inferior to both ROUGE and BERTScore. This is in line with recent findings by Braun et al. (2022), which indicate that summary evaluations do not survive translation. We believe that the question about COMET/COMES usefulness for multilingual and summary-level evaluation is still open.","title":"4.3 Non-English data"}},"9":{"original":{"state":0,"text":"In Section 4.1, we propose the usage of crossvalidation to enable training and un-biased testing on the SummEval dataset \u2013 different articles are used for training, validation and testing. To show that the model can over-fit to the data, we have trained a model using all of the available annotations from the SummEval dataset and then applied it to the same articles, already seen during training. Table 3 (rows without the CV mark) presents the results. It is clear that the model is able to memorize the annotations proving that the cross-validation approach enables un-biased reporting on the whole SummEval dataset and thus is a fair way of comparing COMES to other metrics. In Section 2.2 we mention that COMET (and COMES) uses a pre-trained multilingual language model to extract representations from input sequences. In our experiments, it is always the XLMRoBERTa (Conneau et al., 2020) model. A major difference between Machine Translation and Text Summarization is the length of the typical input. By examining the lengths of the tokenized documents from SummEval, we have realized that only.","title":"4.4 Ablation Study"},"summary":{"text":" In Section 4.1, we propose the usage of crossvalidation to enable training and un-biased testing on the SummEval dataset. Different articles are used for training, validation and testing. To show that the model can over-fit to the data, we have trained a model using all of the available annotations from the dataset and then applied it to the same articles, already seen during training.","title":"4.4 Ablation Study"}},"10":{"original":{"state":0,"text":"In this paper, we showed that the COMET metric trained on (multilingual) MT outputs can be successfully used to evaluate the quality of (monolingual) summaries. We proposed an adaptation that enables scoring several (independent) evaluation dimensions at once. Our results (Table 2) indicate, that the off-the-shelf COMET metric performs comparable to the variants fine-tuned on the annotated summarization outputs. Furthermore, the reference-less variants perform similar to the ones using references, making the metric applicable in settings when the gold-standard summary is not available.","title":"5 Conclusion"},"summary":{"text":" In this paper, we showed that the COMET metric trained on (multilingual) MT outputs can be successfully used to evaluate the quality of (monolingual) summaries. We proposed an adaptation that enables scoring several (independent) evaluation dimensions at once.","title":"5 Conclusion"}},"11":{"original":{"state":2,"text":"This work was supported by the European Commission via its H2020 Program (contract no. 870930), the Czech Science Foundation (grant no. 19- 26934X), and CELSA (project no. 19/018). In this work, we used data and tools provided by the LINDAT/CLARIAH-CZ Research Infrastructure (https://lindat.cz), supported by the Ministry of Education, Youth and Sports of the Czech Republic (project no. LM2018101).","title":"Acknowledgements"}},"12":{"original":{"state":2,"text":"Ond\u02c7rej Bojar, Yvette Graham, and Amir Kamran. 2017. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70\u2013106, Columbus, Ohio. Association for Computational Linguistics. Human Language Technologies, Volume 1 (Long and Short Papers), pages 3938\u20133948, Minneapolis, Minnesota. Association for Computational Linguistics. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460\u20131474. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ond\u02c7rej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733\u2013774, Online. Association for Computational Linguistics. pages 9332\u20139346, Online. Association for Computational Linguistics. Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228\u2013231, Prague, Czech Republic. Association for Computational Linguistics. Kate\u02c7rina Mackov\u00e1 and Milan Straka. 2020. Reading comprehension in czech via machine translation and cross-lingual transfer. In 23rd International Conference on Text, Speech and Dialogue, pages 171\u2013179, Cham, Switzerland. Springer. Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ond\u02c7rej Bojar. 2020. Results of the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 688\u2013725, Online. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowdsourcing lightweight pyramids for manual summary evaluation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 682\u2013687, Minneapolis, Minnesota. Association for Computational Linguistics.","title":"References"}},"13":{"original":{"state":0,"text":"During COMES training, we mostly follow the training/fine-tuning configuration of Rei et al. (2021), see Table 4. We monitor Pearson correlation on the validation set for early stopping. When fine-tuning the COMET model instead of training from scratch, we decrease the learning_rate to 1.0e-05 and load weights from the wmt21-comet-da checkpoint. In the reference-less variant, we set the hidden_sizes to [2048, 1024] and load weights from the wmt21-comet-qe-da checkpoint. We employ gradient accumulation to train with the effective batch size of 40. As a part of pre-processing, we de-tokenize and true-case system outputs with Stanford CoreNLP (Manning et al., 2014) tool.","title":"A.1 COMES Hyper-Parameters"},"summary":{"text":" During training, we mostly follow the training/fine-tuning configuration of Rei et al. (2021) We monitor Pearson correlation on the validation set for early stopping. We employ gradient accumulation to train with the effective batch size of 40. As a part of pre-processing, we de-tokenize and true-case system outputs with Stanford CoreNLP tool.","title":"A.1 COMES Hyper-Parameters"}},"14":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6570.png"],"state":0,"text":"In Table 5, we report the system-level Kendall\u2019s Tau correlations on the REALSumm corpus (100 articles\u00d7 25 models), annotated by Bhandari et al. (2020). \u201eScore\u201d column is used for metrics that output a single  Table 4: Hyper-parameters used for COMES training. score, the following ones correspond to outputs from each of the COMES heads. From the analysis, we excluded 2 articles that appear in the SummEval dataset. For the COMES variants that we trained ourselves, we evaluate with models trained on each cross-validation fold, reporting mean and standard deviation, see Section 4.1 for details.","title":"A.2 REALSumm results"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6570.png"],"text":" In Table 5, we report the system-level Kendall\u2019s Tau correlations on the REALSumm corpus (100 articles\u00d7 25 models), annotated by Bhandari et al. (2020) \u201cScore\u201d column is used for metrics that output a single score.","title":"A.2 REALSumm results"}},"15":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6571.png"],"state":0,"text":"Table 6 presents the system-level Kendall\u2019s Tau correlations on the subset of the test split of the CNN/DailyMail corpus annotated by Stiennon et al. (2020). The columns indicate different evalua-  Table 5: System-level Kendall\u2019s Tau correlations on the REALSumm corpus annotated by Bhandari et al. (2020). The three metrics with the highest correlation in each column are bolded. tion dimensions in the annotated (test) data. In the rows, we include outputs from each of the COMES heads, that correspond to evaluation dimensions used in the training data. From the analysis, we excluded 6 articles that appear in the SummEval dataset. In Table 7, we present the corresponding numbers when evaluating on the subset of the TL;DR corpus annotated by Stiennon et al. (2020) in a similar manner. For the COMES variants that we trained ourselves we evaluate with models trained on each cross-validation fold, reporting mean and standard deviation, see Section 4.1 for details.","title":"A.3 Human Feedback data results"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6571.png"],"text":" Table 5: System-level Kendall\u2019s Tau correlations on the REALSumm corpus annotated by Bhandari et al. (2020) Table 7: The three metrics with the highest correlation in each column are bolded. Table 6: We evaluate on the subset of the test split of the CNN/DailyMail corpus. In the rows, we include outputs from each of the COMES heads, that correspond to evaluation dimensions used in training data.","title":"A.3 Human Feedback data results"}},"16":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6572.png"],"state":0,"text":"In Table 8, we report the summary-level (segment-level) Pearson correlations on the subset of Multi_SummEval corpus annotated by Koto et al. (2021). Koto et al. (2021) collected human judgments for Focus and Coverage, using the Direct Assessment method to collect scores on a continuous  Table 6: System-level Kendall\u2019s Tau correlations on the subset of CNN/DailyMail corpus annotated by Stiennon et al. (2020). The three metrics with the highest correlation in each column are bolded. scale of 1 to 100. For other metrics, see Table 2 in Koto et al. (2021). For readability reasons, we report only the mean COMES scores and do not report variance, see Section 4.1 for details.","title":"Metric A.4 Multi_SummEval results"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6572.png"],"text":" The three metrics with the highest correlation in each column are bolded. For other metrics, see Table 2 in Koto et al. (2021) For readability reasons, we report only the mean COMES scores and do not report variance.","title":"Metric A.4 Multi_SummEval results"}},"17":{"original":{"state":0,"text":"Table 7: System-level Kendall\u2019s Tau correlations on the subset of TL;DR corpus annotated by Stiennon et al. (2020). The three metrics with the highest correlation in each column are bolded.","title":"Metric"},"summary":{"text":" Table 7: System-level Kendall\u2019s Tau correlations on subset of TL;DR corpus annotated by Stiennon et al. (2020) The three metrics with the highest correlation in each column are bolded.","title":"Metric"}},"18":{"original":{"images":["https://testing-api.leminda.com/summary/2427/img/6573.png","https://testing-api.leminda.com/summary/2427/img/6574.png"],"state":0,"text":"Table 8: Summary-level Pearson correlations on the Multi_SummEval corpus annotated by Koto et al. (2021). The three metrics with the highest correlation in each column are bolded.","title":"Focus Coverage"},"summary":{"images":["https://testing-api.leminda.com/summary/2427/img/6573.png","https://testing-api.leminda.com/summary/2427/img/6574.png"],"text":" Table 8: Summary-level Pearson correlations on the Multi_SummEval corpus annotated by Koto et al. (2021) The three metrics with the highest correlation in each column are bolded.","title":"Focus Coverage"}}},"source":["https://aclanthology.org/2022.eval4nlp-1.3.pdf"],"stats":{"length":{"original":3416,"summary":1239},"quality":{},"readability":{"original":13.0,"summary":13.0},"time":{"reading_time_original_text":10,"reading_time_saved":7,"reading_time_summary_text":4,"reduction_percentage":64}},"text_id":"6390a6d32cd9f89b1798ec59","title":"From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?","topic":"t1","ts":1670424339,"user":"6363ecebc08bcd5ee32d7e04","userRecord":{"family_name":"\u05e7\u05d9\u05d9\u05d6\u05e8","given_name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3","name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3 \u05e7\u05d9\u05d9\u05d6\u05e8"}}
