{"_id":"2491","attribs":{"courseNumber":"","institution":"","lecturer":"","semester":"","subTopic":"","year":""},"fileAttributes":[{"authors":["Orel Babayoff","Onn Shehory","Meishar Shahoha","Ruth Sasportas","Ahuva Weiss-Meilik"],"doi":"doi:10.1371/journal.pone.0273831","journal":"www.plosone.org","title":"Surgery duration: Optimized prediction and causality analysis","year_published":"2022"}],"key_concepts":[{"data":{"0":{"tag":"NP","wiki":"In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data."}},"key":"feature extraction","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided in multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation and test sets.\nThe model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\nSuccessively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units\u2014layers and layer widths\u2014in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).\nThis simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term \"validation set\" is sometimes used instead of \"test set\" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available."}},"key":"Model training","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation.  Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (\"subject\"). Some items are more likely to generate a nonresponse than others: for example items about private subjects such as income. Attrition is a type of missingness that can occur in longitudinal studies\u2014for instance studying development where a measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing.\nData often are missing in research in economics, sociology, and political science because governments or private entities choose not to, or fail to, report critical statistics, or because the information is not available. Sometimes missing values are caused by the researcher\u2014for example, when data collection is done improperly or mistakes are made in data entry.These forms of missingness take different types, with different impacts on the validity of conclusions from research: Missing completely at random, missing at random, and missing not at random.  Missing data can be handled similarly as censored data."}},"key":"missing values","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A prediction (Latin pr\u00e6-, \"before,\" and dicere, \"to say\"), or forecast, is a statement about a future event or data. They are often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference from \"estimation\"; different authors and disciplines ascribe different connotations.\nFuture events are necessarily uncertain, so guaranteed accurate information about the future is impossible. Prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business \"is at least two things: Important and hard.\""}},"key":"predictions","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Causal inference is the process of determining the independent, actual effect of a particular phenomenon that is a component of a larger system. The main difference between causal inference and inference of association is that causal inference analyzes the response of an effect variable when a cause of the effect variable is changed. The science of why things occur is called etiology. Causal inference is said to provide the evidence of causality theorized by causal reasoning.\nCausal inference is widely studied across all sciences. Several innovations in the development and implementation of methodology designed to determine causality have proliferated in recent decades. Causal inference remains especially difficult where experimentation is difficult or impossible, which is common throughout most sciences.\nThe approaches to causal inference are broadly applicable across all types of scientific disciplines, and many methods of causal inference that were designed for certain disciplines have found use in other disciplines. This article outlines the basic process behind causal inference and details some of the more conventional tests used across different disciplines; however, this should not be mistaken as a suggestion that these methods apply only to those disciplines, merely that they are the most commonly used in that discipline.\nCausal inference is difficult to perform and there is significant debate amongst scientists about the proper way to determine causality. Despite other innovations, there remain concerns of misattribution by scientists of correlative results as causal, of the usage of incorrect methodologies by scientists, and of deliberate manipulation by scientists of analytical results in order to obtain statistically significant estimates. Particular concern is raised in the use of regression models, especially linear regression models."}},"key":"Causal inference","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In mathematical physics, the causal structure of a Lorentzian manifold describes the causal relationships between points in the manifold."}},"key":"causal relation","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In the context of a relational database, a row\u2014also called a tuple\u2014represents a single, implicitly structured data item in a table. In simple terms, a database table can be thought of as consisting of rows and columns. Each row in a table represents a set of related data, and every row in the table has the same structure.\nFor example, in a table that represents companies, each row would represent a single company. Columns might represent things like company name, company street address, whether the company is publicly held, its VAT number, etc. In a table that represents the association of employees with departments, each row would associate one employee with one department.\nThe implicit structure of a row, and the meaning of the data values in a row, requires that the row be understood as providing a succession of data values, one in each column of the table. The row is then interpreted as a relvar composed of a set of tuples, with each tuple consisting of the two items: the name of the relevant column and the value this row provides for that column.\nEach column expects a data value of a particular type. For example, one column might require a unique identifier, another might require text representing a person's name, another might require an integer representing hourly pay in dollars."}},"key":"record number","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can perform automated deductions (referred to as automated reasoning) and use mathematical and logical tests to divert the code execution through various routes (referred to as automated decision-making). Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input."}},"key":"algorithms","order":["0"],"using":true}],"level":1,"permission":"private","sections":{"0":{"original":{"state":1,"text":"Accurate estimation of duration of surgery (DOS) can lead to cost-effective utilization of surgical staff and operating rooms and decrease patients\u2019 waiting time. In this study, we present a supervised DOS nonlinear regression prediction model whose accuracy outperforms earlier results. In addition, unlike previous studies, we identify the features that influence DOS prediction. Further, in difference from others, we study the causal relationship between the feature set and DOS. The feature sets used in prior studies included a subset of the features presented in this study. This study aimed to derive influential effectors of duration of surgery via optimized prediction and causality analysis. We implemented an array of machine learning algorithms and trained them on datasets comprising surgery-related data, to derive DOS prediction models. The datasets we acquired contain patient, surgical staff, and surgery features. The datasets comprised 23,293 surgery records of eight surgery types performed over a 10-year period in a public hospital. We have introduced new, unstudied features and combined them with features adopted from previous studies to generate a comprehensive feature set. We utilized feature importance methods to identify the influential features, and causal inference methods to identify the causal features. Our model demonstrates superior performance in comparison to DOS prediction models in the art. The performance of our DOS model in terms of the mean absolute error (MAE) was 14.9 minutes. The algorithm that derived the model with the best performance was the gradient boosted trees (GBT). We identified the 10 most influential features and the 10 most causal features. In addition, we showed that 40% of the influential features have a significant (p-value = 0.05) causal relationship with DOS. We developed a DOS prediction model whose accuracy is higher than that of prior models. This improvement is achieved via the introduction of a novel feature set on which the model was trained. Utilizing our prediction model, hospitals can improve the efficiency of surgery schedules, and by exploiting the identified causal relationship, can influence the DOS. Further, the feature importance methods we used can help explain the model\u2019s predictions.","title":"Abstract"},"summary":{"text":"Accurate estimation of duration of surgery (DOS) can lead to cost-effective utilization of surgical staff and operating rooms and decrease patients\u2019 waiting time. In this study, we present a supervised DOS nonlinear regression prediction model whose accuracy outperforms earlier results. In addition, unlike previous studies, we identify the features that influence DOS prediction. Further, in difference from others, we study the causal relationship between the feature set and DOS. The feature sets used in prior studies included a subset of the features presented in this study. This study aimed to derive influential effectors of duration of surgery via optimized prediction and causality analysis. We implemented an array of machine learning algorithms and trained them on datasets comprising surgery-related data, to derive DOS prediction models. The datasets we acquired contain patient, surgical staff, and surgery features. The datasets comprised 23,293 surgery records of eight surgery types performed over a 10-year period in a public hospital. We have introduced new, unstudied features and combined them with features adopted from previous studies to generate a comprehensive feature set. We utilized feature importance methods to identify the influential features, and causal inference methods to identify the causal features. Our model demonstrates superior performance in comparison to DOS prediction models in the art. The performance of our DOS model in terms of the mean absolute error (MAE) was 14.9 minutes. The algorithm that derived the model with the best performance was the gradient boosted trees (GBT). We identified the 10 most influential features and the 10 most causal features. In addition, we showed that 40% of the influential features have a significant (p-value = 0.05) causal relationship with DOS. We developed a DOS prediction model whose accuracy is higher than that of prior models. This improvement is achieved via the introduction of a novel feature set on which the model was trained. Utilizing our prediction model, hospitals can improve the efficiency of surgery schedules, and by exploiting the identified causal relationship, can influence the DOS. Further, the feature importance methods we used can help explain the model\u2019s predictions.","title":"Abstract"}},"1":{"original":{"state":0,"text":"High utilization of resources such as equipment, staff, and facilities in healthcare organizations generates efficient patient flow and cuts costs [1\u20133]. The high cost of surgeries and operating rooms (ORs) have made them key elements for hospital administrators looking to streamline expenses [4]. OR underutilization results in negative consequences such as staff idle time, increased patient waiting times for surgeries, and more. On the other hand, OR overutilization might overload the staff, increase patient waiting time and dissatisfaction, generate disorder, increase the probability of human error, and more [3\u20135]. Each surgery comprises a number of procedures with a surgical staff to support it. This includes the surgeon, an anesthesiologist, nurses, and other staff members. Surgeries, roughly speaking, are either emergency or elective. The duration of surgery (DOS) is defined as the period of time during which the patient is in the OR. DOS is the chief variable affecting surgery scheduling and OR management. Current practices in many hospitals suggest that physicians who are hospital staff members schedule the surgeries. As shown in the art, however, physicians tend to predict DOS inaccurately, thus causing sub-optimal scheduling [6]. In other hospitals, each surgery is allocated a default DOS. This default time is the computed mean duration of the specific procedures of that surgery type [2, 7, 8]. Given the suboptimality of DOS prediction and its negative effect on OR management, multiple studies developed machine learning (ML) DOS prediction models, aiming to optimize OR utilization. Those studies, however, did not examine causality and did not provide systematic explanations for the predictions derived by their models. In this research, we address these lacunae. ML techniques are widely used in health informatics studies [9\u201311]. With the increase in surgery documentation in electronic health records (EHRs), ML has become very relevant for DOS prediction. Given the large size of surgery datasets and the abundance of factors that could influence DOS, ML facilitates data analysis beyond conservative factors and practices. As DOS values are continuous, ML regression models are highly appropriate for their prediction. With this in mind, we have developed a DOS regression prediction model. Explaining predictions produced by ML models, beyond the performance metrics, is a necessary element of ML research in healthcare [12]. Understanding the importance of each feature to the model\u2019s predictions sheds light on the model\u2019s behavior. Such understanding allows domain experts, i.e., physicians and surgeons, to validate the model\u2019s predictions and gives them a tool for optimizing surgery management. Methods for explaining individual predictions by the features used are known in the art. Other methods that explain the cumulative influence of features on the model\u2019s prediction are also known. For an individual prediction, the output of such methods is the contribution of each feature to the prediction value. To calculate the cumulative feature importance, most methods average or sum the contribution of each feature across all records [13]. In our research, we utilize these methods to study the cumulative effects of features. We compute feature importance using algorithms such as Shapley Additive exPlanations (SHAP) [14, 15]. A causal relationship, unlike correlation, describes the relationship between two variables, suggesting that one has caused the other to occur [16\u201318]. Causal inference addresses the problem of identifying cause and effect relationships in data [17] and has a central role in the healthcare [19]. The determination that a connection between a feature and the target variable is causal indicates that intervention may be beneficial [20]. For example, one can intervene by changing the composition of the surgical staff, thus decreasing the DOS. Earlier studies developed regression ML models to predict DOS values. Most studies used linear regression algorithms for the development of the prediction models [21\u201324]. Some recent studies, e.g., that by Jiao et al. (2020), employed ML algorithms, e.g., multilayer perceptron. The feature set used in those studies for model development included patient features and procedure features but did not incorporate surgical staff features as we do [23]. Additionally, their patient and procedure features comprised only a subset of those examined by our study. Further, these studies did not explain the DOS model\u2019s predictions. Studies whose feature set is similar to ours [25, 26] developed ML models to predict length of physician appointment and length of stay in the emergency department, however, they have not analyzed DOS, nor have they generated explanations for their predictions. Unlike previous studies, our focus is on the importance of features and the effect of that importance on the model and the predicted DOS. We study a broad range of patient features (age, gender, BMI, etc.), surgical staff features (experience, age, etc.), and surgical features. In addition, we use explanatory algorithms to analyze our model\u2019s predictions and causal inference algorithms to study the effect of our features on DOS. Our models provide a prediction for both the elective and the emergency surgery classes. To develop our models, we cooperated with the main surgical department of one of the largest Israeli public hospitals, the Tel Aviv Sourasky Medical Center (TASM). In addressing the challenges described above, the contribution of this study is four-fold. (1) We develop DOS prediction models whose performance levels are higher than those of existing DOS prediction models. (2) We introduce a feature set that includes novel features studied here for the first time as well as features examined by previous studies. (3) We identify the most influential features affecting DOS prediction. (4) We study the causal relationship between features and DOS via causal inference algorithms. This study has several OR management implications. Using our prediction model, OR management teams can improve the performance of surgery scheduling in terms of patient waiting time and surgery team idle time. Using the identified causal relationships, OR management teams can control and adjust DOS values. Further, the explanatory methods elucidate the model\u2019s predictions. The paper proceeds as follows. The Introduction section presents the state-of-the-art, the motivation for this study, and its objectives. The Methodology section focuses on the research methodology applied according to IJMEDI checklist [27], including dataset acquisition, preprocessing, causality analysis, and feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies. In the Discussion section, we discuss the main findings and point at future directions of our research. Finally, in the Conclusion section, we summarize the insights that were obtained in our study.","title":"Introduction"},"summary":{"text":" High cost of surgeries and operating rooms (ORs) have made them key elements for hospital administrators looking to streamline expenses. OR underutilization results in negative consequences such as staff idle time, increased patient waiting times for surgeries, and more. OR overutilization might overload the staff, increase patient waiting time and dissatisfaction, generate disorder, increase the probability of human error. Current practices suggest that physicians who are hospital staff members schedule the surgeries. As shown in the art, however, physicians tend to predict DOS inaccurately, thus causing sub-optimal scheduling.  ML techniques are widely used in health informatics studies. With the increase in surgery documentation in electronic health records, ML has become very relevant for DOS prediction. Explaining predictions produced by ML models is a necessary element of ML research in healthcare [12] Explaining the importance of each feature to the model sheds light on the model\u2019s behavior. Such understanding allows domain experts, i.e., physicians and surgeons, to validate the model's predictions and gives them a tool for optimizing surgery management.  Causal inference addresses the problem of identifying cause and effect relationships in data [17] and has a central role in healthcare [19] Unlike previous studies, our focus is on the importance of features and the effect of that importance on the model and the predicted DOS. Our models provide a prediction for both the elective and the emergency surgery classes. We study a broad range of patient features (age, gender, BMI, etc. ) and surgical staff features. In addition, we use explanatory algorithms to analyze our model\u2019s predictions and causal inference algorithms.  Using our prediction model, OR management teams can improve the performance of surgery scheduling in terms of patient waiting time and surgery team idle time. This study has several OR management implications. Using the identified causal relationships, OR teams can control and adjust DOS values. The paper proceeds as follows. The Introduction section presents the state-of-the-art, the motivation for this study, and its objectives.","title":"Introduction"}},"2":{"original":{"state":0,"text":"Our methodology comprises six stages, as follows: 1) collecting and preprocessing a dataset; 2) finding the causal relationship between features and DOS; 3) developing a DOS supervised regression model, referred to as DOSM; 4) evaluating DOSM\u2019s performance; 5) calculating feature importance; and 6) comparing influential and causal features. Fig 1 presents a flowchart of the methodology.","title":"Methodology"},"summary":{"text":" Our methodology comprises six stages, as follows: collecting and preprocessing a dataset, finding the causal relationship between features and DOS; developing a DOS supervised regression model, referred to as DOSM. Fig 1 presents a flowchart of the methodology.","title":"Methodology"}},"3":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6699.png"],"state":0,"text":"Our surgery dataset (SD) was obtained from the Tel Aviv Sourasky Medical Center\u2019s (TASM) (a public hospital) surgery department. The data were approved by the TASM institutional review board (IRB), approval number 0332-21-TLV. This study involves data about human participants but the IRB exempted this study from participant consent. The data were fully  Fig 1. Methodology flowchart. anonymized and then used for this study. The data included 23,293 retrospective surgical records, focusing on the eight most common surgeries in this department between 2010 and 2020. The dataset included surgical features, patient features, and surgical staff features. We examined the features that previous studies used for their DOS models and, from among them, we adopted those that are independent of the surgery type. We moreover used additional features that were suggested by domain experts, i.e., experienced surgeons [3, 8, 21, 23]. The full list of features is shown in Table 1. The table shows feature names, indication of whether a feature is novel (by a V in the Novel column), the value range of each feature, and values\u2019 statistics. For numeric features, statistics include maximum, minimum, mean, and STD values. For nominal features, it includes the distribution. In the data preprocessing stage, we omitted records whose DOS value was missing. We also excluded outliers, which comprised about 5% of the records. Missing surgical staff data were manually completed by the surgery department\u2019s staff. For handling missing data of other features, we used the Sequence of Regression Models (SRM) technique for multiple inputting of missing values [28]. Accordingly, the missing values of features were computed using the values of other features.","title":"Stage 1: Data collection and preprocessing"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6699.png"],"text":" The data was obtained from the Tel Aviv Sourasky Medical Center\u2019s (TASM) (a public hospital) surgery department. The data included 23,293 retrospective surgical records, focusing on the eight most common surgeries in this department between 2010 and 2020. The full list of features is shown in Table 1. The table shows feature names, indication of whether a feature is novel (by a V in the Novel column), the value range of each feature, and values\u2019 statistics. For handling missing data of other features, we used the Sequence of Regression Models (SRM) technique.  The missing values of features were computed using the values of other features.","title":"Stage 1: Data collection and preprocessing"}},"4":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6700.png"],"state":0,"text":"The causal effect of a feature on an outcome variable (in our case, DOS), e.g., in the context of medicine, is called the treatment effect or heterogeneity treatment effect (HTE) [29]. The average treatment effect (ATE) of a feature (whose value range is binary) measures the difference in the mean of the outcomes between data records with different values assigned to the feature. Since our study is observational, the ATE values could not be computed accurately, as a feature in a surgery record only has an observed value and cannot be assigned other values [29]. Consequently, we had to estimate the ATE values to measure their causal effect on DOS. Several ML algorithms are used to estimate the ATE value. For example, the ATE for a binary feature f is calculated as follows:  f 1\u00f0i\u00de y We use Eq (1) and its extensions to calculate the ATE. Here, y f 1\u00f0i\u00de is the value of the outcome in record number i when the value of feature f is 1. y f 0\u00f0i\u00de is the value of the outcome in record number i when the value of feature f is 0. In an observational study, y f 1\u00f0i\u00de and y f 0\u00f0i\u00de are estimated using ML algorithms. Extensions of Eq (1) that we used for calculating the ATE value of non-binary features are presented in [30]. Two main ML model types, propensity and heterogeneity models, are used for estimating causal effects. The former models are used for estimating the propensity score, which is the probability of a record to have a particular feature value given a set of observed other features, i.e., covariates. Propensity scores are used to reduce confounding variables\u2019 effects and the implied bias. The latter models are used for estimating the heterogeneity of the treatment effect [31]. To develop the heterogeneity model, we used forest-based algorithms, which estimate nonlinear HTE. The commonly used algorithms are orthogonal random forest (EstimatorDROrthoForest), forest double ML estimator, i.e., causal forest (CausalForestDML), and forest doubly robust estimator (ForestDRLearner) [32, 33]. For the development of the propensity model, we used the commonly used algorithms LassoCV, RF, and GBT [34]. To optimize the models\u2019 hyperparameters, we used the grid search algorithm.","title":"Stage 2: Causal inference"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6700.png"],"text":" The average treatment effect (ATE) of a feature (whose value range is binary) measures the difference in the mean of the outcomes between data records with different values assigned to the feature. Since our study is observational, the ATE values could not be computed accurately, as a feature in a surgery record only has an observed value and cannot be assigned other values [29] We use Eq (1) and its extensions to calculate ATE. Two main ML model types, propensity and heterogeneity models, are used for estimating causal effects.  The propensity score is the probability of a record to have a particular feature value given a set of observed other features. Propensity scores are used to reduce confounding variables\u2019 effects and the implied bias. The latter models are used for estimating the heterogeneity of the treatment effect [31] To develop the heterogeneity model, we used forest-based algorithms.","title":"Stage 2: Causal inference"}},"5":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6701.png","https://testing-api.leminda.com/summary/2491/img/6702.png"],"state":0,"text":"Recent studies have shown that RF, GBT, and deep neural networks (DNNs) are capable of accurately predicting both binary and high-variance continuous variables in the healthcare domain [10, 26, 35]. Therefore, to develop the model, we utilized tree-based and DNN\u2013based ML algorithms. Two tree-based algorithms were used, RF and GBT. One DNN-based algorithm was used\u2013MLP.  Table 1. Description of features.  For training and testing our model, we split the SD: 70% for training and 30% for testing. Given that the SD contained data from eight surgery types, we measured the performance metrics for the whole training set and for each of its sub-sets, partitioned by surgery type. As noted above, to optimize the model\u2019s hyperparameters, we used the grid search algorithm. Grid search combines all possible hyperparameters to be optimized with predefined value ranges [36]. The algorithm\u2019s output is the model\u2019s hyperparameters whose performance levels are the highest.","title":"Stage 3: Model development"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6701.png","https://testing-api.leminda.com/summary/2491/img/6702.png"],"text":" Recent studies have shown that RF, GBT, and deep neural networks (DNNs) are capable of accurately predicting both binary and high-variance continuous variables in the healthcare domain. For training and testing our model, we split the SD: 70% for training and 30% for testing. We measured the performance metrics for the whole training set and for each of its sub-sets, partitioned by surgery type.","title":"Stage 3: Model development"}},"6":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6703.png","https://testing-api.leminda.com/summary/2491/img/6704.png"],"state":0,"text":"To evaluate our model\u2019s performance, we used the regression metrics Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Root Mean Square Error (RMSE). The metrics are computed as follow: yi ti yi   Where yi is the predicted DOS value of record i, ti is the true value of DOS, and n is the number of records. To evaluate the grid search output, we used K-fold cross-validation, a commonly used method to fully and effectively utilize data [37]. To compare the performance of our model to the performance of the DOS prediction models presented in previous studies, we applied a methodology presented in a state-of-the-art study to our SD [3]. That study was selected for our comparison because the performance it achieved is better than that achieved by other studies. Further, its model\u2019s features do not depend on a specific surgery type. One aim of this comparison was to examine whether the introduction of the novel features in our study results in better model performance than the performance of prior models. To this end, we re-implemented the model presented in [3]. The comparison was performed on the same test and training sets. The performance metric used for the comparison was MAE.","title":"Stage 4: Model validation"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6703.png","https://testing-api.leminda.com/summary/2491/img/6704.png"],"text":" To evaluate our model\u2019s performance, we used the regression metrics Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Root Mean Square Error (RMSE) The metrics are computed as follow: yi is the predicted DOS value of record i, ti is the true value of DOS, and n is the number of records. To evaluate the grid search output, We used K-fold cross-validation, a commonly used method to fully and effectively utilize data.","title":"Stage 4: Model validation"}},"7":{"original":{"state":0,"text":"To identify the features that influence the DOS prediction, we employed feature importance methods that do not depend on the algorithm type. First, we utilized Pearson correlation to compute the correlation between the independent features and the dependent feature. Then, we used SHAP to estimate the contribution of each feature to the model\u2019s prediction [38, 39].","title":"Stage 5: Influential feature identification"},"summary":{"text":" To identify the features that influence the DOS prediction, we employed feature importance methods that do not depend on the algorithm type. We used Pearson correlation to compute the correlation between the independent features and the dependent feature. Then, we used SHAP to estimate the contribution of each feature to the model\u2019s prediction.","title":"Stage 5: Influential feature identification"}},"8":{"original":{"state":0,"text":"We compared the influential features and the causal features. To this end, we filtered out features that had high correlation with the causal features so that the comparison would not be based on highly correlated features. To filter, we initially split the feature set F into two subsets. The first set\u2013causal feature set (CF)\u2013includes features whose absolute ATE value is greater than 0 and are identified as significant causal features (using P = .05). The second subset\u2013noncausal feature set (NCF), NCF = F \\ CF\u2013includes the remaining features. The filtering process was done by calculating the Pearson correlation between the causal features in CF and the non-causal features in NCF and omitting NCF features that highly correlate with CF features (i.e., the correlation value is greater than 0.49) [40]. The resultant filtered subset, whose member features are NCF features that are not correlated to CF features, is the filtered non-causal feature set (FNCF). To calculate feature importance, we developed a DOS prediction model using the features in CF and FNCF. We call this model DOSM-F, as it is similar to DOSM, but with filtered features. For the comparison we used the influential features of DOSM-F and the causal features. We aimed to identify features that influence DOS prediction and also have a causal relationship with DOS. In addition, we examined whether a feature that has a positive causal effect on DOS also has a positive effect on the DOS predicted value, and whether a feature that has a negative causal effect on DOS also has a negative effect on the DOS predicted value. The DOSM-F model was used to estimate the potential change in the DOS as a result of variations in causal feature values (for example, potential changes in the surgical staff size). We used DOSM-F because the features used for training that model were CF and FNCF. Training using only these features allowed the CF values to have a bigger impact on the prediction value of DOSM-F compared to the prediction value derived when using all features including the features correlated with CF.","title":"Stage 6: Comparison between influential and causal features"},"summary":{"text":" We filtered out features that had high correlation with the causal features so that the comparison would not be based on highly correlated features. To calculate feature importance, we developed a DOS prediction model using the features in CF and FNCF. We aimed to identify features that influence DOS prediction and also have a causal relationship with DOS. In addition, we examined whether a feature that has a positive causal effect on DOS also had a positive effect on the DOS predicted value. We call this model DOSM-F, as it is similar to DOSM, but with filtered features.  DOSM-F model was used to estimate the potential change in the DOS as a result of variations in causal feature values. Training using only CF values allowed the CF values to have a bigger impact on the prediction value of the model.","title":"Stage 6: Comparison between influential and causal features"}},"9":{"original":{"state":0,"text":"DOSM development and data analysis were done via Python scripts using the EconML [43], scikit-learn, LightGBM, NumPy, SHAP, and scikit-feature packages.","title":"Implementation"},"summary":{"text":" DOSM development and data analysis were done via Python scripts using the EconML, scikit-learn, LightGBM, NumPy, SHAP, and NumPy.","title":"Implementation"}},"10":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6705.png"],"state":0,"text":"Table 2 summarizes the statistical metrics of DOS in minutes across the surgery types, without our model being applied. Patients\u2019 average age of is 47.5 years and the STD is 19.5.  Table 2. Dataset statistics. Fig 2 presents the DOS distribution, which is a positively skewed distribution. The high STD values across the surgery types indicate that the regression predicted values, i.e., the predicted DOS values, are spread over a broad range.","title":"Data analysis"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6705.png"],"text":" Patients\u2019 average age of is 47.5 years and the STD is 19.5. Fig 2 presents the DOS distribution, which is a positively skewed distribution. The high STD values across the surgery types indicate that the regression predicted values, i.e., the predicted DOS values, are spread over a broad range.","title":"Data analysis"}},"11":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6706.png","https://testing-api.leminda.com/summary/2491/img/6707.png"],"state":0,"text":"Causal model development.  Fig 2. DOS distribution. The causal analysis models we used were trained on the SD. The inputs to these models are a vector of the counterfactual features X, a vector of the out-  Table 3. Causal inference models\u2019 hyperparameter values. come feature Y (i.e., the model\u2019s target feature), and a vector of a selected feature T\u2013a candidate causal feature. The SD comprises n feature column vectors fi and one target feature Y, i.e., SD = {f1,. . ., fn, Y}. To extract X, Y and T from the SD, we select T = fi, X = SD\\{fi, Y}; Y is the DOS column of the SD. We iterated over i and calculated the ATE for all fi features. Thus, we obtained the causal effect of all the features on DOS. Hyperparameter optimization. The hyperparameter values we used to optimize the HTE and propensity models are listed in Table 3. The LassoCV algorithm is an iterative algorithm that finds the optimal parameters for a Lasso model using cross-validation; thus, this algorithm does not appear in Table 3 [44]. The hyperparameter values used (see Table 3) are similar to values commonly used in the art. Causal feature identification. Table 4 presents the 10 features whose absolute ATE values were the highest, in decreasing order. Six of the 10 most causal features are also among the 10 most correlated features by Pearson correlation (shown in Table 9). Half of the top 10 causal features are among the novel features shown in Table 1.","title":"Causal inference"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6706.png","https://testing-api.leminda.com/summary/2491/img/6707.png"],"text":" Causal analysis models we used were trained on the SD. The inputs to these models are a vector of the counterfactual features X and a vector for the model\u2019s target feature Y. The hyperparameter values we used to optimize the HTE and propensity models are listed in Table 3. Table 4 presents the 10 features whose absolute ATE values were the highest, in decreasing order. Half of the top 10 causal features are among the novel features shown in Table 1. The LassoCV algorithm is an iterative algorithm that finds the optimal parameters for a Lasso model using cross-validation.","title":"Causal inference"}},"12":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6708.png","https://testing-api.leminda.com/summary/2491/img/6709.png"],"state":0,"text":"Hyperparameter optimization.  Table 4. ATE values. The hyperparameter values we used for optimizing our DOS prediction models appear in Table 5. These values are similar to values commonly used  Table 5. Hyperparameter values. in the art [45\u201349]. Model training and performance. We trained the DOS models on the dataset using several ML algorithms. The ML algorithms calculate the features\u2019 influence differently; for this reason, the models were trained on all of the features. The algorithms that generated the top performing models\u2013GBT being the best\u2013are presented in Table 6. Overall, the MAE values in the table suggest that the performance is similar across the three algorithms, with GBT performing a bit better. Table 7 presents the per surgery type performance of the GBT model. This was done by splitting the test set by surgery type. In addition, to evaluate the effectiveness of our model against current practices, we calculated the MAE value of the manual method currently used by the clinics\u2019 staff for each surgery type. In the manual method, the mean of the previous surgery by type is used to estimate the future DOS. The DOSM performance is significantly better than that of the manual method (using P = .05) (see Table 7). We have calculated the model\u2019s uncertainty as follows. First, for each record in the test set, we used the DOSM to predict a list of probabilities from each tree in the GBT. Then, for each record, we calculated the STD from the list of probabilities. Finally, we calculated the mean of the STDs. Following this flow, the derived uncertainty of the model was 4.1 minutes.","title":"Model development"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6708.png","https://testing-api.leminda.com/summary/2491/img/6709.png"],"text":" We trained the DOS models on the dataset using several ML algorithms. The algorithms that generated the top performing models\u2013GBT being the best\u2013are presented in Table 6. The MAE values in the table suggest that the performance is similar across the three algorithms, with GBT performing a bit better. We have calculated the model\u2019s uncertainty as follows: For each record in the test set, we used the DOSM to predict a list of probabilities from each tree in the GBT. The derived uncertainty of the model was 4.1 minutes.","title":"Model development"}},"13":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6710.png"],"state":0,"text":"To compare our model\u2019s performance and examine whether the novel features introduced in our study derive a model that outperforms the state of the art, we developed two additional models. The first one, Barket-FM-DOSM, is a DOS model using the features and the methods used in Barket et al. (2019), but trained on our SD. The second one, Barket-F-DOSM, is a DOS model taking only the features used in Barket et al. (2019), but using our methods and trained  Table 6. DOS models\u2019 performance (in minutes). on SD. The results of the comparison are shown in Table 8. One can observe that the MAE value of our model\u2013DOSM\u2013is lower than the MAE values derived for Barket-F-DOSM and BarketFM-DOSM, indicating that our model outperforms recent models, presented in Barket et al. (2019). This comparison led to the conclusion that neither the ML algorithms nor the dataset are the source of differences in the models performance. The major effector of such differences is the set of features.","title":"Comparison to recent results"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6710.png"],"text":" Barket-FM-DOSM is a DOS model using the features and the methods used in Barket et al. (2019) but trained on our SD. The MAE value of our model\u2013DOSM\u2013is lower than the MAE values derived for Barket. This comparison led to the conclusion that neither the ML algorithms nor the dataset are the source of differences in the models performance. The major effector of such differences is the set of features.","title":"Comparison to recent results"}},"14":{"original":{"state":0,"text":"Feature importance was computed using the SHAP algorithm. SHAP computes importance values for all features. To select the most influential features, we transformed the importance values distribution into a normal distribution (via a log transformation). From that normal distribution, we selected only the features whose values were one standard deviation from the rightmost edge of the distribution. The features left were selected as the most influential ones. Fig 3 illustrates the 8 most influential features on DOS prediction computed by SHAP, in a decreasing order of importance. The higher the vertical location\u2013the higher the feature\u2019s importance. Each point in Fig 3 is a SHAP value of a record per feature. The latter determines its position on the y-axis and the former (the record), its position on the x-axis. The color represents the value of the feature from low to high; red indicates that the feature\u2019s value is high. Overlapping points are jittered in the y-axis direction. The horizontal location of a dot indicates its feature\u2019s value effect on DOS, i.e., the impact on the model\u2019s output. Half of the 10 most influential features are among the novel features presented in Table 1 in Section 2. In Table 9, the 8 features with the highest absolute Pearson correlation values vis-\u00e0-vis DOS are presented in decreasing order of correlation values. Features whose correlation values are smaller than 0.3, which are considered weak according to common practices [50], are not presented. From the above results, we observe that 3 out of 8 (37.5%) of the features selected are the same for both methods, SHAP and Pearson correlation.","title":"Feature importance"},"summary":{"text":" Feature importance was computed using the SHAP algorithm. Half of the 10 most influential features are among the novel features presented in Table 1 in Section 2. The 8 features with the highest absolute Pearson correlation values vis-\u00e0-vis DOS are presented in decreasing order of correlation values. We observe that 3 out of 8 (37.5%) of the features selected are the same for both methods, SHAP and Pearson correlation. The higher the vertical location of a dot indicates its feature\u2019s value effect on DOS, i.e.e., the impact on the model's output.","title":"Feature importance"}},"15":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6711.png","https://testing-api.leminda.com/summary/2491/img/6712.png"],"state":0,"text":"For the development of DOSM-F we used the same algorithms and hyperparameters used for developing DOSM. The performance of DOSM-F was 15.4 minutes in terms of MAE. The 10 most influential features are presented in Fig 4.  Fig 3. SHAP\u2014DOSM.  Fig 5 presents the influential features\u2019 causal relation to DOS by showing the ATE value of the 10 most influential features ordered by their importance in the same order as in Fig 4. It demonstrates that the order of the 10 most important features by influence and by causal value is different. 40% of the important features have a significant (P = .05) causal relationship with DOS. Our results reinforce the assertion made in the art that the features that influence prediction are not necessarily causal features [51]. Figs 4 and 5 show that an influential feature that has a positive effect on the predicted DOS value also has a positive causal effect on DOS and vice-versa.","title":"Comparison between feature importance and causal features"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6711.png","https://testing-api.leminda.com/summary/2491/img/6712.png"],"text":" The performance of DOSM-F was 15.4 minutes in terms of MAE. 40% of the important features have a significant (P =.05) causal relationship with DOS.","title":"Comparison between feature importance and causal features"}},"16":{"original":{"images":["https://testing-api.leminda.com/summary/2491/img/6713.png","https://testing-api.leminda.com/summary/2491/img/6714.png","https://testing-api.leminda.com/summary/2491/img/6715.png"],"state":0,"text":"Surgeries are one of hospitals\u2019 largest expenditure sources. Hence, optimizing their flow to reduce costs is an important objective. Improving resource utilization, minimizing surgery lead time, and minimizing patient waiting time in the waiting room could help achieve this goal. This study presented methods to facilitate such optimization.  Table 9. Pearson correlation values. We utilized ML techniques to develop supervised ML models that predict DOS from features related to patients, physicians, and surgeries. For training the models, we built a dataset  Fig 4. SHAP\u2014DOSFM. of 23,293 records, collected and processed in collaboration with one of the biggest public hospitals in Israel. Our dataset contained data on eight types of surgeries from the years 2010 to 2020. Our feature set combines novel features used for the first time here and features adopted from previous studies. The performance of our DOS model in terms of MAE was 14.9 minutes. The ML algorithm  Fig 5. The ATE value of the 10 most influential features by SHAP. that derived the model with the best performance was the GBT. We compared the performance of our model to the performance of existing models by re-implementing the latter and training them on our dataset. Our model outperformed earlier models. The main goal of this study was to identify the features that were most influential in predicting DOS and the features that have a causal relationship with DOS. To this end, we utilized feature importance methods to identify the influential features, and causal inference methods to identify the causal features. We demonstrated that five of the 10 most influential features on DOS prediction and five of the 10 most causal features on DOS are among the novel features we introduced in this study. In addition, we have shown that most of the influential features do not have a causal relationship with DOS. The results of this research have several implications. Firstly, using the DOS value predicted by our model for surgery scheduling can decrease patient waiting time and minimize surgical staff idle time. Additionally, using the identified causal relationship, OR management teams can apply measures to affect DOS. This can be done, for example, using the DOSM-F model and estimating the potential change in DOS as a result of variations in causal feature values. Further, the explanatory methods we used can facilitate validation of the model\u2019s prediction. There are some limitations in our study. Our datasets contained data of eight surgery types. Future research could study additional surgery types at different hospitals to broaden applicability of our results. A future study can evaluate the performance of the prediction model when combined with a scheduling system in a production environment. Further research is needed to quantify the potential cost-saving and OR utilization when using the DOSM.","title":"Discussion"},"summary":{"images":["https://testing-api.leminda.com/summary/2491/img/6713.png","https://testing-api.leminda.com/summary/2491/img/6714.png","https://testing-api.leminda.com/summary/2491/img/6715.png"],"text":" We used ML techniques to develop supervised ML models that predict DOS from features related to patients, physicians, and surgeries. For training the models, we built a dataset of 23,293 records, collected and processed in collaboration with one of the biggest public hospitals in Israel. The performance of our DOS model in terms of MAE was 14.9 minutes. The ATE value of the 10 most influential features by SHAP that derived the model with the best performance was the GBT. The model outperformed earlier models.  Using the DOS value predicted by our model for surgery scheduling can decrease patient waiting time and minimize surgical staff idle time. Using the identified causal relationship, OR management teams can apply measures to affect DOS. Future research could study additional surgery types at different hospitals to broaden applicability of our results. Further research is needed to quantify potential cost-saving and OR utilization when using the DOSM.","title":"Discussion"}},"17":{"original":{"state":0,"text":"We used ML methods to develop a supervised regression model to predict DOS using various novel features of patients, surgical staff, and surgeries. The model we developed outperformed the current method used in hospitals and the DOS models developed in previous studies. Several insights were obtained in our study, including identification of the most influential features on DOS prediction and identification of the causal relationship between the features and DOS.","title":"Conclusions"},"summary":{"text":" We used ML methods to develop a supervised regression model to predict DOS using various novel features of patients, surgical staff, and surgeries. The model outperformed the current method used in hospitals and the DOS models developed in previous studies.","title":"Conclusions"}},"18":{"original":{"state":2,"text":"Data curation: Orel Babayoff, Meishar Shahoha, Ruth Sasportas, Ahuva Weiss-Meilik. Methodology: Orel Babayoff. Project administration: Orel Babayoff, Onn Shehory. Software: Orel Babayoff. Writing \u2013 original draft: Orel Babayoff, Onn Shehory. Writing \u2013 review & editing: Orel Babayoff, Onn Shehory, Meishar Shahoha, Ruth Sasportas, Ahuva Weiss-Meilik.","title":"Author Contributions"}}},"source":[""],"stats":{"length":{"original":4679,"summary":1979},"quality":{},"readability":{"original":13.0,"summary":13.0},"time":{"reading_time_original_text":13,"reading_time_saved":8,"reading_time_summary_text":6,"reduction_percentage":57}},"text_id":"63974a11fcd076664870fa7f","title":"Surgery duration: Optimized prediction and causality analysis","topic":"t1","ts":1670859354,"user":"6363ecebc08bcd5ee32d7e04","userRecord":{"family_name":"\u05e7\u05d9\u05d9\u05d6\u05e8","given_name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3","name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3 \u05e7\u05d9\u05d9\u05d6\u05e8"}}
