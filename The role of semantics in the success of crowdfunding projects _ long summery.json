{"_id":"2841","attribs":{"courseNumber":"","institution":"","lecturer":"","semester":"","subTopic":"","year":""},"fileAttributes":[{"authors":["Orel Babayoff","Onn Shehory"],"doi":"doi:10.1371/journal.pone.0263891","journal":"www.plosone.org","title":"The role of semantics in the success of crowdfunding projects","year_published":"2022"}],"key_concepts":[{"data":{"0":{"tag":"NP","wiki":"Project management is the process of leading the work of a team to achieve all project goals within the given constraints. This information is usually described in project documentation, created at the beginning of the development process. The primary constraints are scope, time, and budget. The secondary challenge is to optimize the allocation of necessary inputs and apply them to meet pre-defined objectives.\nThe objective of project management is to produce a complete project which complies with the client's objectives. In many cases, the objective of project management is also to shape or reform the client's brief to feasibly address the client's objectives. Once the client's objectives are clearly established, they should influence all decisions made by other people involved in the project \u2013 for example, project managers, designers, contractors, and sub-contractors. Ill-defined or too tightly prescribed project management objectives are detrimental to decision making.\nA project is a temporary and unique endeavor designed to produce a product, service, or result with a defined beginning and end (usually time-constrained, and often constrained by funding or staffing) undertaken to meet unique goals and objectives, typically to bring about beneficial change or added value.  The temporary nature of projects stands in contrast with business as usual (or operations), which are repetitive, permanent, or semi-permanent functional activities to produce products or services. In practice, the management of such distinct production approaches requires the development of distinct technical skills and management strategies."}},"key":"project funding","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A project is any undertaking, carried out individually or collaboratively and possibly involving research or design, that is carefully planned to achieve a particular aim.An alternative view sees a project managerially as a sequence of events: a \"set of interrelated tasks to be executed over a fixed period and within certain cost and other limitations\".A project may be a temporary (rather than permanent) social system (work system), possibly staffed by teams (within or across organizations) to accomplish particular tasks under time constraints.A project may form a part of wider programme management\nor function as an ad hoc system.Note that open-source software \"projects\" or artists' musical \"projects\" (for example) may lack defined team-membership, precise planning and/or time-limited durations."}},"key":"Projects","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"A semantic feature is a component of the concept associated with a lexical item ('female' + 'performer' = 'actress'). More generally, it can also be a component of the concept associated with any grammatical unit, whether composed or not ('female' + 'performer' = 'the female performer' or 'the actress'). \nAn individual semantic feature constitutes one component of a word's intention, which is the inherent sense or concept evoked.\nLinguistic meaning of a word is proposed to arise from contrasts and significant differences with other words.\nSemantic features enable linguistics to explain how words that share certain features may be members of the same semantic domain.\nCorrespondingly, the contrast in meanings of words is explained by diverging semantic features.\nFor example, father and son share the common components of \"human\", \"kinship\", \"male\" and are thus part of a semantic domain of male family relations.\nThey differ in terms of \"generation\" and \"adulthood\", which is what gives each its individual meaning."}},"key":"semantic features","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:\n\nsimplification of models to make them easier to interpret by researchers/users,\nshorter training times,\nto avoid the curse of dimensionality,\nimprove data's compatibility with a learning model class,\nencode inherent symmetries present in the input space.The central premise when using a feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant and irrelevant are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples."}},"key":"Feature selection","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data."}},"key":"feature extraction","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics.  The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing. The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages.\nCompare speech segmentation, the process of dividing speech into linguistically meaningful portions."}},"key":"topic analysis","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Accuracy and precision are two measures of observational error. \nAccuracy is how close or far off a given set of measurements (observations or readings) are to their true value, while precision is how close or dispersed the measurements are to each other.\nIn other words, precision is a description of random errors, a measure of statistical variability. Accuracy has two definitions:\n\nMore commonly, it is a description of only systematic errors, a measure of statistical bias of a given measure of central tendency; low accuracy causes a difference between a result and a true value; ISO calls this trueness.\nAlternatively, ISO defines accuracy as describing a combination of both types of observational error (random and systematic), so high accuracy requires both high precision and high trueness.In the first, more common definition of \"accuracy\" above, the concept is independent of \"precision\", so a particular set of data can be said to be accurate, precise, both, or neither.\nIn simpler terms, given a statistical sample or set of data points from repeated measurements of the same quantity, the sample or set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if their standard deviation is relatively small."}},"key":"accuracy","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"In statistical analysis of binary classification, the F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification. \nThe F1 score is the harmonic mean of the precision and recall. The more generic \n  \n    \n      \n        \n          F\n          \n            \u03b2\n          \n        \n      \n    \n    {\\displaystyle F_{\\beta }}\n   score applies additional weights, valuing one of precision or recall more than the other.\nThe highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if both precision and recall are zero."}},"key":"F score","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Crowdfunding is the practice of funding a project or venture by raising money from a large number of people, in modern times typically via the Internet. Crowdfunding is a form of crowdsourcing and Alternative Finance. In 2015, over US$34 billion were raised worldwide by crowdfunding.Although similar concepts can also be executed through mail-order subscriptions, benefit events, and other methods, the term crowdfunding refers to Internet-mediated registries. This modern crowdfunding model is generally based on three types of actors \u2013 the project initiator who proposes the idea or project to be funded, individuals or groups who support the idea, and a moderating organization (the \"platform\") that brings the parties together to launch the idea.Crowdfunding has been used to fund a wide range of for-profit, entrepreneurial ventures such as artistic and creative projects, medical expenses, travel, and community-oriented social entrepreneurship projects. Though crowdfunding has been suggested to be highly linked to sustainability, empirical validation has shown that sustainability plays only a fractional role in crowdfunding. Its use has also been criticized for funding quackery, especially costly and fraudulent cancer treatments."}},"key":"crowdfunding","order":["0"],"using":true},{"data":{"0":{"tag":"NP","wiki":"Content analysis is the study of documents and communication artifacts, which might be texts of various formats, pictures, audio or video. Social scientists use content analysis to examine patterns in communication in a replicable and systematic manner. One of the key advantages of using content analysis to analyse social phenomena is its non-invasive nature, in contrast to simulating social experiences or collecting survey answers.\nPractices and philosophies of content analysis vary between academic disciplines. They all involve systematic reading or observation of texts or artifacts which are assigned labels (sometimes called codes) to indicate the presence of interesting, meaningful pieces of content. By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts.\nComputers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Further, numerous computer-aided text analysis (CATA) computer programs are available that analyze text for pre-determined linguistic, semantic, and psychological characteristics."}},"key":"textual analysis","order":["0"],"using":true}],"level":1,"permission":"private","sections":{"0":{"original":{"state":1,"text":"Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects\u2019 fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdfunding success. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model\u2019s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project funding success chances.","title":"Abstract"},"summary":{"text":"Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects\u2019 fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdfunding success. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model\u2019s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project funding success chances.","title":"Abstract"}},"1":{"original":{"state":0,"text":"In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites [1]. The latter bring together entrepreneurs and supporters to facilitate project funding and launching. There are many crowdfunding websites, of which the most popular ones are Kickstarter and Indiegogo [2]. By 2020, 34 billion dollars were raised by crowdfunding [3]. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4]. In this paper, we refer to a project as a funding success (FS) if it achieved its funding goal (i.e., it raised at least 100% of the funding goal). With the growth in popularity and scale of crowdfunding, achieving success in funding has attracted much popular and scientific attention. Studies have examined properties of successful projects, developed models of such projects, and tried to predict success based on those models. For example [5], examined the type of funding requested and its effect on funding success. In [6], the authors examined the effect of social engagement on success. Work in [7] studied the meta-data features that explain funding success, and based on the meta-data features they developed a prediction model for funding success. While models in the art rely on metadata features and topic analysis for predicting crowdfunding success, they seldom consider semantic features. As we show, this lack negatively affects prediction accuracy. In this study we aim to improve prediction accuracy. To do that, we focus on extraction and analysis of semantic aspects of project posts. In contrast to prior studies, we take a more comprehensive approach to studying and predicting funding success. Unlike prior studies, we use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects\u2019 text. We use semantic features extracted from the text to build a predictive model of funding success. Further, to increase the accuracy of our results, we incorporate already known features that affect funding success. This approach leverages known results and improves upon them to provide high-quality funding success prediction. As we demonstrate, the introduction of semantics into funding success prediction indeed improves accuracy, thus justifying our method. Projects seeking crowdfunding range from one-time events to starting up small companies. Accordingly, these projects differ in terms of the requested investment amount and the type of compensation promised to investors. There are four primary types of crowdfunding [5,8,9]: 1. Reward crowdfunding: Investors contribute in return for non-financial benefits. 2. Debt crowdfunding: Investors fund projects as lenders and are paid interest on their investment. 3. Equity crowdfunding: Investors receive a return on investment by being offered private company securities. 4. Donation crowdfunding: This type is designed for charities or those who raise money for social or charitable projects, enabling them to gather an online community to donate to a project. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. Our second objective is to answer the following questions: Given a dataset of crowdfunding projects\u2019 details as input, consisting of text such as project description, non-text metadata such as project category, funding, etc., can we predict the success of projects in meeting their funding goals? At what level of accuracy? How would diverse learning algorithms affect this accuracy? Achieving these research objectives should provide new insights into the relationship between crowdfunding project\u2019s data and their funding success. In addition, this should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The study offers five main contributions. First, we demonstrate the important role of project\u2019s semantic properties in the project meeting its fundraising goals. To this end, we perform a comprehensive semantic analysis of the project\u2019s description, including topic analysis using LDA, word usage analysis using LIWC [10], and others. Second, we present a novel model which is based on semantic features only, referred to as semantic-model, that predicts the success of crowdfunding projects in meeting their funding goals. We show that the accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, the number of images, etc. Third, building upon our new insights regarding project semantics, we introduce an additional prediction model that combines semantics and project metadata, referred to as combined-model. The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11\u201314] Fourth, we show that semantic features including buzzwords and LIWC are among the highly correlate features with the project\u2019s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict funding success of crowdfunding projects. The dataset used included 160,125 project records from the two biggest crowdfunding websites. Even after filtering records of lesser relevance, 111,273 project records were still included in the analysis. This allows us to deliver a very accurate prediction model with very low error levels. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project funding success chances. These recommendations are based on the results of our study and of earlier studies as well. The semantic analysis introduced in this study is applicable beyond the specific domain examined here. Utilizing such analysis, one could study the effect of semantic properties of textual data on various target readerships. In our research, the textual data analyzed was the crowdfunding projects\u2019 description, and the target audience was individuals and groups that support the project. Another application domain could be, e.g., one where social network posts of an organization are analyzed to examine the level of support exhibited by its followers. The paper proceeds as follows. The Introduction section presents the state-of-the-art, the motivation for this study, and its objectives. The Background section presents background in crowdfunding as well as semantic analysis on which the study is built. The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results Section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies. Finally, in the Conclusion and Future Work section we summarize and discuss the main findings and point at future directions of our research.","title":"Introduction"},"summary":{"text":" In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a funding success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting funding success. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects\u2019 text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality funding success prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project\u2019s data and their funding success. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11\u201314] Fourth, we show that buzzwords and LIWC are among the highly correlate features with the project\u2019s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict funding success of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project funding success chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.","title":"Introduction"}},"2":{"original":{"state":0,"text":"In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects\u2019 textual data is required. Hence, a major data source used in this study is crowdfunding projects\u2019 posts. Such posts typically comprise multiple elements, including (but not limited to): title, subtitle, category, FAQ, project description, video, funding goal, time to achieve the goal, money raised, contact information, updates, backers, support ways and the names of the entrepreneurs or company. Our semantic analysis including topic analysis using LDA, word usage analysis using LIWC and novel features including the use of buzzwords, feeling words and explanation words.","title":"Background"},"summary":{"text":" In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects\u2019 textual data is required.","title":"Background"}},"3":{"original":{"state":0,"text":"The analysis we performed on posts examined their use of buzzwords [15]. Although some studies examined the use of buzzwords in project texts, the relationship between funding success and buzzwords used in the description of the project was not examined. This is a novel contribution of our research. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. Example buzzwords from the technology domain may be big data, cloud computing, opensource, etc. In our study, the buzzwords feature was the percentage contribution of the words in the buzzword dataset to each input text i.e., project\u2019s description.","title":"Buzzwords"},"summary":{"text":" The relationship between funding success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.","title":"Buzzwords"}},"4":{"original":{"state":0,"text":"To extract features from the text, we used the Linguistic Inquiry and Word Count (LIWC) software tool. LIWC includes text analysis software along with a group of built-in dictionaries [10]. Example of dictionaries are ( denotes stems): \u2022 Negative emotion: abuse, sorry, fury, tears, painf, tragic, etc. \u2022 Positive emotion: accept, funn, ador, pleasant, glad, etc. The LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. Specifically, the analysis works as follows. NT = the number words in the description text T and MD = {w1,..,wn}, the set of n words in dictionary D. Accordingly, let Nw = the number of occurrences of the word w from MD in T. SD = \u2211i = 1..n Nwi is the sum of Nw over all words w in MD. The output of LIWC is VD = SD/NT. This value is the value of the feature that corresponds to D. In practice, VD measures the frequency of appearance of words (and their stems) from a specific dictionary D in the examined text. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects (though they did not look for buzzwords) and arrived at high model accuracies (e.g., F-score > 0.8) [14]. The main difference between the present research and those studies is that the number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.","title":"Using Linguistic Inquiry and Word Count (LIWC)"},"summary":{"text":" We used the Linguistic Inquiry and Word Count (LIWC) software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.","title":"Using Linguistic Inquiry and Word Count (LIWC)"}},"5":{"original":{"state":0,"text":"Topic modeling methods are used in text mining tasks to extract topics from text. Latent Dirichlet allocation (LDA) is a widely used topic modeling [14]. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Within a topic to which it belongs, the word has a weight that expresses its importance in the context of that topic. A topic is expressed by the set of words belonging to it and their associated weights. The value of the topic will be the sum of those weights. Once the algorithm has a specified number of topics, it rearranges the distribution of the topics within the documents and the keyword distribution within the topics to obtain a good configuration of the topic\u2013keyword distribution [14]. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates, though on a much smaller and very specific dataset compared to our research [14]. Another study used LDA to analyze Chinese crowdfunding websites and arrived at an accuracy level (F-score) of 86.7%. In our study, we used LDA to extract features from project descriptions, where each topic is a feature. We used the Gensim Python package LDA implementation (from github.com). The latter is known to run faster than other implementations and it generates better topic segregation [16]. To execute LDA, we performed some preprocessing. First, we tokenized each sentence into a list of words, removing punctuation, unnecessary characters, email addresses, and names. Once this was completed, we defined functions that remove the stop words, which generate bigrams (two words frequently occurring together in the document) and perform lemmatization (the process of grouping together the inflected forms of a word so they can be analyzed as a single item). The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16]. To avoid overfitting, we varied the number of topics from 10 to 50 and chose the one that derived the highest coherence value and the smallest number of topics. We ran the LDA on three datasets (of which details are in the Methodology section) and derived three corresponding sets of topics. For each dataset, the input to the LDA was textual descriptions of the projects in that dataset. The output was a set of topics and the percentage contribution of each topic to each input text. In our study, every topic was considered as a feature for developing the prediction model, and the percentage contribution of a topic was the value of the feature that represented that topic. There were 30 topics that derived the highest coherence for the All_D dataset, 15 for the Tech_D dataset, and 15 for the Market_D dataset.","title":"Latent Dirichlet Allocation (LDA)"},"summary":{"text":" Latent Dirichlet allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.","title":"Latent Dirichlet Allocation (LDA)"}},"6":{"original":{"state":0,"text":"Additionally, we incorporated metadata features known to affect funding success. The metadata features we used were extracted from projects\u2019 posts via Python web scraping. The set we used for our analysis included the number of photos, the number of videos, the number of updates, the funding goal, the number of previously created projects by entrepreneur, the number of successfully completed (a.k.a. backed) entrepreneur\u2019s projects, and the time to achieve that goal (i.e., the project\u2019s deadline) [7,12,17\u201322].","title":"Metadata"},"summary":{"text":" We incorporated metadata features known to affect funding success. The metadata features we used were extracted from projects\u2019 posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.","title":"Metadata"}},"7":{"original":{"state":0,"text":"In addition to topics identified by LIWC and LDA, we explored additional topics that could have a high correlation to FS by reviewing those used by studies in the field of textual analysis. The following additional topics were deduced: \u2022 Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. [23]. \u2022 Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, etc. This bag of words comes from the unification of LIWC dictionaries that include feeling words [24].","title":"Additional semantic features"},"summary":{"text":" The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.","title":"Additional semantic features"}},"8":{"original":{"images":["https://api.leminda.com/summary/2841/img/10683.png","https://api.leminda.com/summary/2841/img/10684.png"],"state":0,"text":"We used the following metrics to measure the performance of our models. Precision is the ratio of correctly predicted positive records to the total predicted positive records. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model\u2019s precision and recall. These metrics  are calculated as follows: Precision \u00bc tp tp \u00fe fp Recall \u00bc tp tp \u00fe fn  F1 \u00bc 2 recall1 \u00fe precision1 \u00bc 2 precision recall precision \u00fe recall : Where tp is the number of true positive predictions. fn and fp are the number of false negatives and false positives predictions, respectively.","title":"Performance metrics"},"summary":{"images":["https://api.leminda.com/summary/2841/img/10683.png","https://api.leminda.com/summary/2841/img/10684.png"],"text":" We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model\u2019s precision and recall.","title":"Performance metrics"}},"9":{"original":{"images":["https://api.leminda.com/summary/2841/img/10685.png"],"state":0,"text":"In order to meet the objectives of this study, we have developed a knowledge discovery plan. The plan comprises four main parts: obtaining and preprocessing datasets; feature selection; data analysis based on machine learning models; and, finally, evaluation of the knowledge extracted by the models. We used a dataset of 50,000 Kickstarter and 50,000 Indiegogo projects. We obtained these data from the Kaggle website for 2018. Note that similarly to other studies in the field, we filtered out projects with fewer than 10 lines of description. The goal of such filtering is to avoid compromising the accuracy of the semantic analysis [14]. We also omitted from the dataset projects whose completion due date was still in the future. After filtering, we had 73,583 projects: 42,530 that achieved their funding goal (success = 1), and 31,050 projects that had not managed to achieve their funding goal (success = 0). After filtering the dataset, we used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, LIWC outputs, feelings words, explanation words, and LDA outputs. The values of the features were computed as described above. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets. Whenever there was an overlap, LDA\u2019s topic X was subsumed into LIWC\u2019s dictionary/topic. We preprocessed the dataset via common methods (for example, conversion of an ordinal variable to a numerical one or dataset normalization) to adjust the data to the input requirements of the data analysis algorithms we used for deriving the model. To study the relationship between features and the project category, we built three datasets: \u2022 All_D: comprising all projects regardless of their category. \u2022 Tech_D: comprising only technological projects. \u2022 Market_D: comprising only projects on marketing and business. This division aims to address the following analysis goals. First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with funding success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains (most buzzwords we used are indeed in these sub-domains). We utilized MANOVA (multivariate analysis of variance) to examine the similarity among the datasets. The results show that the separation by category causes statistical significance of the mean differences between the features using p-value = 0.05. We computed the Pearson correlation between all of the features we studied and the FS. All of the models and the correlation values were computed, separately, with respect to the three datasets. To increase comprehensibility, we wanted to visualize the feature space and its general data structure. For this, we had to reduce the multi-dimensional space to a three-dimensional space. Thus, we utilized Principal Component Analysis (PCA) to identify the most influential features [25,26]. Following this analysis, for each dataset, the original features space was reduced into three Principal Components (PCs), as visualized in Fig 1. The sum of the explained variance ratio of these three PCs is 78% of the total variance.  Fig 1. PCA on datasets. The PCA plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Each dot in the plots represents a project record in the dataset. Dots in blue represent unsuccessfully funded projects and dots in red represent successfully funded projects. Most of the records of the successfully funded projects are located in the range where PC3 < 0.5, -1 < PC2 < 0, 1 < PC1 < 2. To identify the most significant set of features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. CFS evaluates a subset of features by considering the individual predictive ability of each feature along with the degree of redundancy between them. We executed this algorithm on each dataset, separately. In this study, as mentioned above, we developed two binary classification models to predict funding success. The labels of the two classes are 1 and 0, where 1 is the label of the class of successfully funded projects and 0 is the label of the class of unsuccessfully funded projects. To develop those models, we utilized widely used classification algorithms (Random forest, decision trees, DNN) as well as state-of-the-art leading algorithms such as LightGBM [27]. The models\u2019 design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikit-feature and LightGBM. Recall that one of our goals was to develop the semantic-model whose accuracy is comparable to the accuracy of existing models. To this end, we developed a model whose features are only semantic features. The inputs of that model consist of buzzwords, explanation words, feeling words and the topics derived from LIWC and LDA. Once the semantic-model was developed and tested, we aimed to further improve model accuracy. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features. For each of the two models, we calculated the F-score as a measure of model\u2019s accuracy. We employed a 10-fold cross-validation test to evaluate the prediction performance. This test has been used widely to validate the performance of models similar to ours [6,21,28]. For each fold, our data sample was randomly divided into 10 parts, then 10 experiments were performed, with nine parts used as training data for the predictive model to predict the remaining part. For each fold, we trained the LDA algorithm on the training data. The average prediction accuracy is reported. Our goal was to achieve an accuracy level greater than 90%. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model\u2019s accuracy to previous research models by applying their methods on our dataset.","title":"Methodology"},"summary":{"images":["https://api.leminda.com/summary/2841/img/10685.png"],"text":" We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, LIWC outputs, feelings words, explanation words, and LDA outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with funding success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components (PCs) The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most significant set of features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The PCA plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models\u2019 design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model\u2019s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.","title":"Methodology"}},"10":{"original":{"images":["https://api.leminda.com/summary/2841/img/10686.png"],"state":1,"text":"Fig 2. Methodology flowchart.","title":"Results"},"summary":{"images":["https://api.leminda.com/summary/2841/img/10686.png"],"text":"Fig 2. Methodology flowchart.","title":"Results"}},"11":{"original":{"images":["https://api.leminda.com/summary/2841/img/10687.png"],"state":0,"text":"Table 1. Feature correlation with funding success. In this section, we provide details of the data setup, the usage of the LDA algorithm, the feature correlation analysis, the feature selection process, and the development of a prediction model. Features that most significantly influence funding success. In what follows we describe the way in which we found the features that are most influential for FS. We used two methods. The first is the Pearson correlation between the independent features and the dependent feature (i.e., the FS). The second is the CFS algorithm. Features that highly correlate with funding success. Below in Table 1, for each of the three datasets, we list the 10 features for which the highest correlation values to FS were derived. The semantics of features annotated with are provided in LIWC documentation (R_All_D stands for Ranked features from All_D). Feature selection. We used the CFS algorithm to find MSSF in every dataset. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them. The following are the algorithm\u2019s results. MSSF from All_D (S_ All_D): \u2022 achieve \u2022 updates \u2022 punc (all punctuation) \u2022 emoticon \u2022 funding goal \u2022 backed (# of backed projects) \u2022 project duration MSSF from Tech_D (S_ Tech_D): \u2022 backed \u2022 funding goal \u2022 buzz_num \u2022 updates \u2022 FAQ (# of FAQ in project\u2019s page) \u2022 project duration MSSF from Market_D (S_ Market_D): \u2022 funding goal \u2022 updates \u2022 buzz_num \u2022 work \u2022 created (# of created projects) \u2022 punc From the above results, we conclude that there are various semantic features that have a high correlation to FS among the features we checked based on the project\u2019s category. Correlation values similar to those computed in our study are typical in the art [12,19]. The semantic features buzzwords and LIWC are among the features that have a higher correlation in all datasets. When inspecting the correlation analysis results, we can observe that among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive in the R_Market_D top 10 list. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category.","title":"Feature extraction and selection"},"summary":{"images":["https://api.leminda.com/summary/2841/img/10687.png"],"text":" In this section, we provide details of the data setup, the usage of the LDA algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for FS. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and LIWC are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.","title":"Feature extraction and selection"}},"12":{"original":{"images":["https://api.leminda.com/summary/2841/img/10688.png"],"state":0,"text":"Semantic model. For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the Semantic-Model: Combined model. To develop the combined-model, we utilized the same classification algorithms as used for the development of the Semantic-Model on our three datasets, using all features. Below we list the best models (by F-score). We used the grid search algorithm to optimize the hyperparameters with which each algorithm was executed (for example, for SDG we optimized batch size, number of epochs, etc.). In addition to the grid search algorithm, we utilized the Random search algorithm and got similar accuracies for our classification models. The grid search algorithm selected a set of hyperparameters, and these were used for training our model. Specifically, the following hyperparameters were used for All_D dataset: \u2022 SDG: batch size = 100, epochs = 500, learning rate = 0.01, loss function = hinge loss (SVM), lambda (regularization constant) = 0.0001. \u2022 Random forest: number of trees = 20, max depth = infinity, max number of features = unlimited. \u2022 LightGBM: number of leaves = 255, number of boosting iterations = 5000, Minimal number of data in one leaf = 10, learning rate = 0.01, max number of bins = 300. As seen above, the semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. Further, as shown in  Table 3. All_D performance. Table 3, by combining the semantic model with the metadata model, we have achieved an accuracy level of 96.2%. Additionally, as seen across Tables 3\u20135, the LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, with LightGBM, the accuracy of the prediction model is greater than 94% for all datasets.","title":"A model for predicting funding success"},"summary":{"images":["https://api.leminda.com/summary/2841/img/10688.png"],"text":" For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.","title":"A model for predicting funding success"}},"13":{"original":{"state":0,"text":"To better understand the significance of our results, we compared them to results of earlier studies. To this end, we implemented two models from earlier studies and trained them on the All_D dataset to compare their accuracy to that of the Sematic-Model and the CombinedModel. In the first study [14], developed their best model (by means of F-score accuracy metrics) and trained it on 80 LDA features that were extracted from project descriptions. Relying on their method, we tried different numbers of topics in the range of 5 to 100. We did the same for each textual corpus and observed the corresponding perplexity scores. Using perplexity, we found 30 topics to be the best fit. We denote the model that trained on those features LDA-Model. In the second study [18], utilized only metadata features to develop their prediction model. We denote the model that trained on those features Metadata-Model. Our study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. To this end, we examined the influence of the learning algorithms, the feature sets and the dataset on the derived F-score. Accordingly, we trained the LDA-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation. This extensive experimental trial led to the conclusion that neither the learning algorithms nor the dataset are a source of significant differences in F-score. The major effector of such differences is the set of features. Fig 3 presents the highest F-score metric values of the LDA-Model, the Metadata-Model, the Semantic-Model and the CombinedModel. F-score is used for consistency with the earlier studies to which we compare. The Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to the accuracy of the LDA-Model and higher than the accuracy of the Metadata-Model.","title":"Comparing our model to earlier models"},"summary":{"text":" The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the LDA-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.","title":"Comparing our model to earlier models"}},"14":{"original":{"images":["https://api.leminda.com/summary/2841/img/10689.png","https://api.leminda.com/summary/2841/img/10690.png"],"state":0,"text":"In recent years, crowdfunding platforms such as Kickstarter and Indiegogo have been offering entrepreneurs the possibility to present their projects and attract funders, and thus raise the funds necessary for their projects. The question of how different properties of the project\u2019s presentations can increase the chance of successful project funding is important. While previous studies have identified some metadata features and topic analysis for predicting crowdfunding  Table 5. Market_D performance. success, little research was conducted to explore the semantic features in this context. In this study, we addressed this void. We focused on analysis of semantic aspects of project posts to improve the accuracy of funding success predictions. We designed a text analytics framework and developed a prediction model for analyzing and predicting crowdfunding success. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. To the best of our knowledge, this study is the first that investigates the relationship between funding success and buzzwords. We show that the buzzwords feature is among the features that are highly correlated to funding success compared to both the parameters that we examined and that other researchers examined. We have additionally shown that the features correlated to FS are dependent on project category. From a practical perspective, the results of our study are highly relevant to fundraisers using crowdfunding web platforms. In addition to the scientific contributions listed above, a set of recommendations that may increase project funding success chances can be proposed. These recommendations are based on the features we listed above, and on influential features from previous studies. We have shown that the category of the project influences the features that have a high correlation with funding success. We offer (example) recommendations for technological projects, as follows: \u2022 Entrepreneurs should update their project information during the funding period. \u2022 The project\u2019s post should contain more feelings words. \u2022 The project\u2019s description should contain buzzwords.  Fig 3. Comparison of models\u2019 accuracy. While this paper arrived at very high prediction accuracies of funding success, future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model\u2019s performance could be achieved via novel feature selection algorithms, e.g., [29].","title":"Conclusion and future work"},"summary":{"images":["https://api.leminda.com/summary/2841/img/10689.png","https://api.leminda.com/summary/2841/img/10690.png"],"text":" The study is the first that investigates the relationship between funding success and buzzwords. The buzzwords feature is among the features that are highly correlated to funding success compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model\u2019s performance could be achieved via novel feature selection algorithms.","title":"Conclusion and future work"}},"15":{"original":{"state":2,"text":"Methodology: Orel Babayoff, Onn Shehory. Writing \u2013 original draft: Orel Babayoff, Onn Shehory.","title":"Author Contributions"}}},"source":[""],"stats":{"length":{"original":5036,"summary":1890},"quality":{},"readability":{"original":13.0,"summary":13.0},"time":{"reading_time_original_text":14,"reading_time_saved":9,"reading_time_summary_text":6,"reduction_percentage":61}},"text_id":"63aad5688d85ebca948f9fe9","title":"The role of semantics in the success of crowdfunding projects","topic":"Computer science","ts":1672140191,"user":"6316144a32fa3e3614c02e2f","userRecord":{"family_name":"\u05e7\u05d9\u05d9\u05d6\u05e8","given_name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3","name":"\u05e0\u05e2\u05dd \u05d3\u05d5\u05d3 \u05e7\u05d9\u05d9\u05d6\u05e8"}}
