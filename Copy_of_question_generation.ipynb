{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noamKayzer/Questions_generation/blob/main/Copy_of_question_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aweZgxXBDsOQ",
        "outputId": "844d705d-a67b-4f0f-a81e-0423288eb2bf"
      },
      "outputs": [],
      "source": [
        "%pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FARi6xuQ4IZ",
        "outputId": "9194e4e6-1f8d-4864-daff-f0972c13ebf3"
      },
      "outputs": [],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFldiKn8EIp5",
        "outputId": "83fc620d-aa4d-470a-8c01-58f1a6a6e04f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'question_generation'...\n",
            "remote: Enumerating objects: 268, done.\u001b[K\n",
            "remote: Total 268 (delta 0), reused 0 (delta 0), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (268/268), 299.04 KiB | 14.95 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/patil-suraj/question_generation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC8YvSLRE56B",
        "outputId": "03740481-caac-42e9-b44c-0c8d1d6450a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 16.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAOgL63nEKIx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk3X75xXRH5r"
      },
      "outputs": [],
      "source": [
        "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \\\n",
        "and first released in 1991, Python's design philosophy emphasizes code \\\n",
        "readability with its notable use of significant whitespace.\"\n",
        "\n",
        "text2 = \"Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all \\\n",
        "things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) \\\n",
        "one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. \\\n",
        "The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing \\\n",
        "and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of \\\n",
        "the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly \\\n",
        "weaker as objects get further away\"\n",
        "\n",
        "text3 = \"42 is the answer to life, universe and everything.\"\n",
        "\n",
        "text4 = \"Forrest Gump is a 1994 American comedy-drama film directed by Robert Zemeckis and written by Eric Roth. \\\n",
        "It is based on the 1986 novel of the same name by Winston Groom and stars Tom Hanks, Robin Wright, Gary Sinise, \\\n",
        "Mykelti Williamson and Sally Field. The story depicts several decades in the life of Forrest Gump (Hanks), \\\n",
        "a slow-witted but kind-hearted man from Alabama who witnesses and unwittingly influences several defining \\\n",
        "historical events in the 20th century United States. The film differs substantially from the novel.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "intro = \"In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The latter bring together entrepreneurs and supporters to facilitate project funding and launching. There are many crowdfunding websites, of which the most popular ones are Kickstarter and Indiegogo. By 2020, 34 billion dollars were raised by crowdfunding. The average success rate of funded crowdfunding projects in Kickstarter is 37.5%. In this paper, we refer to a project as a funding success (FS) if it achieved its funding goal (i.e., it raised at least 100% of the funding goal). With the growth in popularity and scale of crowdfunding, achieving success in funding has attracted much popular and scientific attention. Studies have examined properties of successful projects, developed models of such projects, and tried to predict success based on those models. For example, examined the type of funding requested and its effect on funding success. In, the authors examined the effect of social engagement on success. Work in studied the meta-data features that explain funding success, and based on the meta-data features they developed a prediction model for funding success. While models in the art rely on metadata features and topic analysis for predicting crowdfunding success, they seldom consider semantic features. As we show, this lack negatively affects prediction accuracy. In this study we aim to improve prediction accuracy. To do that, we focus on extraction and analysis of semantic aspects of project posts. In contrast to prior studies, we take a more comprehensive approach to studying and predicting funding success. Unlike prior studies, we use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. We use semantic features extracted from the text to build a predictive model of funding success. Further, to increase the accuracy of our results, we incorporate already known features that affect funding success. This approach leverages known results and improves upon them to provide high-quality funding success prediction. As we demonstrate, the introduction of semantics into funding success prediction indeed improves accuracy, thus justifying our method.\"\n",
        "results = \"In recent years, crowdfunding platforms such as Kickstarter and Indiegogo have been offering entrepreneurs the possibility to present their projects and attract funders, and thus raise the funds necessary for their projects. The question of how different properties of the project’s presentations can increase the chance of successful project funding is important. While previous studies have identified some metadata features and topic analysis for predicting crowdfunding success, little research was conducted to explore the semantic features in this context. In this study, we addressed this void. We focused on analysis of semantic aspects of project posts to improve the accuracy of funding success predictions. We designed a text analytics framework and developed a prediction model for analyzing and predicting crowdfunding success. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive F-score of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. To the best of our knowledge, this study is the first that investigates the relationship between funding success and buzzwords. We show that the buzzwords feature is among the features that are highly correlated to funding success compared to both the parameters that we examined and that other researchers examined. We have additionally shown that the features correlated to FS are dependent on project category. From a practical perspective, the results of our study are highly relevant to fundraisers using crowdfunding web platforms. In addition to the scientific contributions listed above, a set of recommendations that may increase project funding success chances can be proposed. These recommendations are based on the features we listed above, and on influential features from previous studies. We have shown that the category of the project influences the features that have a high correlation with funding success. We offer (example) recommendations for technological projects, as follows:Entrepreneurs should update their project information during the funding period. The project’s post should contain more feelings words. The project’s description should contain buzzwords.  While this paper arrived at very high prediction accuracies of funding success, future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.\"\n",
        "\n",
        "\n",
        "roelfsema_paper_abstract = \"Much recent work has focused on biologically plausible variants of supervised learning algorithms. However, there is no teacher in the motor cortex that instructs the motor neurons and learning in the brain depends on reward and punishment. We demonstrate a biologically plausible reinforcement learning scheme for deep networks with an arbitrary number of layers. The network chooses an action by selecting a unit in the output layer and uses feedback connections to assign credit to the units in successively lower layers that are responsible for this action. After the choice, the network receives reinforcement and there is no teacher correcting the errors. We show how the new learning scheme – Attention-Gated Brain Propagation (BrainProp) – is mathematically equivalent to error backpropagation, for one output unit at a time. We demonstrate successful learning of deep fully connected, convolutional and locally connected networks on classical and hard image-classification benchmarks; MNIST, CIFAR10, CIFAR100 and Tiny ImageNet. BrainProp achieves an accuracy that is equivalent to that of standard error-backpropagation, and better than state-of-the-art biologically inspired learning schemes. The trial-and-error nature of learning is associated with limited additional training time so that BrainProp is a factor of 1-3.5 times slower. Our results thereby provide new insights into how deep learning may be implemented in the brain.\"\n",
        "\n",
        "roelfsema_paper_conclusion = \"We conclude that the present and related work on biologically plausible learning is starting to bridge the gap between learning in machines and in the brain. Insights from the machine learning and neuroscience fields is contributing to a genuine understanding of learning in the brain, with its many processing stages between sensory neurons and the motor neurons that ultimately control behavior\"\n",
        "roelfsema_paper_intro1 = \"Artificial neural networks with many layers of neural units now attain human-level performance in speech and image recognition and in complex computer games like Chess, Go and Starcraft. Central to this success is the application of the error-backpropagation algorithm (EBP) to efficiently assign credit to individual weights between the layers in deep networks. EBP computes the error gradient for connections between all the layers. Can the brain, with its hierarchically arranged cortical areas, solve the credit assignment problem in a similar manner?\"\n",
        "roelfsema_paper_intro2 = \"Recent studies proposed versions of EBP for the brain that required a teacher, studying the feasibility of asymmetric feedback networks and developing variants of equilibrium propagation, reviewed in X papers. In equilibrium propagation , activity is first 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. propagated to the output layer, after which the output units are nudged in the direction of the desired target values. Feedback connections then propagate the perturbed signals back to lower layers, which can then compare activity before and after teacher intervention to estimate the error gradient for all connections. The present work addresses a number of limitations of these previous schemes. First, it avoids a teacher and replaces it by trial-and-error learning in a reinforcement learning context. Second, the scheme takes the neuromodulatory systems in the brain, such as dopamine, into account that provide a reward prediction error signal (RPE). The RPE is positive if an action selected by the network is associated with more reward than expected or if the prospects of receiving reward increase, and the RPE is negative if the outcome of the action is disappointing. These neuromodulatory signals are accessible for all neurons in the brain. Third, it uses feedback connections to propagate attentional “credit assignment” signals from higher to lower network levels which are known to gate plasticity. When a network chooses an action, the feedback signal is strongest for neurons and synapses that are responsible for the selected action. We show how this combination of the attentional feedback signals and the neuromodulatory influence can be combined at the level of a synapse to provide a learning scheme that is equivalent to EBP, for one output unit at a time. The result is a biologically plausible synaptic update rule. The learning scheme requires weight symmetry, which can occur as the result of the BrainProp learning scheme itself. Furthermore, a previous study demonstrated that the learning scheme can also function in spite of delays between the signals propagated by feedforward and feedback connections.\"\n",
        "\n",
        "news_article = \"A strong majority of Americans continue to support sending arms and economic aid to Ukraine, according to a poll released yasterday. But as the conflict drags into winter, Americans are divided over whether Washington should push Ukraine to reach a negotiated peace as soon as possible. More than two-thirds of respondents back supplying Ukraine with weapons and economic assistance, and about three-quarters support accepting Ukrainian refugees and sanctioning Russia, according to the survey conducted by the Chicago Council on Global Affairs last month. While support among the American public for assistance to Ukraine remains robust, Republican backing for aid to Ukraine has slipped since the spring, with 55 percent of Republicans saying they support sending military aid, compared with 68 percent in July and 80 percent in March. Half of Republicans favored providing economic assistance to Ukraine last month, compared with roughly three-quarters in March, according to the Chicago Council’s findings. The United States announced its latest tranche of military aid to Ukraine last month — the 25th since August 2021. The $400 million package includes additional arms, munitions and equipment, the Defense Department said, and brings total U.S. military assistance to Ukraine to nearly $20 billion since President Biden took office. The United States is also sending $53 million to help repair Ukraine’s electrical systems, which have sustained significant damage from Russian missile strikes in recent weeks. With Russia’s war in Ukraine in its 10th month, and no end in sight, Americans are split over whether Washington should urge Ukraine to reach a peace settlement with Russia imminently, the survey found. A plurality — 40 percent — said the United States should continue its current levels of support to Ukraine indefinitely. Fifty-three percent of Democrats favor this approach. In July, however, 58 percent of American respondents said the United States should help Ukraine for as long as it takes, even if that meant higher gas and food prices for American consumers. Now, 47 percent say Washington should push Kyiv to reach a peace settlement soon.\"\n",
        "\n",
        "mindfulness_paper = \"Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population's quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \\\"from moment to moment,\\\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The first successful studies on mindfulness examined patients suffering from chronic pain and tension. These first studies encouraged the development of mindfulness-based stress reduction (MBSR) for this population. At a later stage, mindfulness-based cognitive therapy (MBCT) was developed. MBCT is a therapeutic intervention based on MBSR. However, it differs from it in its focus on depression and in its use of a dominant psycho-educational feature that focuses on the prevention of relapse. These two standard interventions comprise eight weekly sessions averaging 2.5 hours. A meta-analysis of 64 studies found that mindfulness-based interventions are effective in coping with chronic pain, fibromyalgia, coronary artery diseases, cancer, anxiety disorders, depression, and contextual stressors. Several theoretical underpinnings have been suggested to explain the mechanism behind the psychological-therapeutic effect accompanying the methodological practice of mindfulness. These theoretical explanations have gained broad empirical support from meta-analyses on meditation research. One theoretical explanation suggests that focusing attention and responding with acceptance and non-judgment regarding life experiences improves attentional and self-regulatory capacities. Another explanation is that the suggestion to accept any thoughts and emotions without avoidance encourages a diminished use of repression and assists in arousing autobiographical memories from specific places and times, both of which are of considerable value to mental health treatment. A third theoretical explanation suggests that mindfulness helps diminish negative ruminative thinking by bringing negative thoughts to consciousness and decreasing reactivity to them. Mindfulness also appears to influence emotional and cognitive flexibility and self-perception changes. The current study aimed to expand our understanding and knowledge of mental health in old age and to highlight its importance. The senior population is continuously challenged with maintaining and advancing its well-being. Data from a U.S. Census study that included 12,312 adults 64 years of age or older suggested that 11.4% of older adults cope with anxiety disorders and 6.8% cope with emotional disorders, all of which met the clinical criteria for diagnosis. It is important to note that by including symptoms outside the clinical criteria for diagnosis, the percentage of depression and anxiety increases significantly in the elderly, more than in any other age group. Anxiety and the decline of one's physical state are cyclically associated. Anxiety encourages avoidance behavior, which promotes muscle weakening and pathology of the muscular and skeletal systems, which then foments further atrophy. A meta-analysis on loneliness in old age identified general themes explaining and detailing the experience of loneliness in old age: the loss of significant interpersonal relationships, the negative influence of loneliness on self-perceptions, and negative emotions associated with loneliness, such as anxiety, sadness, and anger. These themes were found to be reciprocally associated with one another. The biopsychosocial model describes the mutual influence of the individual's physical, mental, and social state, positing a more significant interrelation in the elderly than in younger individuals. This model suggests that a chronically stressful environment significantly increases vulnerability to illness and the loss of functional capacities, which, in turn, increase anxiety and a sense of helplessness. Another example relates to chronic illnesses in old age, such as anxiety, depression, and bipolar disorder, which are all associated with a higher risk for suicidal behavior in old age. Aligning with Garroway and Rybarczyk's model, depression and anxiety were found to encourage cognitive biases that act as psychological barriers which potentially delay or prevent adaptive behavior in response to challenges in old age. For example, elderly persons may eschew the use of a walking stick, as it symbolizes a deteriorating physical state and some level of submission––experienced as symbolic threats to one's identity and ego. The increased vulnerability to emotional distress in old age highlights the critical need for effective short-term interventions that can quickly improve seniors' health and quality of life.\"# A study on elderly participants that examined the efficacy of a brief, 4-week behavioral intervention found significant improvement in various psychological and cognitive measures15. Another study examined a brief 4-week behavioral intervention intended to assist seniors with chronic sleep difficulties and found significant improvement in the sleep quality of the participants in the intervention group. Only a few research studies have examined the influence of mindfulness on measures of anxiety and depression in the elderly. For example, a meta-analysis examining existing mindfulness-based interventions and their influence on mental and physical well-being in the elderly included only four studies. Two of these studies examined the influence of a standard, 8-week MBCT intervention, finding decreases in anxiety and depression measures with a moderate-to-high effect size. The other two studies examined a standard 8-week MBSR intervention, finding a decrease in measures of anxiety and depression with a high effect size. None of these four studies compared the experimental group with a control group that underwent a different treatment intervention. Three of the studies offered no comparison between the experimental group and a control group. An MBCT study on elderly individuals, incorporating an activity-based control treatment group and whose experimental group was exposed to a shortened intervention (1.5 h), also found a decrease in measures of anxiety and had a high effect size. Although these interventions demonstrated their efficacy, the various interventions were characterized by high attrition rates (about 15%). The primary sources of these attrition rates were gender (men discontinued their participation more than women), a few years of education, expectations for a quick change, and high levels of distress. A meta-analysis by Carmody and Baer suggested a solution for these high attrition rates, which examined 25 studies that implemented mindfulness-based interventions of various lengths and age groups. They believed that the high number of sessions made perseverance during the sessions problematic. They did not offer any evidence to suggest that abridged versions of mindfulness interventions are less effective than the standard intervention in diminishing emotional distress; however, they suggested shortening mindfulness interventions to make them more accessible for population groups (i.e., the elderly) that have difficulties committing to standard interventions. Another popular intervention for depression and anxiety in the elderly is CBT. As its name suggests, CBT aims to modify the thoughts and behaviors that cause distress to the patient while cultivating thoughts and behaviors that are more effective for the patient. CBT's' basic assumption is that emotional distress results from how individuals perceive and interpret their reality and is not necessarily the consequence of objective reality. CBT, therefore, places significant emphasis on solving current issues. A review of multiple meta-analyses examining the efficacy of CBT for anxiety and depression in the elderly found significant efficacy, as displayed by the higher than average effect sizes for CBT. It is important to note that alongside the many benefits of these interventions and the many and varied solutions mental health treatment offers the elderly, this population is less likely to seek mental health services than younger age groups1. Three factors were found to have the most significant impact on elderly persons' attitudes toward seeking treatment: (1) their perception of their aging process, (2) their perception of psychologists, and (3) their perception of social support. Furthermore, stigma––a distinguishing characteristic or trait that disqualifies those that carry it from full social acceptance––is one of the central factors for why the elderly avoid seeking mental health treatment. Considering elderly persons' distancing themselves from mental health services, the need for short-term interventions to help modify the cognitive perceptions associated with attitudes toward seeking mental health treatment becomes apparent. The appropriate intervention should contribute to increasing the utilization of therapy by the elderly, whether individual or group therapy. As noted, mindfulness-based interventions assist individuals in accepting their current thoughts and emotions non-judgmentally. In this way, the mindfulness-practicing individual observes or contemplates his thoughts instead of giving vent to an emotional reaction to them. Introspection helps create mental and emotional distance between the individual and their thoughts, facilitating, among other things, a more flexible approach to the individual's thoughts, including non-adaptive perceptions. The current study aimed to contribute to the existing literature in two ways: (1) Establishing support for a short-term mindfulness-based intervention for seniors to be effective in reducing age-related anxiety and depression and encouraging seniors' utilization of mental health treatment; (2) Comparing a mindfulness-based intervention with an alternative treatment intervention (CBT). Mindfulness-based interventions that have been demonstrated to reduce anxiety and stress in the elderly often include breathing exercises. Further, behavioral relaxation techniques such as diaphragmatic breathing are most effective in reducing anxiety and stress in the elderly. To our knowledge, research has yet to examine the influence of mindfulness on anxiety in the elderly relative to a control group that isolates the effects of diaphragmatic breathing (a potential confounding variable). For this reason, it is critical to isolate and examine the unique attentional component of mindfulness-based interventions. Similarly, to our knowledge, research has yet to examine the influence of mindfulness on attitudes toward seeking mental health treatment among the elderly. Thus, another practical goal of this study was to examine an easily deliverable and accessible group intervention in a nursing home. In the wake of the COVID-19 crisis, it has become clear that efforts should be made to bring therapy to where the people are and not rely on inviting them to professional center .\"\n",
        "\n",
        "COMES_sections = ['COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.',\n",
        "                  ' Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.',\n",
        "                  ' For a comprehensive survey on the summary evaluation resources see Koto et al. and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task – based on question answering, similarity between summary and reference embeddings.',\n",
        "                  ' COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jE2pMLRj1sP-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('summery_example.json') as f:\n",
        "  result = json.load(f)\n",
        "sections = [s['summary']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ]\n",
        "min_words_in_section=60\n",
        "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvBFbCVGELuW"
      },
      "source": [
        "## Single task QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy_T1CiVVNuH"
      },
      "outputs": [],
      "source": [
        "%pip install pandas numpy scikit-learn\n",
        "%pip install spacy\n",
        "%pip install torch\n",
        "%pip install tqdm\n",
        "%pip install nbformat\n",
        "%pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "import plotly.io as pio\n",
        "\n",
        "# Set the default renderer to the Jupyter notebook environment\n",
        "pio.renderers.default = \"jupyterlab\"\n",
        "# Create a Plotly plot\n",
        "fig = px.scatter(x=[1, 2, 3], y=[1, 2, 3])\n",
        "\n",
        "# Use the .show() method to display the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxoSS2_WEMvx",
        "outputId": "43c0b171-8895-4b12-fe2e-6dc00c2889c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<scispacy.abbreviation.AbbreviationDetector at 0x14eb67541e80>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#from pipelines import pipeline\n",
        "try:\n",
        "  from transformers import AutoTokenizer,AutoModel,T5Tokenizer,T5ForConditionalGeneration\n",
        "except:\n",
        "  %pip install transformers\n",
        "  %pip install -U sentencepiece \n",
        "  from transformers import AutoTokenizer,AutoModel,T5Tokenizer,T5ForConditionalGeneration\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "try:\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "except:\n",
        "  %pip install sentence_transformers\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentence_model = SentenceTransformer('all-mpnet-base-v2')#'all-MiniLM-L6-v2')\n",
        "\n",
        "import torch\n",
        "import plotly.express as px\n",
        "from collections import Counter\n",
        "\n",
        "model = \"google/flan-t5-large\"#\"google/flan-t5-large\"\n",
        "import torch\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model).to(device)\n",
        "'''QA_model_name = \"allenai/unifiedqa-v2-t5-large-1363200\" # you can specify the model size here\n",
        "QA_tokenizer = T5Tokenizer.from_pretrained(QA_model_name)\n",
        "QA_model = T5ForConditionalGeneration.from_pretrained(QA_model_name)'''\n",
        "\n",
        "sections_ranks=[0.27965833333333334,\n",
        "0.28353125,\n",
        "0.28348,\n",
        "0.2847533333333333,\n",
        "0.28539,\n",
        "0.28317714285714285,\n",
        "0.28727,\n",
        "0.28658285714285714,0.2793,0.287516,0.28511333333333333]\n",
        "\n",
        "import spacy\n",
        "try:\n",
        "  from scispacy.abbreviation import AbbreviationDetector\n",
        "except:\n",
        "  !pip install scispacy\n",
        "  \n",
        "  from scispacy.abbreviation import AbbreviationDetector\n",
        "try:\n",
        "  abrv_nlp = spacy.load(\"en_core_sci_sm\")\n",
        "except:\n",
        "  !pip install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
        "  abrv_nlp = spacy.load(\"en_core_sci_sm\")\n",
        "\n",
        "from scispacy.hyponym_detector import HyponymDetector\n",
        "\n",
        "abrv_nlp.add_pipe(\"hyponym_detector\", last=True, config={\"extended\": False})\n",
        "\n",
        "# Add the abbreviation pipe to the spacy pipeline.\n",
        "abrv_nlp.add_pipe(\"abbreviation_detector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fWpzr1RxVpgc",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_output_from_prompt(model,prompt,args):\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "  \n",
        "  res = model.generate(input_ids, **args)\n",
        "  output = tokenizer.batch_decode(res['sequences'], skip_special_tokens=True)\n",
        "  output = [item.split(\"<sep>\") for item in output]\n",
        "  if all([len(cur_sen)==1 for cur_sen in output]):  \n",
        "      output = [cur_sen[0] for cur_sen in output]\n",
        "  output =[qs.strip() for qs in output]\n",
        "  output = [*Counter(output)] #remove exactly similar questions if exist\n",
        "  # Data originally sorted by probabilities, multiply be the penalities activated (`length_penalty` and/or `diversity_penalty`).\n",
        "  # We want it to be sorted by the perplexity (PPL) score, the coherence of the sentence - probabilty normlized by the length of the sentence. \n",
        "  ppl = [np.exp(np.array(log_likelihoods.cpu()).mean() / np.array(len(cur_output.split())).mean()) for log_likelihoods,cur_output in zip(res['sequences_scores'],output)]\n",
        "  sorted_idx  = np.argsort(ppl)\n",
        "  return [output[id] for id in sorted_idx], [ppl[id] for id in sorted_idx]\n",
        "\n",
        "#\n",
        "#input_string = \"ask question and answer: \" + results + \" </s>\"\n",
        "#input_string = \"generate questions: \" + results\n",
        "\n",
        "\n",
        "def find_similarity(questions):\n",
        "  #Sentences are encoded by calling model.encode()\n",
        "  if len(questions)>=1 and all([len(cur_sen)==1 for cur_sen in questions]):  \n",
        "      questions = [cur_sen[0] for cur_sen in questions]\n",
        "  embeddings = sentence_model.encode(questions)\n",
        "  similarity_matrix = np.dot(embeddings,embeddings.T).round(3)\n",
        "  return similarity_matrix\n",
        "\n",
        "def plot_similarity_matrix(questions_list):\n",
        "  #x_label,y_label = ['']*questions_list.shape[0],['']*questions_list.shape[0]\n",
        "  if isinstance(questions_list,list):\n",
        "    if len(questions_list)>=1 and all([len(cur_sen)==1 for cur_sen in questions_list]):\n",
        "      questions_list = [cur_sen[0] for cur_sen in questions_list]\n",
        "      similarity_mat = find_similarity(questions_list)\n",
        "  if not isinstance(questions_list,np.ndarray):\n",
        "    similarity_mat = find_similarity(questions_list)\n",
        "  elif isinstance(questions_list,np.ndarray) and questions_list.dtype.type is np.object_:\n",
        "    similarity_mat = find_similarity(questions_list)\n",
        "  else:\n",
        "    similarity_mat = questions_list\n",
        "  x_label,y_label = questions_list,questions_list\n",
        "  fig = px.imshow(similarity_mat,x= x_label,y=y_label,text_auto=True)\n",
        "  fig.update_layout(yaxis={'visible': False, 'showticklabels': False})\n",
        "  fig.update_layout(xaxis={'visible': False, 'showticklabels': False})\n",
        "  fig.show()\n",
        "  return fig\n",
        "def filter_questions(questions, similarity_matrix,similarity_thrs=0.65,n_thrs=3):\n",
        "  #select the question based on order (assumes ordered by scores, defualt of model.generate() output)\n",
        "\n",
        "  #consider use also the answer for similarity check\n",
        "  selected_questions_idx = []\n",
        "  for i in range(0,len(questions)):\n",
        "    cur_qs = questions[i]\n",
        "    if not cur_qs.strip().endswith('?'):\n",
        "      continue\n",
        "    elif len(selected_questions_idx) >= n_thrs:\n",
        "      break\n",
        "    elif len(selected_questions_idx)==0 or all(similarity_matrix[i,selected_questions_idx]<similarity_thrs):\n",
        "      selected_questions_idx.append(i)\n",
        "  return [questions[a] for a in selected_questions_idx]\n",
        "import re\n",
        "def replace_abbreviations(abrv_dict, text,only_first = False):\n",
        "  if len(abrv_dict)==0:\n",
        "    return text\n",
        "  if only_first:\n",
        "    only_first = 1\n",
        "  else:\n",
        "    only_first = 0\n",
        "  if isinstance(text,list):\n",
        "    return [replace_abbreviations(abrv_dict, cur_text,only_first) for cur_text in text]\n",
        "  #`only_first` resolve only the first abbreviation in the text (e.g. if the text includes the results section, and the abbreviation was define at the introduction, the method will resolve once the abbreviation in the text the model is exposed to)\n",
        "  for abrv,long_form_abbr in abrv_dict.items():\n",
        "    text = text.replace(f'{long_form_abbr} ({abrv})',abrv) #first the function abbreviate all occurences\n",
        "    text = text.replace(f'{long_form_abbr}({abrv})',abrv) \n",
        "    pattern = re.compile(r'(^|\\s|\\.)'+abrv+ r'( |[\\W\\s])')\n",
        "\n",
        "    # Use the re.sub() function to replace the first instance of the pattern\n",
        "    # with the word \"B\"\n",
        "    text = re.sub(pattern, f' {long_form_abbr} ({abrv})'+ r'\\1', text, count=only_first)\n",
        "  return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Living in the NYC is diffuclt, the NYC people are stressed.\n",
            "Living in the NYC is diffuclt, the NYC people are stressed.\n",
            "\n",
            "1 <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "#replace_abbreviations(abrv_dict, [''],only_first = False)\n",
        "print(replace_abbreviations({'NYC':'New York City'},'Living in the NYC is diffuclt, the NYC people are stressed.'))\n",
        "print(replace_abbreviations({'NYC':'New York City'},'Living in the NYC is diffuclt, the NYC people are stressed.',True))\n",
        "print(replace_abbreviations({'NYC':'New York City'},'',True))\n",
        "dd=replace_abbreviations({'NYC':'New York City'},[''],True)\n",
        "print(len(dd),type(dd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LGlP6Pd64UJO",
        "notebookRunGroups": {
          "groupValue": "2"
        },
        "outputId": "c5584f2a-cdf8-4339-c4c3-7c7f88230a61"
      },
      "outputs": [],
      "source": [
        "dir_path = '/home/ubuntu/Questions_generation/'\n",
        "from tqdm.auto import tqdm, trange\n",
        "def compute_questions(sections,org_sections,sections_ranks,generator_args,answers_generator_args,short_answers_generator_args,verbose):\n",
        "  doc = abrv_nlp('.\\n'.join(org_sections))\n",
        "  abrv_dict ={}\n",
        "  for abrv in doc._.abbreviations:\n",
        "    #print(f\"{abrv} \\t ({abrv.start}, {abrv.end}) {abrv._.long_form}\")\n",
        "    if abrv not in abrv_dict.keys():\n",
        "      abrv_dict[str(abrv)] = str(abrv._.long_form)\n",
        "  #print(doc._.hearst_patterns)\n",
        "  print(\"Abbreviation\", \"\\t\", \"Definition\")\n",
        "  print(abrv_dict)   \n",
        "  assert len(sections_ranks)==len(sections), 'Sections ranks aren\\'t consistant with number of ranks'\n",
        "  questions_df = pd.DataFrame((),columns=['section_n','section_rank','text','question','question_ppl',\n",
        "                                          'answer_1','answer_2','answer_3','answer_4',\n",
        "                                          'short_answer_1','short_answer_2','short_answer_3','short_answer_4'])\n",
        "  for section_i,cur in tqdm(enumerate(sections)):\n",
        "    print(cur)\n",
        "    cur = replace_abbreviations(abrv_dict,cur,only_first=True)\n",
        "    input_string = \"generate question: \" + cur\n",
        "    #input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "    qs,qs_ppl = get_output_from_prompt(model,input_string,generator_args)\n",
        "    print(f'Total text shape is {tokenizer.encode(cur, return_tensors=\"pt\",truncation=False).shape}')\n",
        "    if verbose:\n",
        "      for cur_qs in qs:\n",
        "        print(f'Qs:{cur_qs}')\n",
        "      #input_string = i[0]+\" /n \" + cur\n",
        "      #ans_output = get_output_from_prompt(QA_model,input_string,answers_generator_args)\n",
        "    similarity_matrix = find_similarity(qs)\n",
        "    if verbose:\n",
        "      plot_similarity_matrix(qs)\n",
        "    qs_filtered = filter_questions(qs, similarity_matrix,similarity_thrs=0.7,n_thrs=7)\n",
        "    for i,cur_qs in enumerate(qs_filtered):\n",
        "      print(f'Qs:{replace_abbreviations(abrv_dict,cur_qs,only_first=True)}')\n",
        "      ans_input_string = \"answer to the question, step by step: \"+cur_qs+\" </s> context: \" + cur\n",
        "      ans_output,_ = get_output_from_prompt(model,ans_input_string,answers_generator_args)\n",
        "      ans_output = replace_abbreviations(abrv_dict,ans_output,only_first=True)\n",
        "      if verbose:\n",
        "        for ans in ans_output:\n",
        "          print('----Ans:--',ans)\n",
        "      short_ans_input_string = \"answer to the question: \"+cur_qs+\" </s> context: \" + cur\n",
        "      short_ans_output,_ = get_output_from_prompt(model,short_ans_input_string,short_answers_generator_args)\n",
        "      short_ans_output = replace_abbreviations(abrv_dict,short_ans_output,only_first=True)\n",
        "      if verbose:\n",
        "        for ans in short_ans_output:\n",
        "          print('----Short Ans:--',ans)\n",
        "      while (len(ans_output)<=4 or len(short_ans_output)<=4): #sometimes the outputs results with less than 4 answers\n",
        "        short_ans_output.append('')\n",
        "        ans_output.append('')\n",
        "      assert len(short_ans_output)>=4\n",
        "      questions_df.loc[len(questions_df)] = [section_i,np.round(sections_ranks[section_i],4),cur,cur_qs,qs_ppl[i],\n",
        "                                            ans_output[0],ans_output[1],ans_output[2],ans_output[3],\n",
        "                                            short_ans_output[0],short_ans_output[1],short_ans_output[2],short_ans_output[3]]\n",
        "    similarity_matrix = find_similarity(qs_filtered)\n",
        "    if verbose:\n",
        "      plot_similarity_matrix(qs_filtered)\n",
        "  return questions_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# old Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator_args = {\n",
        "    \"max_new_tokens\":150,\n",
        "#\"max_length\": 256,\n",
        "\"num_beams\": 20,\n",
        "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
        "'top_p' :0.95,\n",
        "#'do_sample':True,\n",
        "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
        "'num_beam_groups':20, \n",
        "\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':7\n",
        "}\n",
        "\n",
        "answers_generator_args = {\n",
        "    \"max_new_tokens\":150,\n",
        "#\"max_length\": 256,\n",
        "\"num_beams\": 10,\n",
        "\"length_penalty\":0.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[tokenizer.encode(['.'])],\n",
        "'top_p' :0.97,\n",
        "'diversity_penalty':float(8),\n",
        "'num_beam_groups':10,\n",
        "\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':6\n",
        "}\n",
        "import copy\n",
        "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
        "short_answers_generator_args[\"length_penalty\"]=-0.9\n",
        "questions_df = compute_questions(sections=sections,sections_ranks=sections_ranks,\n",
        "                                generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
        "questions_df.to_pickle(dir_path+'questions_generated2.pickle')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir_path = '/home/ubuntu/Questions_generation/'\n",
        "questions_df.to_pickle(dir_path+'questions_generated.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tysZOSbkjPyB",
        "outputId": "7c2f4102-b113-4ffb-fb23-c218ddbd7171"
      },
      "outputs": [],
      "source": [
        "questions_df2 = questions_df\n",
        "answers_generator_args = {\n",
        "    \"max_new_tokens\":100,\n",
        "#\"max_length\": 256,\n",
        "\"num_beams\": 16,\n",
        "\"length_penalty\":-1.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[tokenizer.encode(['.'])],\n",
        "'top_p' :0.95,\n",
        "'diversity_penalty':float(10),\n",
        "'num_beam_groups':8,\n",
        "\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':5\n",
        "}\n",
        "\n",
        "papers_list = sections#,news_article,roelfsema_paper_abstract,roelfsema_paper_intro2,roelfsema_paper_intro1,roelfsema_paper_conclusion]\n",
        "doc = nlp('.\\n'.join(sections))\n",
        "print(\"Abbreviation\", \"\\t\", \"Definition\")\n",
        "abrv_dict ={}\n",
        "for abrv in doc._.abbreviations:\n",
        "\t#print(f\"{abrv} \\t ({abrv.start}, {abrv.end}) {abrv._.long_form}\")\n",
        "  if abrv not in abrv_dict.keys():\n",
        "    abrv_dict[str(abrv)] = str(abrv._.long_form)\n",
        "print(doc._.hearst_patterns)\n",
        "print(abrv_dict)   \n",
        "\n",
        "questions_df = pd.DataFrame((),columns=['section_n','section_rank','text','question','answer_1','answer_2','answer_3','answer_4','question_ppl'])\n",
        "for section_i,cur in enumerate(papers_list):\n",
        "  print(cur)\n",
        "  cur = replace_abbreviations(abrv_dict,cur,only_first=True)\n",
        "  input_string = \"generate question: \" + cur\n",
        "  #input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "  qs,qs_ppl = get_output_from_prompt(model,input_string,generator_args)\n",
        "  print(qs_ppl)\n",
        "  print(f'Total text shape is {tokenizer.encode(cur, return_tensors=\"pt\",truncation=False).shape}')\n",
        "  for cur_qs in qs:\n",
        "    print(f'Qs:{cur_qs}')\n",
        "    #input_string = i[0]+\" /n \" + cur\n",
        "    #ans_output = get_output_from_prompt(QA_model,input_string,answers_generator_args)\n",
        "  similarity_matrix = find_similarity(qs)\n",
        "  plot_similarity_matrix(qs)\n",
        "  qs_filtered = filter_questions(qs, similarity_matrix,similarity_thrs=0.7,n_thrs=7)\n",
        "  for i,cur_qs in enumerate(qs_filtered):\n",
        "    print(f'Qs:{replace_abbreviations(abrv_dict,cur_qs,only_first=True)}')\n",
        "    ans_input_string = \"answer to the question, step by step: \"+cur_qs+\" </s> context: \" + cur\n",
        "    ans_output,_ = get_output_from_prompt(model,ans_input_string,answers_generator_args)\n",
        "    for ans in ans_output:\n",
        "      print('----Ans:--',replace_abbreviations(abrv_dict,ans,only_first=True))\n",
        "    ans_output[0]\n",
        "    questions_df.loc[len(questions_df)] = [section_i,np.round(sections_ranks[section_i],4),cur,cur_qs,\n",
        "                                           ans_output[0],ans_output[1],ans_output[2],ans_output[3],qs_ppl[i]]\n",
        "    '''questions_df = pd.concat([questions_df.loc[:],\n",
        "                             pd.DataFrame((cur,cur_qs,\n",
        "                                           ans_output[0],ans_output[1],ans_output[2],ans_output[3],qs_ppl[i]\n",
        "                                           ),columns=questions_df.columns)]).reset_index(drop=True)'''\n",
        "    '''dist_input_string = \"generate distractor to the question, step by step: \"+i+\" </s> context: \" + cur\n",
        "    dist_output = get_output_from_prompt(model,dist_input_string,answers_generator_args)\n",
        "    for dist in dist_output:\n",
        "      print('----Distractor:--',dist)'''\n",
        "  similarity_matrix = find_similarity(qs_filtered)\n",
        "  plot_similarity_matrix(qs_filtered)\n",
        "questions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CY5UsYDRCeA"
      },
      "outputs": [],
      "source": [
        "questions_df.to_pickle(dir_path+'questions_generated2.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihMQKKkWYJk9"
      },
      "source": [
        "# QA+QG models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "abb16877e3da40418070a9d454ed3efe",
            "bd051383b0c64e99ae015006faa88566",
            "be42ae6aee554318bb07fe3a03499062",
            "6e232bf110c848978295f77475938326",
            "b7be025a8f6741a6aaed9893d0565452",
            "ed102c7f999a4927bd833d637b2ea867",
            "9a496f7a3b614863bc493ea2f2fa4bb3",
            "965033b332ee4c48bd233b2e64da3f78",
            "4d3123861517461883dad2f0b1df8610",
            "3d7f637b90c94df9943de38974066ef9",
            "4eee7dad4502441ead4d3416d920879b",
            "f93c5540b7cd4730b1a1d044aa954f12",
            "276fc7a3ed994b44a78b1fe682384ef1",
            "22bfa2b6304f41b485d4f60a33beec8a",
            "e77f7a984b4e4814af128964fc509eef",
            "010205a92d6444178afb7027270cd583",
            "cc32deac3449452183e2e275907e5c03",
            "47481f637f3742c0bf022fdea29ee39e",
            "292e7911379a42dc8d8f6373ac86145f",
            "0cd0bb5768da4cde8948551235d9be0f",
            "6302a8d0355d4f678ba15a6e831fa367",
            "22818ab4c92b42d581825afa9b544a17",
            "a0ec9c4d11db4a6bb878c189916c16be",
            "9443cce1956346cebcb42bfea6f0429b",
            "fc7b6d0a32104609bfb7a0ae2f6db2d0",
            "1041e11ae58f4846a39e16ddf45b7477",
            "458ef3b2aa934cdf9c5367afcf6d5609",
            "a6a5b4cc5935410288f11ef197bdc223",
            "d66bc4ef74eb411ba1915bcde1c3bfa0",
            "d456038ce21944bb84e592fba181484e",
            "944b0ebb16b248ef8720ef22a807b130",
            "fbd78e57aab1415087eec7b2fa4202fe",
            "bde77fab7cfd4d399f79add1b210a0f4",
            "6e39bde94ab742a49159399e5467466e",
            "461c35f738894b10b3a6cbc956941f94",
            "5c3eb3c7b038419e980e85eb151df6b0",
            "951c85d99bb9439d8e2700331a6a5900",
            "78bd7877d76f40b68cf2ec43085eecc6",
            "4319d8ca6ee74b67b52b0c6bfcc1bb4e",
            "b21d9ba0530b4109b7aa9c5cef5de67e",
            "60e7277e0372406b8aec312fd969307f",
            "f00acf8a444844ab871c326c9556aea0",
            "fbf3c7defad6444581ee24eed49fef0e",
            "049495d9315e4e2fb11bdda61e044e37",
            "00995f81b8264f359381d0d0118f956a",
            "87be85b1ed784d30b487083cf846c1d0",
            "c48be6368fc848caa740cdf52fe0d39d",
            "ffe99f3b2a454586966c1cd0e96f0e54",
            "627ad910a3d2481db45f1a9bb90a1bfe",
            "b2e74f9f05c94ab5a6bfa81292b43da1",
            "79e201af2bbe44a2bad6fa0d47500ec5",
            "c3c8d5facdcf48c4994bc1e7954bd57f",
            "8829e41bbca54ad28066cbf3e8d0bcd9",
            "4c451f3095b243f797b32e0700d11e10",
            "43a0a349583a407e86a2252685f4aeee"
          ]
        },
        "id": "PLTZ9ebXeUux",
        "notebookRunGroups": {
          "groupValue": "2"
        },
        "outputId": "3c2278d2-6909-455c-9b97-4d504af44edb"
      },
      "outputs": [],
      "source": [
        "QA_model_name = \"allenai/unifiedqa-v2-t5-3b-1363200\" # you can specify the model size here\n",
        "from transformers import AutoTokenizer,AutoModel,T5Tokenizer,T5ForConditionalGeneration\n",
        "QA_tokenizer = T5Tokenizer.from_pretrained(QA_model_name)\n",
        "QA_model = T5ForConditionalGeneration.from_pretrained(QA_model_name)\n",
        "\n",
        "#questions_df = pd.read_pickle(dir_path+'questions_generated2.pickle')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ls-xjhTDB6EW",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "def score_string_similarity(str1, str2):\n",
        "    if str1 == str2:\n",
        "        return 3.0   # Better than perfect token match\n",
        "    str1 = fix_buggy_characters(replace_punctuation(str1))\n",
        "    str2 = fix_buggy_characters(replace_punctuation(str2))\n",
        "    if str1 == str2:\n",
        "        return 2.0\n",
        "    if \" \" in str1 or \" \" in str2:\n",
        "        str1_split = str1.split(\" \")\n",
        "        str2_split = str2.split(\" \")\n",
        "        overlap = list(set(str1_split) & set(str2_split))\n",
        "        return len(overlap) / max(len(str1_split), len(str2_split))\n",
        "    else:\n",
        "        if str1 == str2:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "def replace_punctuation(str):\n",
        "    return str.replace(\"\\\"\", \"\").replace(\"'\", \"\")\n",
        "\n",
        "# Temporary fix for bug where {}^<\\` characters roundtrip into \\u2047 (??) character\n",
        "def fix_buggy_characters(text):\n",
        "    return re.sub(\"[{}^\\\\\\\\`\\u2047<]\", \" \", text)\n",
        "\n",
        "def norm_text_for_QA_model(text):\n",
        "  text =text.replace(\"'(.*)'\", r\"\\1\")\n",
        "  return text.lower()\n",
        "def run_QA_model(context,question_and_answers, **generator_args):\n",
        "  #make sure tokenizer trunction isnt cut the questions part. \n",
        "  '''  question_and_answers_ids = QA_tokenizer.encode(norm_text_for_QA_model(question_and_answers), return_tensors=\"pt\")\n",
        "  qs_and_answers_token_len = question_and_answers_ids.shape[1]\n",
        "  context_ids = QA_tokenizer.encode(norm_text_for_QA_model(context), return_tensors=\"pt\",max_length =QA_tokenizer.model_max_length-qs_and_answers_token_len,truncation=True)\n",
        "  input_ids = torch.cat((question_and_answers_ids[:,:-1],context_ids[:,1:]),1)'''\n",
        "\n",
        "  input_ids = QA_tokenizer.encode(norm_text_for_QA_model(question_and_answers+context), return_tensors=\"pt\").to(QA_model.device)\n",
        "  res = QA_model.generate(input_ids, **generator_args)\n",
        "\n",
        "  return QA_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "def select_best_answer(questions_df):\n",
        "  questions_df['generated_selected_ans']=''\n",
        "  questions_df['selected_ans']=''\n",
        "  for i in range(len(questions_df)):\n",
        "    qs = questions_df.iloc[i]\n",
        "    ans_list = [qs.answer_1,qs.answer_2,qs.answer_3,qs.answer_4,qs.short_answer_1,qs.short_answer_2,qs.short_answer_3,qs.short_answer_4]\n",
        "    qa_output = run_QA_model(context = qs.text,\n",
        "                             question_and_answers =qs.question+' \\n (a) '+qs.answer_1 +' (b) '+qs.answer_2+' (c) '+qs.answer_3+' (d) '+qs.answer_4 +\\\n",
        "                              ' (e)'+qs.short_answer_1 +' (f) '+qs.short_answer_2+' (g) '+qs.short_answer_3+' (h) '+qs.short_answer_4 +' \\n ' \n",
        "                            ,max_new_tokens=150 )#,return_dict_in_generate =True,output_scores=True)\n",
        "    questions_df.loc[i,'generated_selected_ans'] = qa_output[0]\n",
        "    sim_scores = [score_string_similarity(qa_output[0].strip().lower(), s.strip().lower()) for s in ans_list]\n",
        "    questions_df.loc[i,'selected_ans'] = ans_list[int(np.argmax(sim_scores))]\n",
        "  return questions_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1rNlmZVbTTT",
        "outputId": "05bfafb4-f951-4d55-f31c-14434b8bf5e2"
      },
      "outputs": [],
      "source": [
        "'''!pip install levenshtein\n",
        "import re\n",
        "\n",
        "\n",
        "import Levenshtein\n",
        "for i in range(len(questions_df)):\n",
        "   cur_qs = questions_df.iloc[i]\n",
        "   lev_dist = [Levenshtein.distance(cur_qs.selected_ans.strip().lower(), s.strip().lower()) for s in [cur_qs.answer_1,cur_qs.answer_2,cur_qs.answer_3,cur_qs.answer_4]]\n",
        "   print(lev_dist) \n",
        "   lev_dist = [score_string_similarity(cur_qs.selected_ans.strip().lower(), s.strip().lower()) for s in [cur_qs.answer_1,cur_qs.answer_2,cur_qs.answer_3,cur_qs.answer_4]]\n",
        "   print(lev_dist) '''\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrCll7BYZVaJ",
        "outputId": "d39c21a5-62b5-4146-bd68-42c39d926cf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "QA_tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qxUhcD9n-kFN",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "from tqdm.auto import tqdm,trange\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\"text2text-generation\", model='Salesforce/mixqg-large', tokenizer='Salesforce/mixqg-large')\n",
        "import pandas as pd\n",
        "#questions_df = pd.read_pickle(dir_path+'questions_generated2_withQA.pickle')\n",
        "\n",
        "def format_inputs(context: str, answer: str):\n",
        "  if isinstance(context,list):\n",
        "    return [f\"{cur_answer} \\\\n {cur_context}\" for cur_answer,cur_context in zip(answer,context)]\n",
        "  else:\n",
        "    return f\"{answer} \\\\n {context}\" \n",
        "\n",
        "#new_qs = nlp(format_inputs(questions_df.loc[:,'text'].tolist(), questions_df.loc[:,'selected_ans'].tolist()))\n",
        "def create_question_MixQG(questions_df):\n",
        "  questions_df['new_question']=''\n",
        "\n",
        "  for i in trange(len(questions_df)):\n",
        "    questions_df.loc[i,'new_question']= nlp(format_inputs(questions_df.loc[i,'text'], questions_df.loc[i,'selected_ans']))[0]['generated_text']\n",
        "\n",
        "  print('MixQG generation activated!')\n",
        "  return questions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_len_tokens(text,tokenizer):\n",
        "    if isinstance(text,list):\n",
        "        text = list(filter(lambda x: x is not None, text))\n",
        "        return [check_len_tokens(cur_text,tokenizer) for cur_text in text]\n",
        "    return tokenizer.encode(text,return_tensors='pt').shape[1]\n",
        "'''\n",
        "with open('/home/ubuntu/Questions_generation/orel babayof surgical duration.json') as f:\n",
        "  result = json.load(f)\n",
        "print('Original tokens N per section:')\n",
        "print(check_len_tokens([s['original']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ],tokenizer))\n",
        "print('Summary tokens N per section:')\n",
        "print(check_len_tokens([s['summary']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ],tokenizer))\n",
        "\n",
        "with open('/home/ubuntu/Questions_generation/The role of semantics in the success of crowdfunding projects.json') as f:\n",
        "  result = json.load(f)\n",
        "print('Original tokens N per section:')\n",
        "print(check_len_tokens([s['original']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ],tokenizer))\n",
        "print('Summary tokens N per section:')\n",
        "print(check_len_tokens([s['summary']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ],tokenizer))\n",
        "with open('/home/ubuntu/Questions_generation/Dementia care mental health.json') as f:\n",
        "  result = json.load(f)'''\n",
        "print('Original tokens N per section:')\n",
        "print(check_len_tokens([s['original']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ],tokenizer))\n",
        "print('Summary tokens N per section:')\n",
        "print(check_len_tokens([s['summary']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ],tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ],
      "source": [
        "%pdb on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12085
        },
        "id": "fxfkfvOHJ2O1",
        "outputId": "d4015646-b305-471f-89f2-c763b5d69ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abbreviation \t Definition\n",
            "{'ROME': 'Rank-One Model Editing', 'zsRE': 'zero-shot relation extraction', 'TE': 'total effect', 'IE': 'indirect effect', 'ATE': 'average total effect', 'AIE': 'average indirect effect', 'effect': 'effect (AIE', 'it': 'in the MLP modules (ii', 'iii': 'ii) at specific middle layers', 'FT': 'Fine-Tuning', 'KE': 'Knowledge Editor', 'ES': 'Efficacy Score', 'EM': 'Efficacy Magnitude', 'PS': 'Paraphrase Scores', 'NS': 'Neighborhood Score', 'KN': 'Knowledge Neurons'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model’s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset of difficult counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:01,  1.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total text shape is torch.Size([1, 275])\n",
            " Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "Total text shape is torch.Size([1, 148])\n",
            "Qs:Why are facts stored in transformer models?\n",
            "Qs:Which middle layers determine the weights of transformer models?\n",
            "Qs:How do factual knowledge associations emerge in GPT-like transformer models?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [00:12,  7.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "Total text shape is torch.Size([1, 221])\n",
            "Qs:what can we learn about language model?\n",
            "Qs:Where do causality states in middle layers?\n",
            "Qs:how many layers are there?\n",
            "Qs:For what purpose is attention important?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [00:26, 10.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, it will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "Total text shape is torch.Size([1, 371])\n",
            "Qs:Who has more influence on recalling facts than others?\n",
            "Qs:To understand causal mediation, we use what?\n",
            "Qs:Do causal mediation analysis for GPT?\n",
            "Qs:Which method is used to determine the causal importance of a state variable?\n",
            "Qs:Why does G lose some information about the subject?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4it [00:53, 16.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Figure 2 plots the AIE of the internal components of GPT-2 XL (1.5B parameters) The ATE of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "Total text shape is torch.Size([1, 216])\n",
            "Qs:On how many layers does causality occur?\n",
            "Qs:Where does Causal Tracing reveal causal states?\n",
            "Qs:Are there any other interesting aspects about this paper?\n",
            "Qs:How is causality mediated by individual states at the last subject token?\n",
            "Qs:What are the parameters of the experiment?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [01:12, 17.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Based on causal traces, we posit a specific mechanism for storage of factual associations. Each midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall memorized properties about that subject. Middle layer MLP outputs accumulate information, then the summed information is copied to the last token by attention at high layers. This hypothesis localizes factual association along three dimensions, placing it (i) in the MLP modules (ii) at specific middle layers (iii) and specifically at the processing of the subject’s last token.  (e) To write new value vector v∗ into the layer, (f) we calculate a rank-one update to cause ˆ W (l) projk∗ = v, while minimizing interference with other memories stored in the layer. could be equivalently stored in any one of the middle MLP layers.\n",
            "Total text shape is torch.Size([1, 197])\n",
            "Qs:Do we store factual associations along three dimensions?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6it [01:16, 12.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " A new key–value pair (k∗, v∗) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize ∥ ˆWK − V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k∗, v∗) is computed. Step 1: Choosing k∗ to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value vâ� that encodes the new relation (r, o∗), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l∗) i := z), will cause the network to predict the target object o∗ in response to the factual prompt p′ (of the form “{subject} is a”) The optimization does not directly alter model weights; it identifies a vector representation v∗ that when output at the targeted MLP module, represents the new property for the subject s.\n",
            "Total text shape is torch.Size([1, 361])\n",
            "Qs:Where does this approach come from and how does in the MLP modules (ii (it) work?\n",
            "Qs:How do we select the subject?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7it [01:34, 14.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " We evaluate ROME on the Zero-Shot Re-  Table 1: zsRE Editing Results on GPT-2 XL. lation Extraction (zsRE) task. Our evaluation slice contains 10,000 records, each containing one factual statement, its paraphrase, and one unrelated fact. “Efficacy” and “Paraphrase” measure post-edit accuracy I o∗ = argmaxoPG′ [o], while “Specificity” measures the edited model’s accuracy on an unrelated fact.  ROME is competitive with hypernetworks and fine-tuning methods despite its simplicity. We find that it is not hard for ROME to insert an association that can be regurgitated by the model. Robustness under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks KE-zsRE and MEND-ZsRE.\n",
            "Total text shape is torch.Size([1, 228])\n",
            "Qs:What is Rank-One Model Editing (ROME) \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8it [01:38, 11.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of ES, PS, NS as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "Total text shape is torch.Size([1, 215])\n",
            "Qs:Where are false facts from?\n",
            "Qs:Who wrote this paper?\n",
            "Qs:Which metrics are used to measure the effectiveness of counterfactual edits?\n",
            "Qs:What is the purpose of this paper?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "9it [01:49, 11.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test ROME’s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "Total text shape is torch.Size([1, 203])\n",
            "Qs:Why is rank one model editing (ROME) better than other methods?\n",
            "Qs:Are there any problems with Rank-One Model Editing (ROME) \n",
            "Qs:On what metrics do we observe strong correlation with causal analysis?\n",
            "Qs:Where do generalization and specificity peak?\n",
            "Qs:Which method demonstrates both generalization and specificity?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [02:08, 13.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The purpose of ROME is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and it is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "Total text shape is torch.Size([1, 109])\n",
            "Qs:When did Rome begin?\n",
            "Qs:Who developed this method?\n",
            "Qs:Where can one find this information?\n",
            "Qs:Does Rank-One Model Editing (ROME) have a practical purpose?\n",
            "Qs: Rank-One Model Editing (ROME)is a tool for understanding mechanisms of what?\n",
            "Qs:How many facts does Rank-One Model Editing (ROME) edit?\n",
            "Qs:What is the purpose of Rank-One Model Editing?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11it [02:36, 17.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete it (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer ROME parameter intervention has comparable capabilities.\n",
            "Total text shape is torch.Size([1, 329])\n",
            "Qs:Is knowledge extraction and editing along multiple dimensions?\n",
            "Qs:Can we measure causal structure of models?\n",
            "Qs:What is the Causal Tracing method?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12it [02:47, 13.98s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stage 1:/home/ubuntu/Questions_generation/GPT_factualGQ.pickle has been saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stage 2:/home/ubuntu/Questions_generation/GPT_factualQG+QA.pickle has been saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:47<00:00,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MixQG generation activated!\n",
            "Stage 3:/home/ubuntu/Questions_generation/GPT_factualGQ+QA+GQ.pickle has been saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "    \"max_new_tokens\":150,\n",
        "#\"max_length\": 256,\n",
        "\"num_beams\": 20,\n",
        "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
        "'top_p' :0.955,\n",
        "#'do_sample':True,\n",
        "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
        "'num_beam_groups':20, \n",
        "\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':7\n",
        "}\n",
        "\n",
        "answers_generator_args = {\n",
        "    \"max_new_tokens\":150,\n",
        "    #\"max_length\": 256,\n",
        "    \"num_beams\": 10,\n",
        "    \"length_penalty\":0.2,\n",
        "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
        "    'top_p' :0.97,\n",
        "    'diversity_penalty':float(8),\n",
        "    'num_beam_groups':10,\n",
        "    \"return_dict_in_generate\" :True,\n",
        "    'output_scores':True,\n",
        "    \"early_stopping\": True,\n",
        "    'num_return_sequences':6\n",
        "}\n",
        "import json\n",
        "import copy\n",
        "import warnings\n",
        "\n",
        "# Disable all warning messages\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "'''name = 'surgical2'\n",
        "with open('/home/ubuntu/Questions_generation/orel babayof surgical duration.json') as f:\n",
        "  result = json.load(f)\n",
        "sections = [s['summary']['text'] if 'summary' in s.keys() else None for k,s in result['sections'].items() ]\n",
        "org_text = [s['original']['text'] for k,s in result['sections'].items() ]\n",
        "min_words_in_section=60\n",
        "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
        "\n",
        "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
        "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
        "questions_df = compute_questions(sections=sections,org_sections=org_text,sections_ranks=np.ones(len(sections)),\n",
        "                                generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
        "questions_df.to_pickle(dir_path+name+'GQ.pickle')\n",
        "print(f\"Stage 1:{dir_path+name+'GQ.pickle'} has been saved\")\n",
        "questions_df = select_best_answer(questions_df)\n",
        "questions_df.to_pickle(dir_path+name+'QG+QA.pickle')\n",
        "print(f\"Stage 2:{dir_path+name+'QG+QA.pickle'} has been saved\")\n",
        "questions_df = create_question_MixQG(questions_df)\n",
        "questions_df.to_pickle(dir_path+name+'GQ+QA+GQ.pickle')\n",
        "print(f\"Stage 3:{dir_path+name+'GQ+QA+GQ.pickle'} has been saved\")\n",
        "'''\n",
        "name = 'GPT_factual'\n",
        "with open('/home/ubuntu/Questions_generation/Locating and Editing Factual Associations in GPT.json') as f:\n",
        "  result = json.load(f)\n",
        "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
        "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
        "min_words_in_section=60\n",
        "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
        "org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
        "\n",
        "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
        "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
        "questions_df = compute_questions(sections=sections,org_sections=org_text,sections_ranks=np.ones(len(sections)),\n",
        "                                generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args,\n",
        "                                verbose=False)\n",
        "questions_df.to_pickle(dir_path+name+'GQ.pickle')\n",
        "print(f\"Stage 1:{dir_path+name+'GQ.pickle'} has been saved\")\n",
        "torch.cuda.empty_cache()\n",
        "QA_model.to(device)\n",
        "questions_df = select_best_answer(questions_df)\n",
        "questions_df.to_pickle(dir_path+name+'QG+QA.pickle')\n",
        "print(f\"Stage 2:{dir_path+name+'QG+QA.pickle'} has been saved\")\n",
        "questions_df = create_question_MixQG(questions_df)\n",
        "questions_df.to_pickle(dir_path+name+'GQ+QA+GQ.pickle')\n",
        "print(f\"Stage 3:{dir_path+name+'GQ+QA+GQ.pickle'} has been saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stage 2:/home/ubuntu/Questions_generation/DementiaQG+QA.pickle has been saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:32<00:00,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MixQG generation activated!\n",
            "Stage 3:/home/ubuntu/Questions_generation/DementiaGQ+QA+GQ.pickle has been saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "questions_df = select_best_answer(questions_df)\n",
        "questions_df.to_pickle(dir_path+name+'QG+QA.pickle')\n",
        "print(f\"Stage 2:{dir_path+name+'QG+QA.pickle'} has been saved\")\n",
        "questions_df = create_question_MixQG(questions_df)\n",
        "questions_df.to_pickle(dir_path+name+'GQ+QA+GQ.pickle')\n",
        "print(f\"Stage 3:{dir_path+name+'GQ+QA+GQ.pickle'} has been saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "QA_model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ],
      "source": [
        "%pdb on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1:Why are facts stored in transformer models?\n",
            "Q2:What can large language modeling models do?\n",
            "Best ans: Large language modeling models can predict factual statements about the world.\n",
            "\n",
            "A0: Large language modeling models can predict factual statements about the world.\n",
            "A1: A transformer model is a model that can store knowledge about the world.\n",
            "A2: Rank-One Model Editing is a method for editing transformer models.\n",
            "A3: Factual knowledge can be stored in transformer models.\n",
            "A4: The Space Needle is located in the city\n",
            "A5: Rank-One Model Editing method\n",
            "A6: the last token of the subject name is decisive\n",
            "A7: Rank-One Model Editing\n",
            "Text: Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "\n",
            "\n",
            "Q1:Which middle layers determine the weights of transformer models?\n",
            "Q2:What does our analysis reveal?\n",
            "Best ans: MLPs at different middle layers determine the weights of transformer models.\n",
            "\n",
            "A0: This work presents a new method for weighting transformer models that uses feedforward MLPs to determine the last token of subject names.\n",
            "A1: We analyze transformer models that use a variety of middle layers to store factual knowledge about the world and test this finding in model weights.\n",
            "A2: MLPs at different middle layers determine the weights of transformer models.\n",
            "A3: In this paper, we investigate how factual knowledge is stored in transformer models. We first analyze the model weights of transformer models by introducing rank-one model editing.\n",
            "A4: fed forward\n",
            "A5: feeder MLP\n",
            "A6: input model weights\n",
            "A7: MLPs\n",
            "Text: Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "\n",
            "\n",
            "Q1:How do factual knowledge associations emerge in GPT-like transformer models?\n",
            "Q2:What does this paper investigate?\n",
            "Best ans: Factual knowledge associations emerge in GPT-like transformer models. We investigate how such associations are stored within GPT models.\n",
            "\n",
            "A0: This work presents a new method for learning factual knowledge associations in GPT-like transformer models.\n",
            "A1: Rank-One Model Editing is a method for modifying the weights of GPT-like transformer model weights.\n",
            "A2: Factual knowledge associations emerge in GPT-like transformer models.\n",
            "A3: Factual knowledge associations emerge in GPT-like transformer models. We investigate how such associations are stored within GPT models.\n",
            "A4: fed forward\n",
            "A5: Rank-One Model Editing method.\n",
            "A6: feeder MLPs at a range\n",
            "A7: Rank-One Model Editing method\n",
            "Text: Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "\n",
            "\n",
            "Q1:what can we learn about language model?\n",
            "Q2:What is a new discovery in the autoregressive transformer language model?\n",
            "Best ans: strong causal states are found in middle layers\n",
            "\n",
            "A0: strong causal states are found in middle layers\n",
            "A1: strong causal states are found in middle layers of the language model\n",
            "A2: Context: Autoregressive transformer language Model G :\n",
            "A3: Context: Autoregressive transformer language Model G :X Y over vocabulary V maps\n",
            "A4: attention\n",
            "A5: attention is important\n",
            "A6: c\n",
            "A7: strong causal states\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:Where do causality states in middle layers?\n",
            "Q2:What is the main finding of the model?\n",
            "Best ans: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery.\n",
            "\n",
            "A0: This paper presents an algorithm for learning to model the language of the user. The model is an autoregressive transformer language model. The final output is read from the last hidden state vector h(L) T. The answer: (b) MLP contributions dominate.\n",
            "A1: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is  a two-layer neural network.\n",
            "A2: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site\n",
            "A3: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery.\n",
            "A4: [x1]\n",
            "A5: Early\n",
            "A6: the early site\n",
            "A7: ‘early site”\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:how many layers are there?\n",
            "Q2:How many layers of MLP are in each layer?\n",
            "Best ans: Three\n",
            "\n",
            "A0: X  Y over vocabulary V maps\n",
            "A1: There are three layers of the model.\n",
            "A2: One of the main problems with the model is that in the MLP modules (ii (it) has only one layer. The final answer: (c).\n",
            "A3: Each layer’s MLP is a two-layer neural network. Each layer has a MLP that is based on a pair of layers.\n",
            "A4: two\n",
            "A5: four\n",
            "A6: Two\n",
            "A7: Three\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:For what purpose is attention important?\n",
            "Q2:What is the purpose of the probability distribution?\n",
            "Best ans: To predict the next-token continuations of a sequence.\n",
            "\n",
            "A0: To predict the next-token continuations of a sequence.\n",
            "A1: For strong causality to occur, attention is important at the late site.\n",
            "A2: The late site is important at the late site. Each layer’s MLP is a two layer neural network.\n",
            "A3: At the late site, attention is important because in the MLP modules (ii (it) can help the model to learn to predict the next-token continuations of x.\n",
            "A4: later\n",
            "A5: c\n",
            "A6: Late\n",
            "A7: Late Site\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:Who has more influence on recalling facts than others?\n",
            "Q2:What is the indirect effect of?\n",
            "Best ans: the mediating state\n",
            "\n",
            "A0: We use the recurrent model to model the state-space dynamics of a GPT. We use a set of recursive models to model state-state interactions. We model state states as a graph. We select  to be 3 times larger than the empirical standard deviation of embeddings. We set  = 3 times the empirical normal deviation of the embeddables.\n",
            "A1: This paper presents an approach to causal mediation analysis of GPT. We use the following model: G is a G-based model that uses a set of states to represent a subject. We first generate the graph of states by using the G-model. We then use a few state-specific variables to represent the subject. Then we use the resulting graph to represent our subject.\n",
            "A2: As in GPT, we use the following model: We use the MLP to perform the computations. We use a set of corrupted activations to perform computation. We select  to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "A3: A set is defined as a set of states. The grid of states is a causal graph. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact.\n",
            "A4: states\n",
            "A5: the mediating state\n",
            "A6: hidden state variables\n",
            "A7: A few clean states\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:To understand causal mediation, we use what?\n",
            "Q2:What model do we use?\n",
            "Best ans: We use the following model:\n",
            "\n",
            "A0: the GPT\n",
            "A1: Computational graphs\n",
            "A2: We use the following model:\n",
            "A3: a grid of states\n",
            "A4: A\n",
            "A5: MLP\n",
            "A6: computational complexity\n",
            "A7: corruption\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:Do causal mediation analysis for GPT?\n",
            "Q2:What does the GPT framework do?\n",
            "Best ans: Using the Generalized Pyramid Topology (GPT) framework, we propose a method for causal mediation analysis for GPT that uses a grid of states to represent the information about the subject. We use the GPT framework to model the information in the graph and to learn the causal importance for each state in the computation graph.\n",
            "\n",
            "A0: In this paper, we use GPT to investigate the causal importance of the underlying variables. We use a GPT-based model to train a model that can learn to recall a fact. We train the model to learn to remember a given fact.\n",
            "A1: We use GPT to model the recall of facts. We use a GPT-based model to model a causal graph. We run GPT on the noisy embeddings. We select the  to be 3 times larger than the empirical standard deviation of embeddINGs.\n",
            "A2: Using the Generalized Pyramid Topology (GPT) framework, we propose a method for causal mediation analysis for GPT that uses a grid of states to represent the information about the subject. We use the GPT framework to model the information in the graph and to learn the causal importance for each state in the computation graph.\n",
            "A3: This paper presents an approach to causal mediation for GPT. We use GPT as a baseline for causal mediation analysis. We first perform an initialization run on the baseline. We then run the baseline on clean data. We select  to be 3 times larger than the empirical standard deviation (ESD) of embeddings.\n",
            "A4: \n",
            "A5: Computational Analysis of Corrupted GPT\n",
            "A6: the grid\n",
            "A7: A model for GPT\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:Which method is used to determine the causal importance of a state variable?\n",
            "Q2:What is the grid of states called?\n",
            "Best ans: Computation graph\n",
            "\n",
            "A0: Computation graph\n",
            "A1: Corrupted-with-restoration run\n",
            "A2: This paper presents an approach to causal mediation analysis using a grid of states (Figure 1) and an MLP model (G).\n",
            "A3: Computation graph. The ability of a few clean states (Figure 1) to recover the correct fact, despite many other states being corrupted by the corruptes state will indicate their causal importance in computation graph.\n",
            "A4: direct\n",
            "A5: indirect\n",
            "A6: direct influence\n",
            "A7: IE\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:Why does G lose some information about the subject?\n",
            "Q2:What is this paper about?\n",
            "Best ans: This paper presents an approach to causal mediation analysis that uses a set of corrupted activations to train a GPT model on a graph of states.\n",
            "\n",
            "A0: We use the recurrent model to model the subject. We use a set of corrupted activations to model a subject. The recursive model is a model of a state.\n",
            "A1: Using a set of corrupted activations, we learn that the subject is a complex graph of states. We use the MLP to model the subject and the corruption to learn the causal importance of the subject.\n",
            "A2: This paper presents an approach to causal mediation analysis that uses a set of corrupted activations to train a GPT model on a graph of states.\n",
            "A3: In the GPT variants we use the following model: G is allowed to continue normally, giving us a set of corrupted activations. We select  to be 3 times larger than the empirical standard deviation of embeddings G is a GPT model that uses the MLP to perform computations on the noisy embeddments.\n",
            "A4: normalized\n",
            "A5: normal\n",
            "A6: corruption of activations\n",
            "A7: the corruption\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:On how many layers does causality occur?\n",
            "Q2:What is the number of layers of MLP?\n",
            "Best ans: 15 layers of MLP\n",
            "\n",
            "A0: 15 layers\n",
            "A1: 15 layers of MLP.\n",
            "A2: 15 layers of MLP\n",
            "A3:  average indirect effect (AIE (effect) (AIE)8.7% at layer 15\n",
            "A4: 15.\n",
            "A5: 14\n",
            "A6: 16\n",
            "A7: 13\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Where does Causal Tracing reveal causal states?\n",
            "Q2:What do we hypothesize about the localized midlayer MLP key-value mapping?\n",
            "Best ans: We use Causal Tracing to probe causal effects at an early site. We use the GPT-2 XL experiment to test this hypothesis. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "A0: We use Causal Tracing to probe causal effects at an early site. We use the GPT-2 XL experiment to test this hypothesis.\n",
            "A1: Causal Tracing is a method for probing path-specific effects. The presence of strong causal states at an early site immediately before the prediction.\n",
            "A2: At the last subject token, Causal Tracing reveals strong causal individual states at a late site.\n",
            "A3: We use Causal Tracing to probe causal effects at an early site. We use the GPT-2 XL experiment to test this hypothesis. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "A4: late\n",
            "A5: path\n",
            "A6: midlayer\n",
            "A7: the early site\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Are there any other interesting aspects about this paper?\n",
            "Q2:What is the use of Causal tracing?\n",
            "Best ans: Causal Tracing can be used to probe path-specific effects for paths that avoid MLP computations\n",
            "\n",
            "A0: The authors also propose a new method for detecting causal effects in a path-specific manner.\n",
            "A1: This paper proposes an alternative to the MLP approach to path-specific effects.\n",
            "A2: We show that the causal effects of a path are mediated by MLP modules at the early site.\n",
            "A3: A large portion of the effect (AIE (effect) is strongly causal at a late site immediately before the prediction.\n",
            "A4: Causal Tracing\n",
            "A5: Causal Tracing can be used to probe path-specific effects\n",
            "A6: Causal Tracing can be used to probe path-specific effects for paths that avoid MLP computations\n",
            "A7: A large portion of the effect (AIE (effect) is mediated by strongly causal individual states at a late site immediately before the prediction.\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:How is causality mediated by individual states at the last subject token?\n",
            "Q2:What do we hypothesize about the localized midlayer MLP key-value mapping?\n",
            "Best ans: Localized midlayer MLP key–value mapping recalls facts about the subject. The presence and emergence at an early site of strong causal states at the last subject token is a new discovery.\n",
            "\n",
            "A0: Localized midlayer MLP key–value mapping recalls facts about the subject. The presence and emergence at an early site of strong causal states at the last subject token is a new discovery.\n",
            "A1: This paper presents a novel method for probing path-specific effects. We propose a localized midlayer MLP key–value mapping to recall facts about the subject.\n",
            "A2: Localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "A3:  average total effect (AIE (effect) (ATE)is the average total effect of the internal components of GPT-2. The average total effects of this experiment are 18.6%. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery.\n",
            "A4: strong\n",
            "A5: locally\n",
            "A6: At layer 15\n",
            "A7: layer 15\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:What are the parameters of the experiment?\n",
            "Q2:What are the internal components of GPT-2 XL?\n",
            "Best ans: 1.5B parameters = 0, 1, 2, 3, 4, 5, 6, 7, 0\n",
            "\n",
            "A0: 1.5B parameters = 0, 1, 2, 3, 4, 5, 6, 7, 0\n",
            "A1: This experiment is designed to test the hypothesis that causal states at the early site are more informative than gradient-based salience methods such as integrated gradients.\n",
            "A2: This experiment is designed to test the hypothesis that causal states at the early site are more informative than gradient-based salience methods such as integrated gradients. The experiment is based on the GPT-2 XL model.\n",
            "A3:  average total effect (AIE (effect) (ATE)= average indirect effect (AIE) (AUE) of the internal components of GPT-2 XL (1.5B parameters) The ATE of this experiment is 18.6%.\n",
            "A4: 15.\n",
            "A5: 2.5\n",
            "A6: ATE\n",
            "A7: 15\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Do we store factual associations along three dimensions?\n",
            "Q2:is factual associations stored in any one of the middle MLP layers\n",
            "Best ans: The hypothesis is that factual associations could be stored in any one of the middle MLP layers. The final answer: yes...\n",
            "\n",
            "A0: The hypothesis is that factual associations could be stored in any one of the middle MLP layers. The final answer: yes...\n",
            "A1: An MLP module is an information processing unit that stores information. Factual associations are stored in MLP modules. The answer: yes\n",
            "A2: In the MLP modules (ii) and specifically at the processing of a subject’s last token, we store factual associations along three dimensions.\n",
            "A3: The hypothesis is that factual associations could be stored in any one of the middle MLP layers. The final answer: yes..\n",
            "A4: Yes.\n",
            "A5: i\n",
            "A6: No\n",
            "A7: no\n",
            "Text: Based on causal traces, we posit a specific mechanism for storage of factual associations. Each midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall memorized properties about that subject. Middle layer MLP outputs accumulate information, then the summed information is copied to the last token by attention at high layers. This hypothesis localizes factual association along three dimensions, placing in the MLP modules (ii (it) (i) in the MLP modules (iii and specifically at the processing of the subject’s last token.  (e) To write new value vector v∗ into the layer, (f) we calculate a rank-one update to cause ˆ W (l) projk∗ = v, while minimizing interference with other memories stored in the layer. could be equivalently stored in any one of the middle MLP layers.\n",
            "\n",
            "\n",
            "Q1:Where does this approach come from and how does it work?\n",
            "Q2:What is the solution based on?\n",
            "Best ans: Moore-Penrose pseudoinverse: W = V K+. Because of this simple algebraic structure, we can insert any fact directly once (k, v) is computed\n",
            "\n",
            "A0: Convolutional Networks\n",
            "A1: It is proposed by Bau et al. in the paper:\n",
            "A2: Convolutional Networks are used to solve the problem of insertion of new key–value pairs (k, v) into memory by solving the constrained least-squares problem. In this paper, we propose an optimization method for solving the problem using a fully-connected layer.\n",
            "A3: This approach comes from the Moore-Penrose pseudoinverse: W = V K+. Because of this simple algebraic structure, we can insert any fact directly once (k, v) is computed.\n",
            "A4: inference\n",
            "A5: Moore-Penrose pseudoinverse: W = V K+\n",
            "A6: the Moore-Penrose pseudoinverse: W = V K+\n",
            "A7: Moore-Penrose pseudoinverse: W = V K+. Because of this simple algebraic structure, we can insert any fact directly once (k, v) is computed\n",
            "Text: A new key–value pair (k∗, v∗) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize ∥ ˆWK − V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k∗, v∗) is computed. Step 1: Choosing k∗ to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value vâ� that encodes the new relation (r, o∗), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l∗) i := z), will cause the network to predict the target object o∗ in response to the factual prompt p′ (of the form “{subject} is a”) The optimization does not directly alter model weights; in the MLP modules (ii (it) identifies a vector representation v∗ that when output at the targeted MLP module, represents the new property for the subject s.\n",
            "\n",
            "\n",
            "Q1:How do we select the subject?\n",
            "Q2:How does the optimization process work?\n",
            "Best ans: In the MLP module, we identify the vector representation of the new property for the subject s. We use the following algorithm:\n",
            "\n",
            "A0: In the MLP module, we identify the vector representation of the new property for the subject s. We use the following algorithm:\n",
            "A1: In the MLP module, we identify the vector representation of the new property for the subject s.\n",
            "A2: Using the Moore-Penrose pseudoinverse, we can derive a closed form solution: minimize  WK  V. In a fully-connected layer, we have a solution that minimizes  WK - V.\n",
            "A3: A new key–value pair (k, v) can be inserted optimally into the memory by solving a constrained least-squares problem. In a fully-connected layer we can derive an closed form solution: minimize WK  V. Because of this simple algebraic structure, we can insert any fact directly once (k) is computed.\n",
            "A4: \n",
            "A5: subject is a\n",
            "A6: we chose the subject at its last token as the lookup key\n",
            "A7: choose the subject at its last token as the lookup key\n",
            "Text: A new key–value pair (k∗, v∗) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize ∥ ˆWK − V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k∗, v∗) is computed. Step 1: Choosing k∗ to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value vâ� that encodes the new relation (r, o∗), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l∗) i := z), will cause the network to predict the target object o∗ in response to the factual prompt p′ (of the form “{subject} is a”) The optimization does not directly alter model weights; in the MLP modules (ii (it) identifies a vector representation v∗ that when output at the targeted MLP module, represents the new property for the subject s.\n",
            "\n",
            "\n",
            "Q1:What is ROME?\n",
            "Q2:What is the Rank-One Model Editing (ROME) on the Zero-S\n",
            "Best ans: Role of Model Editing in Zero-Shot Relation Extraction (zsRE) Evaluation\n",
            "\n",
            "A0: Research on the Zero Shot Relation Extraction task.\n",
            "A1: Role of Model Editing in Zero-Shot Relation Extraction (zsRE) Evaluation of ROME\n",
            "A2: Role of Model Editing in Zero-Shot Relation Extraction (zsRE) Evaluation\n",
            "A3: Role of Model Editing in Zero-Shot Relation Extraction (zsRE)\n",
            "A4: rank one model editing (ROME)\n",
            "A5: model editor\n",
            "A6: rank one model editing\n",
            "A7: Model editing method for zero-shot relation extraction\n",
            "Text: We evaluate Rank-One Model Editing (ROME) on the Zero-Shot Re-  Table 1: zero-shot relation extraction (zsRE) Editing Results on GPT-2 XL. lation Extraction (zsRE) task. Our evaluation slice contains 10,000 records, each containing one factual statement, its paraphrase, and one unrelated fact. “Efficacy” and “Paraphrase” measure post-edit accuracy I o∗ = argmaxoPG′ [o], while “Specificity” measures the edited model’s accuracy on an unrelated fact.  ROME is competitive with hypernetworks and fine-tuning methods despite its simplicity. We find that in the MLP modules (ii (it) is not hard for ROME to insert an association that can be regurgitated by the model. Robustness under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks Knowledge Editor (KE) zsRE and MEND-ZsRE.\n",
            "\n",
            "\n",
            "Q1:Where are false facts from?\n",
            "Q2:What does the COUNTERFACT dataset contain?\n",
            "Best ans: Counterfactual dataset contains 21,919 records with diverse set of subjects, relations, and languages.\n",
            "\n",
            "A0: Language models\n",
            "A1: Counterfactual dataset contains 21,919 records with diverse set of subjects, relations, and languages.\n",
            "A2: This paper is about a dataset for evaluating counterfactual edits in a language model.\n",
            "A3: In this paper, we use the COUNTERFACT dataset to evaluate counterfactual edits in language models.\n",
            "A4: G′\n",
            "A5: vocabulary\n",
            "A6: literature\n",
            "A7: languages\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Who wrote this paper?\n",
            "Q2:Who observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model\n",
            "Best ans: Hasee and colleagues\n",
            "\n",
            "A0: Hasee and colleagues\n",
            "A1: has et al.\n",
            "A2: A. Hass et al.\n",
            "A3: has et al. (2021)\n",
            "A4: authors:\n",
            "A5: Hasee\n",
            "A6: Dr.\n",
            "A7: has et al\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Which metrics are used to measure the effectiveness of counterfactual edits?\n",
            "Q2:What are the scores we use to measure the efficacy of significant changes?\n",
            "Best ans: Efficacy score (ES) Paraphrase Scores (PS) neighborhood score (NS) Efficiency score (PS) PS Efficity score (PM)\n",
            "\n",
            "A0: Efficacy score (ES) Paraphrase Scores (PS) neighborhood score (NS) Efficiency score (PS) PS Efficity score (PM)\n",
            "A1: Efficacy score (ES) Paraphrase Scores (PS) neighborhood score (NS) Efficiency score (PS) PS\n",
            "A2: A dataset for evaluating counterfactual edits in language models. Paraphrase Scores (PS) and Neighborhood Score (NS) are used for assessing Efficacy of significant changes.\n",
            "A3: In this paper, we report Paraphrase Scores, Neighborhood Score, and Efficacy Score (ES) Paraphrase Scores (PS)  Neighborhood Score (NS) as Score (S) to measure efficacy.\n",
            "A4: (PS) and\n",
            "A5: Phrase scores\n",
            "A6: Ps\n",
            "A7: PS\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:What is the purpose of this paper?\n",
            "Q2:What is the COUNTERFACT dataset used for?\n",
            "Best ans: Evaluation of counterfactual edits in language models.\n",
            "\n",
            "A0: Efficacy and Specificity of Language Models\n",
            "A1: The goal of this paper was to evaluate the effectiveness of counterfactual edits in language models.\n",
            "A2: A dataset for evaluation of counterfactual edits in language models.\n",
            "A3: A dataset for evaluation of counterfactual edits in language models\n",
            "A4: Model-editing benchmark\n",
            "A5: to measure generalization–specificity tradeoff\n",
            "A6: evaluate counterfactual edits in language models.\n",
            "A7: Evaluation of counterfactual edits in language models.\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Why is rank one model editing (ROME) better than other methods?\n",
            "Q2:What is ROME based on?\n",
            "Best ans:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity. ROME is a method for rewriting MLP modules that output decisions. ROMET is based on the Causal Tracing method.\n",
            "\n",
            "A0:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity. ROME is a method for rewriting MLP modules that output decisions. ROMET is based on the Causal Tracing method.\n",
            "A1:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity.\n",
            "A2: To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) effectiveness when targeted at various levels and tokens.\n",
            "A3: The layers at which edits generalize best correspond to the middle layers of the early site identified by 7. Rank-One Model Editing (ROME) demonstrates both generalization and specificity. ROME is a method for rewriting MLP modules that output decisions.\n",
            "A4: generalization and specificity.\n",
            "A5: both generalization and specificity\n",
            "A6: demonstrates both generalization and specificity\n",
            "A7:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:Are there any problems with ROME?\n",
            "Q2:What does ROME demonstrate?\n",
            "Best ans: The layers at which edits generalize best correspond to the middle layers. Rank-One Model Editing (ROME) demonstrates both generalization and specificity.\n",
            "\n",
            "A0: The layers at which edits generalize best correspond to the middle layers.\n",
            "A1: The layers at which edits generalize best correspond to the middle layers\n",
            "A2: The layers at which edits generalize best correspond to the middle layers. Rank-One Model Editing (ROME) demonstrates both generalization and specificity.\n",
            "A3: These methods exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects\n",
            "A4: No.\n",
            "A5: F2) underfitting\n",
            "A6: None\n",
            "A7: No\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:On what metrics do we observe strong correlation with causal analysis?\n",
            "Q2:What is ROME?\n",
            "Best ans:  Rank-One Model Editing (ROME)is a model editing method that demonstrates both generalization and specificity when targeted at various layers and tokens. ROME is an algorithm that rewrites MLP modules that output those states.\n",
            "\n",
            "A0: To confirm that factual associations are stored in the MLP modules that output those state, we test Rank-One Model Editing (ROME) on four metrics: Generalizability specificity\n",
            "A1:  Rank-One Model Editing (ROME)is a model editing method that demonstrates both generalization and specificity when targeted at various layers and tokens.\n",
            "A2:  Rank-One Model Editing (ROME)is a model editing method that demonstrates both generalization and specificity when targeted at various layers and tokens. ROME is an algorithm that rewrites MLP modules that output those states.\n",
            "A3: The layers at which edits generalize best correspond to the middle layers of the early site identified by 7 Rank-One Model Editing (ROME) demonstrates both generalization and specificity. ROME is a method that demonstrates generalization.\n",
            "A4: generalized\n",
            "A5: specialty\n",
            "A6: specificity\n",
            "A7: generalization and specificity.\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:Where do generalization and specificity peak?\n",
            "Q2:What do we observe about rewrites\n",
            "Best ans: Rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to middle layer of the early site identified by 7\n",
            "\n",
            "A0:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity at middle layers.\n",
            "A1: Rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to middle layer of the early site identified by 7\n",
            "A2: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test rank-One Model Editing (ROME)\n",
            "A3: At the last subject token, rewrites are most successful at middle layer. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7\n",
            "A4: center\n",
            "A5: inference\n",
            "A6: middle layers.\n",
            "A7: last subject token\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:Which method demonstrates both generalization and specificity?\n",
            "Q2:What is ROME based on?\n",
            "Best ans:  Rank-One Model Editing (ROME)is a method that rewrites MLP modules that output decisions. ROME demonstrates both generalization and specificity. ROMET is based on Causal Tracing.\n",
            "\n",
            "A0: Rome demonstrates both generalization and specificity.\n",
            "A1: Rome demonstrates both generalization and specificity. Rank-One Model Editing (ROME) is a method that generalizes and specificifies a model of a causal association.\n",
            "A2:  Rank-One Model Editing (ROME)is a method that rewrites MLP modules that output decisions. ROME demonstrates both generalization and specificity. ROMET is based on Causal Tracing.\n",
            "A3: The method Rank-One Model Editing (ROME) demonstrates both generalization and specificity.\n",
            "A4: \n",
            "A5: Rome\n",
            "A6: MODE\n",
            "A7: ROME\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:When did Rome begin?\n",
            "Q2:What year did ROMET begin?\n",
            "Best ans: Rome is a city in ancient Rome. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the year AD 100. Therefore, the final answer is AD 100 - 100.\n",
            "\n",
            "A0:  Rank-One Model Editing (ROME)is a tool for understanding mechanisms of knowledge storage. ROME was created in the year AD 66. AD 660 is the year that Rome began. Therefore, the final answer is AD 67.\n",
            "A1: Rome is a city in ancient Rome. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the year AD 100. Therefore, the final answer is AD 100 - 100..\n",
            "A2: To serve as an understanding mechanism of knowledge storage, Rank-One Model Editing (ROME) edit a single fact at a time. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the 4th century BC\n",
            "A3: Rome is a city in ancient Rome. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the year AD 100. Therefore, the final answer is AD 100 - 100.\n",
            "A4: \n",
            "A5: numeric\n",
            "A6: numeral\n",
            "A7: 1\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Who developed this method?\n",
            "Q2:Who developed ROME?\n",
            "Best ans:  Rank-One Model Editing (ROME)was developed by a group of researchers.\n",
            "\n",
            "A0: Rome was developed by Rank-One Model Editing.\n",
            "A1: A team at the University of California, Berkeley.\n",
            "A2: Researchers at the University of California, Berkeley.\n",
            "A3:  Rank-One Model Editing (ROME)was developed by a group of researchers.\n",
            "A4: we\n",
            "A5: GPT\n",
            "A6: team\n",
            "A7: authors\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Where can one find this information?\n",
            "Q2:Where can research papers on Rank-One Model Editing and Causal Tracing\n",
            "Best ans: Research papers on Rank-One Model Editing (ROME) and Causal Tracing can be found at: https://doi.org/10.1007/s10365-018-004\n",
            "\n",
            "A0: Research papers on Rank-One Model Editing (ROME) and Causal Tracing can be found at: https://doi.org/10.1007/s10365-018-004\n",
            "A1: One can find this information at:\n",
            "A2: This information can be found in the following:\n",
            "A3: Research papers on Rank-One Model Editing (ROME) and Causal Tracing can be found at:\n",
            "A4: (ii)\n",
            "A5: GPT\n",
            "A6: Knowledge\n",
            "A7: the MLP modules\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Does ROME have a practical purpose?\n",
            "Q2:is rome used for training large scale models\n",
            "Best ans:  Rank-One Model Editing (ROME)is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT. ROMET is not designed to train large-scale models. Therefore, the final answer is no..\n",
            "\n",
            "A0: Not intended as practical method for large-scale model training.\n",
            "A1: No, Rank-One Model Editing (ROME) is not intended as a practical method for large scale model training. Therefore, the final answer is no. ii (it)\n",
            "A2:  Rank-One Model Editing (ROME)is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT. ROMET is not designed to train large-scale models. Therefore, the final answer is no..\n",
            "A3: A tool for understanding mechanisms of knowledge storage is called Rank-One Model Editing (ROME)  ROME is not intended as a practical method for large-scale model training. ROMET is not designed to train large-scale models. Therefore, the final answer is no.\n",
            "A4: [II]\n",
            "A5: Not meant\n",
            "A6: yes\n",
            "A7: (ii)\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:ROME is a tool for understanding mechanisms of what?\n",
            "Q2:ROME is a tool for understanding mechanisms of what?\n",
            "Best ans: knowledge stored\n",
            "\n",
            "A0: knowledge stored\n",
            "A1: A tool for understanding mechanisms. Knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT. ROMES purpose is to serve as a tool for learning knowledge. ROMET is a method for learning.\n",
            "A2:  Rank-One Model Editing (ROME)is to serve as a tool for understanding mechanisms of knowledge storage. Knowledge storage is the process of organizing and storing information. Knowledge is organized into facts and stored in memory. Therefore, the final answer is information. knowledge.\n",
            "A3:  Rank-One Model Editing (ROME)is to serve as a tool for understanding mechanisms of knowledge storage. Knowledge storage is the process of organizing and storing information. Knowledge is organized into facts and stored in memory. Therefore, the final answer is information.\n",
            "A4: information\n",
            "A5: science\n",
            "A6: discovery\n",
            "A7: learning\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:How many facts does ROME edit?\n",
            "Q2:What is the purpose of ROME?\n",
            "Best ans: This question is about Rank-One Model Editing (ROME)  The answer: It only edits a single fact at a time. It only modifies one fact at time.\n",
            "\n",
            "A0: This question is about Rank-One Model Editing (ROME)  The answer: It only edits a single fact at a time. It only modifies one fact at time.\n",
            "A1: One Fact Editing (ROME) edits one fact at a time.\n",
            "A2: Single fact at time is the purpose of Rank-One Model Editing (ROME)  It only edits one fact at a time. Therefore, the final answer is one fact.\n",
            "A3: This question is about Rank-One Model Editing (ROME)  The answer: It only edits a single fact at a time.\n",
            "A4: One Fact\n",
            "A5: 1\n",
            "A6: Single\n",
            "A7: One\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:What is the purpose of Rank-One Model Editing?\n",
            "Q2:What is the purpose of ROME?\n",
            "Best ans: to understand mechanisms of knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET is to serve as an understanding tool. Therefore, the final answer is to understand. It only edits a single fact at a time.\n",
            "\n",
            "A0: to understand mechanisms of knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET is to serve as an understanding tool. Therefore, the final answer is to understand.\n",
            "A1: to understand mechanisms of knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET is to serve as an understanding tool. Therefore, the final answer is to understand. It only edits a single fact at a time.\n",
            "A2: In the MLP modules (ii) (it) in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial or numerical knowledge. Therefore, the final answer is a tool for understanding mechanisms of knowledge storage. It only edits one fact at time.\n",
            "A3:  Rank-One Model Editing (ROME)is to serve as a tool for understanding mechanisms of knowledge storage and is not intended as ii (it) is not a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial or numerical knowledge. Therefore, the final answer is to understand mechanisms of knowlege storage.\n",
            "A4: To serve\n",
            "A5: understanding\n",
            "A6: understanding mechanism\n",
            "A7: knowledge storage\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Is knowledge extraction and editing along multiple dimensions?\n",
            "Q2:What does this paper do?\n",
            "Best ans: This paper presents a method for measuring knowledge extraction and editing along multiple dimensions in a model. We use the Causal Tracing method to measure knowledge extraction. We compare our approach to previous methods in our experiments.\n",
            "\n",
            "A0: This paper presents a method for measuring knowledge extraction and editing along multiple dimensions in a model.\n",
            "A1: In this paper, we introduce COUNTERFACT, an approach to knowledge extraction that enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, and compare in the MLP modules (ii (it) to previous approaches.\n",
            "A2: This paper presents a method for measuring knowledge extraction and editing along multiple dimensions in a model. We use the Causal Tracing method to measure knowledge extraction. We compare our approach to previous methods in our experiments.\n",
            "A3: COUNTERFACT is a method for measuring knowledge extraction and editing along multiple dimensions. The method is based on the Causal Tracing method. The purpose of the method is to measure knowledge extraction along multiple dimension.\n",
            "A4: NO\n",
            "A5: No\n",
            "A6: no\n",
            "A7: COUNTERFACT\n",
            "Text: The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete in the MLP modules (ii (it) (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer Rank-One Model Editing (ROME) parameter intervention has comparable capabilities.\n",
            "\n",
            "\n",
            "Q1:Can we measure causal structure of models?\n",
            "Q2:is it possible for a model to learn\n",
            "Best ans: yes\n",
            "\n",
            "A0: To measure causal structure of models, we use our Causal tracing approach (Petroni et al., 2019 2020).\n",
            "A1: This paper presents a method for measuring causal structure of models. We use our method to measure the causal structure.\n",
            "A2: This paper presents a method for measuring causal structure of models.\n",
            "A3: In this paper, we use the Causal Tracing method to measure the causal structure of models.\n",
            "A4: no\n",
            "A5: No\n",
            "A6: yes\n",
            "A7: Can we measure causal structure of models?\n",
            "Text: The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete in the MLP modules (ii (it) (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer Rank-One Model Editing (ROME) parameter intervention has comparable capabilities.\n",
            "\n",
            "\n",
            "Q1:What is the Causal Tracing method?\n",
            "Q2:What does the Causal Tracing method introduce?\n",
            "Best ans: paired interventions that allow explicit measurement of causal indirect effects\n",
            "\n",
            "A0: A method for evaluating the causal structure of models.\n",
            "A1: paired interventions that allow explicit measurement of causal indirect effects\n",
            "A2: This paper introduces paired interventions that allow explicit measurement of causal indirect effects.\n",
            "A3: To measure causal indirect effects of individual hidden state vectors.\n",
            "A4: implicit measurements\n",
            "A5: measurement of causal indirect effects\n",
            "A6: measure causal indirect effects\n",
            "A7: measures the causal structure of models.\n",
            "Text: The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete in the MLP modules (ii (it) (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer Rank-One Model Editing (ROME) parameter intervention has comparable capabilities.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "questions_df = pd.read_pickle('/home/ubuntu/Questions_generation/GPT_factualGQ+QA+GQ.pickle')\n",
        "questions_df.columns\n",
        "i=42\n",
        "def show_qs(df,i):\n",
        "       q= df.iloc[i,:]\n",
        "       print(f'Q1:{q.question}\\nQ2:{q.new_question}\\nBest ans: {q.selected_ans}\\n')\n",
        "       [print('A'+str(i)+': '+q[cur]) for i,cur in enumerate(['answer_1', 'answer_2', 'answer_3', 'answer_4', 'short_answer_1',\n",
        "              'short_answer_2', 'short_answer_3', 'short_answer_4'])]\n",
        "       print(f'Text:{q.text}\\n\\n')\n",
        "for i in range(len(questions_df)):\n",
        "       show_qs(questions_df,i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1:ROME is a tool for understanding mechanisms of what?\n",
            "Q2:ROME is a tool for understanding mechanisms of what?\n",
            "Best ans: knowledge stored\n",
            "\n",
            "A0: knowledge stored\n",
            "A1: A tool for understanding mechanisms. Knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT. ROMES purpose is to serve as a tool for learning knowledge. ROMET is a method for learning.\n",
            "A2:  Rank-One Model Editing (ROME)is to serve as a tool for understanding mechanisms of knowledge storage. Knowledge storage is the process of organizing and storing information. Knowledge is organized into facts and stored in memory. Therefore, the final answer is information. knowledge.\n",
            "A3:  Rank-One Model Editing (ROME)is to serve as a tool for understanding mechanisms of knowledge storage. Knowledge storage is the process of organizing and storing information. Knowledge is organized into facts and stored in memory. Therefore, the final answer is information.\n",
            "A4: information\n",
            "A5: science\n",
            "A6: discovery\n",
            "A7: learning\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:What is the Causal Tracing method?\n",
            "Q2:What does the Causal Tracing method introduce?\n",
            "Best ans: paired interventions that allow explicit measurement of causal indirect effects\n",
            "\n",
            "A0: A method for evaluating the causal structure of models.\n",
            "A1: paired interventions that allow explicit measurement of causal indirect effects\n",
            "A2: This paper introduces paired interventions that allow explicit measurement of causal indirect effects.\n",
            "A3: To measure causal indirect effects of individual hidden state vectors.\n",
            "A4: implicit measurements\n",
            "A5: measurement of causal indirect effects\n",
            "A6: measure causal indirect effects\n",
            "A7: measures the causal structure of models.\n",
            "Text: The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete in the MLP modules (ii (it) (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer Rank-One Model Editing (ROME) parameter intervention has comparable capabilities.\n",
            "\n",
            "\n",
            "Q1:When did Rome begin?\n",
            "Q2:What year did ROMET begin?\n",
            "Best ans: Rome is a city in ancient Rome. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the year AD 100. Therefore, the final answer is AD 100 - 100.\n",
            "\n",
            "A0:  Rank-One Model Editing (ROME)is a tool for understanding mechanisms of knowledge storage. ROME was created in the year AD 66. AD 660 is the year that Rome began. Therefore, the final answer is AD 67.\n",
            "A1: Rome is a city in ancient Rome. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the year AD 100. Therefore, the final answer is AD 100 - 100..\n",
            "A2: To serve as an understanding mechanism of knowledge storage, Rank-One Model Editing (ROME) edit a single fact at a time. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the 4th century BC\n",
            "A3: Rome is a city in ancient Rome. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET began in the year AD 100. Therefore, the final answer is AD 100 - 100.\n",
            "A4: \n",
            "A5: numeric\n",
            "A6: numeral\n",
            "A7: 1\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Are there any problems with ROME?\n",
            "Q2:What does ROME demonstrate?\n",
            "Best ans: The layers at which edits generalize best correspond to the middle layers. Rank-One Model Editing (ROME) demonstrates both generalization and specificity.\n",
            "\n",
            "A0: The layers at which edits generalize best correspond to the middle layers.\n",
            "A1: The layers at which edits generalize best correspond to the middle layers\n",
            "A2: The layers at which edits generalize best correspond to the middle layers. Rank-One Model Editing (ROME) demonstrates both generalization and specificity.\n",
            "A3: These methods exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects\n",
            "A4: No.\n",
            "A5: F2) underfitting\n",
            "A6: None\n",
            "A7: No\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:how many layers are there?\n",
            "Q2:How many layers of MLP are in each layer?\n",
            "Best ans: Three\n",
            "\n",
            "A0: X  Y over vocabulary V maps\n",
            "A1: There are three layers of the model.\n",
            "A2: One of the main problems with the model is that in the MLP modules (ii (it) has only one layer. The final answer: (c).\n",
            "A3: Each layer’s MLP is a two-layer neural network. Each layer has a MLP that is based on a pair of layers.\n",
            "A4: two\n",
            "A5: four\n",
            "A6: Two\n",
            "A7: Three\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:what can we learn about language model?\n",
            "Q2:What is a new discovery in the autoregressive transformer language model?\n",
            "Best ans: strong causal states are found in middle layers\n",
            "\n",
            "A0: strong causal states are found in middle layers\n",
            "A1: strong causal states are found in middle layers of the language model\n",
            "A2: Context: Autoregressive transformer language Model G :\n",
            "A3: Context: Autoregressive transformer language Model G :X Y over vocabulary V maps\n",
            "A4: attention\n",
            "A5: attention is important\n",
            "A6: c\n",
            "A7: strong causal states\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:Do we store factual associations along three dimensions?\n",
            "Q2:is factual associations stored in any one of the middle MLP layers\n",
            "Best ans: The hypothesis is that factual associations could be stored in any one of the middle MLP layers. The final answer: yes...\n",
            "\n",
            "A0: The hypothesis is that factual associations could be stored in any one of the middle MLP layers. The final answer: yes...\n",
            "A1: An MLP module is an information processing unit that stores information. Factual associations are stored in MLP modules. The answer: yes\n",
            "A2: In the MLP modules (ii) and specifically at the processing of a subject’s last token, we store factual associations along three dimensions.\n",
            "A3: The hypothesis is that factual associations could be stored in any one of the middle MLP layers. The final answer: yes..\n",
            "A4: Yes.\n",
            "A5: i\n",
            "A6: No\n",
            "A7: no\n",
            "Text: Based on causal traces, we posit a specific mechanism for storage of factual associations. Each midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall memorized properties about that subject. Middle layer MLP outputs accumulate information, then the summed information is copied to the last token by attention at high layers. This hypothesis localizes factual association along three dimensions, placing in the MLP modules (ii (it) (i) in the MLP modules (iii and specifically at the processing of the subject’s last token.  (e) To write new value vector v∗ into the layer, (f) we calculate a rank-one update to cause ˆ W (l) projk∗ = v, while minimizing interference with other memories stored in the layer. could be equivalently stored in any one of the middle MLP layers.\n",
            "\n",
            "\n",
            "Q1:How many facts does ROME edit?\n",
            "Q2:What is the purpose of ROME?\n",
            "Best ans: This question is about Rank-One Model Editing (ROME)  The answer: It only edits a single fact at a time. It only modifies one fact at time.\n",
            "\n",
            "A0: This question is about Rank-One Model Editing (ROME)  The answer: It only edits a single fact at a time. It only modifies one fact at time.\n",
            "A1: One Fact Editing (ROME) edits one fact at a time.\n",
            "A2: Single fact at time is the purpose of Rank-One Model Editing (ROME)  It only edits one fact at a time. Therefore, the final answer is one fact.\n",
            "A3: This question is about Rank-One Model Editing (ROME)  The answer: It only edits a single fact at a time.\n",
            "A4: One Fact\n",
            "A5: 1\n",
            "A6: Single\n",
            "A7: One\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Where does this approach come from and how does it work?\n",
            "Q2:What is the solution based on?\n",
            "Best ans: Moore-Penrose pseudoinverse: W = V K+. Because of this simple algebraic structure, we can insert any fact directly once (k, v) is computed\n",
            "\n",
            "A0: Convolutional Networks\n",
            "A1: It is proposed by Bau et al. in the paper:\n",
            "A2: Convolutional Networks are used to solve the problem of insertion of new key–value pairs (k, v) into memory by solving the constrained least-squares problem. In this paper, we propose an optimization method for solving the problem using a fully-connected layer.\n",
            "A3: This approach comes from the Moore-Penrose pseudoinverse: W = V K+. Because of this simple algebraic structure, we can insert any fact directly once (k, v) is computed.\n",
            "A4: inference\n",
            "A5: Moore-Penrose pseudoinverse: W = V K+\n",
            "A6: the Moore-Penrose pseudoinverse: W = V K+\n",
            "A7: Moore-Penrose pseudoinverse: W = V K+. Because of this simple algebraic structure, we can insert any fact directly once (k, v) is computed\n",
            "Text: A new key–value pair (k∗, v∗) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize ∥ ˆWK − V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k∗, v∗) is computed. Step 1: Choosing k∗ to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value vâ� that encodes the new relation (r, o∗), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l∗) i := z), will cause the network to predict the target object o∗ in response to the factual prompt p′ (of the form “{subject} is a”) The optimization does not directly alter model weights; in the MLP modules (ii (it) identifies a vector representation v∗ that when output at the targeted MLP module, represents the new property for the subject s.\n",
            "\n",
            "\n",
            "Q1:Which metrics are used to measure the effectiveness of counterfactual edits?\n",
            "Q2:What are the scores we use to measure the efficacy of significant changes?\n",
            "Best ans: Efficacy score (ES) Paraphrase Scores (PS) neighborhood score (NS) Efficiency score (PS) PS Efficity score (PM)\n",
            "\n",
            "A0: Efficacy score (ES) Paraphrase Scores (PS) neighborhood score (NS) Efficiency score (PS) PS Efficity score (PM)\n",
            "A1: Efficacy score (ES) Paraphrase Scores (PS) neighborhood score (NS) Efficiency score (PS) PS\n",
            "A2: A dataset for evaluating counterfactual edits in language models. Paraphrase Scores (PS) and Neighborhood Score (NS) are used for assessing Efficacy of significant changes.\n",
            "A3: In this paper, we report Paraphrase Scores, Neighborhood Score, and Efficacy Score (ES) Paraphrase Scores (PS)  Neighborhood Score (NS) as Score (S) to measure efficacy.\n",
            "A4: (PS) and\n",
            "A5: Phrase scores\n",
            "A6: Ps\n",
            "A7: PS\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Do causal mediation analysis for GPT?\n",
            "Q2:What does the GPT framework do?\n",
            "Best ans: Using the Generalized Pyramid Topology (GPT) framework, we propose a method for causal mediation analysis for GPT that uses a grid of states to represent the information about the subject. We use the GPT framework to model the information in the graph and to learn the causal importance for each state in the computation graph.\n",
            "\n",
            "A0: In this paper, we use GPT to investigate the causal importance of the underlying variables. We use a GPT-based model to train a model that can learn to recall a fact. We train the model to learn to remember a given fact.\n",
            "A1: We use GPT to model the recall of facts. We use a GPT-based model to model a causal graph. We run GPT on the noisy embeddings. We select the  to be 3 times larger than the empirical standard deviation of embeddINGs.\n",
            "A2: Using the Generalized Pyramid Topology (GPT) framework, we propose a method for causal mediation analysis for GPT that uses a grid of states to represent the information about the subject. We use the GPT framework to model the information in the graph and to learn the causal importance for each state in the computation graph.\n",
            "A3: This paper presents an approach to causal mediation for GPT. We use GPT as a baseline for causal mediation analysis. We first perform an initialization run on the baseline. We then run the baseline on clean data. We select  to be 3 times larger than the empirical standard deviation (ESD) of embeddings.\n",
            "A4: \n",
            "A5: Computational Analysis of Corrupted GPT\n",
            "A6: the grid\n",
            "A7: A model for GPT\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:Does ROME have a practical purpose?\n",
            "Q2:is rome used for training large scale models\n",
            "Best ans:  Rank-One Model Editing (ROME)is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT. ROMET is not designed to train large-scale models. Therefore, the final answer is no..\n",
            "\n",
            "A0: Not intended as practical method for large-scale model training.\n",
            "A1: No, Rank-One Model Editing (ROME) is not intended as a practical method for large scale model training. Therefore, the final answer is no. ii (it)\n",
            "A2:  Rank-One Model Editing (ROME)is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT. ROMET is not designed to train large-scale models. Therefore, the final answer is no..\n",
            "A3: A tool for understanding mechanisms of knowledge storage is called Rank-One Model Editing (ROME)  ROME is not intended as a practical method for large-scale model training. ROMET is not designed to train large-scale models. Therefore, the final answer is no.\n",
            "A4: [II]\n",
            "A5: Not meant\n",
            "A6: yes\n",
            "A7: (ii)\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Can we measure causal structure of models?\n",
            "Q2:is it possible for a model to learn\n",
            "Best ans: yes\n",
            "\n",
            "A0: To measure causal structure of models, we use our Causal tracing approach (Petroni et al., 2019 2020).\n",
            "A1: This paper presents a method for measuring causal structure of models. We use our method to measure the causal structure.\n",
            "A2: This paper presents a method for measuring causal structure of models.\n",
            "A3: In this paper, we use the Causal Tracing method to measure the causal structure of models.\n",
            "A4: no\n",
            "A5: No\n",
            "A6: yes\n",
            "A7: Can we measure causal structure of models?\n",
            "Text: The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete in the MLP modules (ii (it) (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer Rank-One Model Editing (ROME) parameter intervention has comparable capabilities.\n",
            "\n",
            "\n",
            "Q1:On how many layers does causality occur?\n",
            "Q2:What is the number of layers of MLP?\n",
            "Best ans: 15 layers of MLP\n",
            "\n",
            "A0: 15 layers\n",
            "A1: 15 layers of MLP.\n",
            "A2: 15 layers of MLP\n",
            "A3:  average indirect effect (AIE (effect) (AIE)8.7% at layer 15\n",
            "A4: 15.\n",
            "A5: 14\n",
            "A6: 16\n",
            "A7: 13\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Who has more influence on recalling facts than others?\n",
            "Q2:What is the indirect effect of?\n",
            "Best ans: the mediating state\n",
            "\n",
            "A0: We use the recurrent model to model the state-space dynamics of a GPT. We use a set of recursive models to model state-state interactions. We model state states as a graph. We select  to be 3 times larger than the empirical standard deviation of embeddings. We set  = 3 times the empirical normal deviation of the embeddables.\n",
            "A1: This paper presents an approach to causal mediation analysis of GPT. We use the following model: G is a G-based model that uses a set of states to represent a subject. We first generate the graph of states by using the G-model. We then use a few state-specific variables to represent the subject. Then we use the resulting graph to represent our subject.\n",
            "A2: As in GPT, we use the following model: We use the MLP to perform the computations. We use a set of corrupted activations to perform computation. We select  to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "A3: A set is defined as a set of states. The grid of states is a causal graph. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact.\n",
            "A4: states\n",
            "A5: the mediating state\n",
            "A6: hidden state variables\n",
            "A7: A few clean states\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:Where are false facts from?\n",
            "Q2:What does the COUNTERFACT dataset contain?\n",
            "Best ans: Counterfactual dataset contains 21,919 records with diverse set of subjects, relations, and languages.\n",
            "\n",
            "A0: Language models\n",
            "A1: Counterfactual dataset contains 21,919 records with diverse set of subjects, relations, and languages.\n",
            "A2: This paper is about a dataset for evaluating counterfactual edits in a language model.\n",
            "A3: In this paper, we use the COUNTERFACT dataset to evaluate counterfactual edits in language models.\n",
            "A4: G′\n",
            "A5: vocabulary\n",
            "A6: literature\n",
            "A7: languages\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Why is rank one model editing (ROME) better than other methods?\n",
            "Q2:What is ROME based on?\n",
            "Best ans:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity. ROME is a method for rewriting MLP modules that output decisions. ROMET is based on the Causal Tracing method.\n",
            "\n",
            "A0:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity. ROME is a method for rewriting MLP modules that output decisions. ROMET is based on the Causal Tracing method.\n",
            "A1:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity.\n",
            "A2: To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) effectiveness when targeted at various levels and tokens.\n",
            "A3: The layers at which edits generalize best correspond to the middle layers of the early site identified by 7. Rank-One Model Editing (ROME) demonstrates both generalization and specificity. ROME is a method for rewriting MLP modules that output decisions.\n",
            "A4: generalization and specificity.\n",
            "A5: both generalization and specificity\n",
            "A6: demonstrates both generalization and specificity\n",
            "A7:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:What is ROME?\n",
            "Q2:What is the Rank-One Model Editing (ROME) on the Zero-S\n",
            "Best ans: Role of Model Editing in Zero-Shot Relation Extraction (zsRE) Evaluation\n",
            "\n",
            "A0: Research on the Zero Shot Relation Extraction task.\n",
            "A1: Role of Model Editing in Zero-Shot Relation Extraction (zsRE) Evaluation of ROME\n",
            "A2: Role of Model Editing in Zero-Shot Relation Extraction (zsRE) Evaluation\n",
            "A3: Role of Model Editing in Zero-Shot Relation Extraction (zsRE)\n",
            "A4: rank one model editing (ROME)\n",
            "A5: model editor\n",
            "A6: rank one model editing\n",
            "A7: Model editing method for zero-shot relation extraction\n",
            "Text: We evaluate Rank-One Model Editing (ROME) on the Zero-Shot Re-  Table 1: zero-shot relation extraction (zsRE) Editing Results on GPT-2 XL. lation Extraction (zsRE) task. Our evaluation slice contains 10,000 records, each containing one factual statement, its paraphrase, and one unrelated fact. “Efficacy” and “Paraphrase” measure post-edit accuracy I o∗ = argmaxoPG′ [o], while “Specificity” measures the edited model’s accuracy on an unrelated fact.  ROME is competitive with hypernetworks and fine-tuning methods despite its simplicity. We find that in the MLP modules (ii (it) is not hard for ROME to insert an association that can be regurgitated by the model. Robustness under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks Knowledge Editor (KE) zsRE and MEND-ZsRE.\n",
            "\n",
            "\n",
            "Q1:Who developed this method?\n",
            "Q2:Who developed ROME?\n",
            "Best ans:  Rank-One Model Editing (ROME)was developed by a group of researchers.\n",
            "\n",
            "A0: Rome was developed by Rank-One Model Editing.\n",
            "A1: A team at the University of California, Berkeley.\n",
            "A2: Researchers at the University of California, Berkeley.\n",
            "A3:  Rank-One Model Editing (ROME)was developed by a group of researchers.\n",
            "A4: we\n",
            "A5: GPT\n",
            "A6: team\n",
            "A7: authors\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:For what purpose is attention important?\n",
            "Q2:What is the purpose of the probability distribution?\n",
            "Best ans: To predict the next-token continuations of a sequence.\n",
            "\n",
            "A0: To predict the next-token continuations of a sequence.\n",
            "A1: For strong causality to occur, attention is important at the late site.\n",
            "A2: The late site is important at the late site. Each layer’s MLP is a two layer neural network.\n",
            "A3: At the late site, attention is important because in the MLP modules (ii (it) can help the model to learn to predict the next-token continuations of x.\n",
            "A4: later\n",
            "A5: c\n",
            "A6: Late\n",
            "A7: Late Site\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:Why are facts stored in transformer models?\n",
            "Q2:What can large language modeling models do?\n",
            "Best ans: Large language modeling models can predict factual statements about the world.\n",
            "\n",
            "A0: Large language modeling models can predict factual statements about the world.\n",
            "A1: A transformer model is a model that can store knowledge about the world.\n",
            "A2: Rank-One Model Editing is a method for editing transformer models.\n",
            "A3: Factual knowledge can be stored in transformer models.\n",
            "A4: The Space Needle is located in the city\n",
            "A5: Rank-One Model Editing method\n",
            "A6: the last token of the subject name is decisive\n",
            "A7: Rank-One Model Editing\n",
            "Text: Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "\n",
            "\n",
            "Q1:What are the parameters of the experiment?\n",
            "Q2:What are the internal components of GPT-2 XL?\n",
            "Best ans: 1.5B parameters = 0, 1, 2, 3, 4, 5, 6, 7, 0\n",
            "\n",
            "A0: 1.5B parameters = 0, 1, 2, 3, 4, 5, 6, 7, 0\n",
            "A1: This experiment is designed to test the hypothesis that causal states at the early site are more informative than gradient-based salience methods such as integrated gradients.\n",
            "A2: This experiment is designed to test the hypothesis that causal states at the early site are more informative than gradient-based salience methods such as integrated gradients. The experiment is based on the GPT-2 XL model.\n",
            "A3:  average total effect (AIE (effect) (ATE)= average indirect effect (AIE) (AUE) of the internal components of GPT-2 XL (1.5B parameters) The ATE of this experiment is 18.6%.\n",
            "A4: 15.\n",
            "A5: 2.5\n",
            "A6: ATE\n",
            "A7: 15\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Are there any other interesting aspects about this paper?\n",
            "Q2:What is the use of Causal tracing?\n",
            "Best ans: Causal Tracing can be used to probe path-specific effects for paths that avoid MLP computations\n",
            "\n",
            "A0: The authors also propose a new method for detecting causal effects in a path-specific manner.\n",
            "A1: This paper proposes an alternative to the MLP approach to path-specific effects.\n",
            "A2: We show that the causal effects of a path are mediated by MLP modules at the early site.\n",
            "A3: A large portion of the effect (AIE (effect) is strongly causal at a late site immediately before the prediction.\n",
            "A4: Causal Tracing\n",
            "A5: Causal Tracing can be used to probe path-specific effects\n",
            "A6: Causal Tracing can be used to probe path-specific effects for paths that avoid MLP computations\n",
            "A7: A large portion of the effect (AIE (effect) is mediated by strongly causal individual states at a late site immediately before the prediction.\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Which method is used to determine the causal importance of a state variable?\n",
            "Q2:What is the grid of states called?\n",
            "Best ans: Computation graph\n",
            "\n",
            "A0: Computation graph\n",
            "A1: Corrupted-with-restoration run\n",
            "A2: This paper presents an approach to causal mediation analysis using a grid of states (Figure 1) and an MLP model (G).\n",
            "A3: Computation graph. The ability of a few clean states (Figure 1) to recover the correct fact, despite many other states being corrupted by the corruptes state will indicate their causal importance in computation graph.\n",
            "A4: direct\n",
            "A5: indirect\n",
            "A6: direct influence\n",
            "A7: IE\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:What is the purpose of this paper?\n",
            "Q2:What is the COUNTERFACT dataset used for?\n",
            "Best ans: Evaluation of counterfactual edits in language models.\n",
            "\n",
            "A0: Efficacy and Specificity of Language Models\n",
            "A1: The goal of this paper was to evaluate the effectiveness of counterfactual edits in language models.\n",
            "A2: A dataset for evaluation of counterfactual edits in language models.\n",
            "A3: A dataset for evaluation of counterfactual edits in language models\n",
            "A4: Model-editing benchmark\n",
            "A5: to measure generalization–specificity tradeoff\n",
            "A6: evaluate counterfactual edits in language models.\n",
            "A7: Evaluation of counterfactual edits in language models.\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Where do causality states in middle layers?\n",
            "Q2:What is the main finding of the model?\n",
            "Best ans: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery.\n",
            "\n",
            "A0: This paper presents an algorithm for learning to model the language of the user. The model is an autoregressive transformer language model. The final output is read from the last hidden state vector h(L) T. The answer: (b) MLP contributions dominate.\n",
            "A1: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is  a two-layer neural network.\n",
            "A2: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site\n",
            "A3: Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site’ is a new discovery.\n",
            "A4: [x1]\n",
            "A5: Early\n",
            "A6: the early site\n",
            "A7: ‘early site”\n",
            "Text: Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.\n",
            "\n",
            "\n",
            "Q1:How do we select the subject?\n",
            "Q2:How does the optimization process work?\n",
            "Best ans: In the MLP module, we identify the vector representation of the new property for the subject s. We use the following algorithm:\n",
            "\n",
            "A0: In the MLP module, we identify the vector representation of the new property for the subject s. We use the following algorithm:\n",
            "A1: In the MLP module, we identify the vector representation of the new property for the subject s.\n",
            "A2: Using the Moore-Penrose pseudoinverse, we can derive a closed form solution: minimize  WK  V. In a fully-connected layer, we have a solution that minimizes  WK - V.\n",
            "A3: A new key–value pair (k, v) can be inserted optimally into the memory by solving a constrained least-squares problem. In a fully-connected layer we can derive an closed form solution: minimize WK  V. Because of this simple algebraic structure, we can insert any fact directly once (k) is computed.\n",
            "A4: \n",
            "A5: subject is a\n",
            "A6: we chose the subject at its last token as the lookup key\n",
            "A7: choose the subject at its last token as the lookup key\n",
            "Text: A new key–value pair (k∗, v∗) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize ∥ ˆWK − V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k∗, v∗) is computed. Step 1: Choosing k∗ to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value vâ� that encodes the new relation (r, o∗), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l∗) i := z), will cause the network to predict the target object o∗ in response to the factual prompt p′ (of the form “{subject} is a”) The optimization does not directly alter model weights; in the MLP modules (ii (it) identifies a vector representation v∗ that when output at the targeted MLP module, represents the new property for the subject s.\n",
            "\n",
            "\n",
            "Q1:Where can one find this information?\n",
            "Q2:Where can research papers on Rank-One Model Editing and Causal Tracing\n",
            "Best ans: Research papers on Rank-One Model Editing (ROME) and Causal Tracing can be found at: https://doi.org/10.1007/s10365-018-004\n",
            "\n",
            "A0: Research papers on Rank-One Model Editing (ROME) and Causal Tracing can be found at: https://doi.org/10.1007/s10365-018-004\n",
            "A1: One can find this information at:\n",
            "A2: This information can be found in the following:\n",
            "A3: Research papers on Rank-One Model Editing (ROME) and Causal Tracing can be found at:\n",
            "A4: (ii)\n",
            "A5: GPT\n",
            "A6: Knowledge\n",
            "A7: the MLP modules\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Where do generalization and specificity peak?\n",
            "Q2:What do we observe about rewrites\n",
            "Best ans: Rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to middle layer of the early site identified by 7\n",
            "\n",
            "A0:  Rank-One Model Editing (ROME)demonstrates both generalization and specificity at middle layers.\n",
            "A1: Rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to middle layer of the early site identified by 7\n",
            "A2: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test rank-One Model Editing (ROME)\n",
            "A3: At the last subject token, rewrites are most successful at middle layer. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7\n",
            "A4: center\n",
            "A5: inference\n",
            "A6: middle layers.\n",
            "A7: last subject token\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:How is causality mediated by individual states at the last subject token?\n",
            "Q2:What do we hypothesize about the localized midlayer MLP key-value mapping?\n",
            "Best ans: Localized midlayer MLP key–value mapping recalls facts about the subject. The presence and emergence at an early site of strong causal states at the last subject token is a new discovery.\n",
            "\n",
            "A0: Localized midlayer MLP key–value mapping recalls facts about the subject. The presence and emergence at an early site of strong causal states at the last subject token is a new discovery.\n",
            "A1: This paper presents a novel method for probing path-specific effects. We propose a localized midlayer MLP key–value mapping to recall facts about the subject.\n",
            "A2: Localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "A3:  average total effect (AIE (effect) (ATE)is the average total effect of the internal components of GPT-2. The average total effects of this experiment are 18.6%. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery.\n",
            "A4: strong\n",
            "A5: locally\n",
            "A6: At layer 15\n",
            "A7: layer 15\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:Where does Causal Tracing reveal causal states?\n",
            "Q2:What do we hypothesize about the localized midlayer MLP key-value mapping?\n",
            "Best ans: We use Causal Tracing to probe causal effects at an early site. We use the GPT-2 XL experiment to test this hypothesis. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "A0: We use Causal Tracing to probe causal effects at an early site. We use the GPT-2 XL experiment to test this hypothesis.\n",
            "A1: Causal Tracing is a method for probing path-specific effects. The presence of strong causal states at an early site immediately before the prediction.\n",
            "A2: At the last subject token, Causal Tracing reveals strong causal individual states at a late site.\n",
            "A3: We use Causal Tracing to probe causal effects at an early site. We use the GPT-2 XL experiment to test this hypothesis. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "A4: late\n",
            "A5: path\n",
            "A6: midlayer\n",
            "A7: the early site\n",
            "Text: Figure 2 plots the average indirect effect (AIE (effect) (AIE) of the internal components of GPT-2 XL (1.5B parameters) The average total effect (ATE) of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.\n",
            "\n",
            "\n",
            "Q1:To understand causal mediation, we use what?\n",
            "Q2:What model do we use?\n",
            "Best ans: We use the following model:\n",
            "\n",
            "A0: the GPT\n",
            "A1: Computational graphs\n",
            "A2: We use the following model:\n",
            "A3: a grid of states\n",
            "A4: A\n",
            "A5: MLP\n",
            "A6: computational complexity\n",
            "A7: corruption\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:Who wrote this paper?\n",
            "Q2:Who observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model\n",
            "Best ans: Hasee and colleagues\n",
            "\n",
            "A0: Hasee and colleagues\n",
            "A1: has et al.\n",
            "A2: A. Hass et al.\n",
            "A3: has et al. (2021)\n",
            "A4: authors:\n",
            "A5: Hasee\n",
            "A6: Dr.\n",
            "A7: has et al\n",
            "Text: Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of Efficacy Score (ES)  PS, Neighborhood Score (NS) as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.\n",
            "\n",
            "\n",
            "Q1:Why does G lose some information about the subject?\n",
            "Q2:What is this paper about?\n",
            "Best ans: This paper presents an approach to causal mediation analysis that uses a set of corrupted activations to train a GPT model on a graph of states.\n",
            "\n",
            "A0: We use the recurrent model to model the subject. We use a set of corrupted activations to model a subject. The recursive model is a model of a state.\n",
            "A1: Using a set of corrupted activations, we learn that the subject is a complex graph of states. We use the MLP to model the subject and the corruption to learn the causal importance of the subject.\n",
            "A2: This paper presents an approach to causal mediation analysis that uses a set of corrupted activations to train a GPT model on a graph of states.\n",
            "A3: In the GPT variants we use the following model: G is allowed to continue normally, giving us a set of corrupted activations. We select  to be 3 times larger than the empirical standard deviation of embeddings G is a GPT model that uses the MLP to perform computations on the noisy embeddments.\n",
            "A4: normalized\n",
            "A5: normal\n",
            "A6: corruption of activations\n",
            "A7: the corruption\n",
            "Text: The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, in the MLP modules (ii (it) will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (AIE (effect) (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.\n",
            "\n",
            "\n",
            "Q1:How do factual knowledge associations emerge in GPT-like transformer models?\n",
            "Q2:What does this paper investigate?\n",
            "Best ans: Factual knowledge associations emerge in GPT-like transformer models. We investigate how such associations are stored within GPT models.\n",
            "\n",
            "A0: This work presents a new method for learning factual knowledge associations in GPT-like transformer models.\n",
            "A1: Rank-One Model Editing is a method for modifying the weights of GPT-like transformer model weights.\n",
            "A2: Factual knowledge associations emerge in GPT-like transformer models.\n",
            "A3: Factual knowledge associations emerge in GPT-like transformer models. We investigate how such associations are stored within GPT models.\n",
            "A4: fed forward\n",
            "A5: Rank-One Model Editing method.\n",
            "A6: feeder MLPs at a range\n",
            "A7: Rank-One Model Editing method\n",
            "Text: Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "\n",
            "\n",
            "Q1:What is the purpose of Rank-One Model Editing?\n",
            "Q2:What is the purpose of ROME?\n",
            "Best ans: to understand mechanisms of knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET is to serve as an understanding tool. Therefore, the final answer is to understand. It only edits a single fact at a time.\n",
            "\n",
            "A0: to understand mechanisms of knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET is to serve as an understanding tool. Therefore, the final answer is to understand.\n",
            "A1: to understand mechanisms of knowledge storage. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge. ROMET is to serve as an understanding tool. Therefore, the final answer is to understand. It only edits a single fact at a time.\n",
            "A2: In the MLP modules (ii) (it) in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. Rank-One Model Editing (ROME) and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial or numerical knowledge. Therefore, the final answer is a tool for understanding mechanisms of knowledge storage. It only edits one fact at time.\n",
            "A3:  Rank-One Model Editing (ROME)is to serve as a tool for understanding mechanisms of knowledge storage and is not intended as ii (it) is not a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial or numerical knowledge. Therefore, the final answer is to understand mechanisms of knowlege storage.\n",
            "A4: To serve\n",
            "A5: understanding\n",
            "A6: understanding mechanism\n",
            "A7: knowledge storage\n",
            "Text: The purpose of Rank-One Model Editing (ROME) is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and in the MLP modules (ii (it) is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.\n",
            "\n",
            "\n",
            "Q1:Which middle layers determine the weights of transformer models?\n",
            "Q2:What does our analysis reveal?\n",
            "Best ans: MLPs at different middle layers determine the weights of transformer models.\n",
            "\n",
            "A0: This work presents a new method for weighting transformer models that uses feedforward MLPs to determine the last token of subject names.\n",
            "A1: We analyze transformer models that use a variety of middle layers to store factual knowledge about the world and test this finding in model weights.\n",
            "A2: MLPs at different middle layers determine the weights of transformer models.\n",
            "A3: In this paper, we investigate how factual knowledge is stored in transformer models. We first analyze the model weights of transformer models by introducing rank-one model editing.\n",
            "A4: fed forward\n",
            "A5: feeder MLP\n",
            "A6: input model weights\n",
            "A7: MLPs\n",
            "Text: Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.\n",
            "\n",
            "\n",
            "Q1:Is knowledge extraction and editing along multiple dimensions?\n",
            "Q2:What does this paper do?\n",
            "Best ans: This paper presents a method for measuring knowledge extraction and editing along multiple dimensions in a model. We use the Causal Tracing method to measure knowledge extraction. We compare our approach to previous methods in our experiments.\n",
            "\n",
            "A0: This paper presents a method for measuring knowledge extraction and editing along multiple dimensions in a model.\n",
            "A1: In this paper, we introduce COUNTERFACT, an approach to knowledge extraction that enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, and compare in the MLP modules (ii (it) to previous approaches.\n",
            "A2: This paper presents a method for measuring knowledge extraction and editing along multiple dimensions in a model. We use the Causal Tracing method to measure knowledge extraction. We compare our approach to previous methods in our experiments.\n",
            "A3: COUNTERFACT is a method for measuring knowledge extraction and editing along multiple dimensions. The method is based on the Causal Tracing method. The purpose of the method is to measure knowledge extraction along multiple dimension.\n",
            "A4: NO\n",
            "A5: No\n",
            "A6: no\n",
            "A7: COUNTERFACT\n",
            "Text: The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete in the MLP modules (ii (it) (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer Rank-One Model Editing (ROME) parameter intervention has comparable capabilities.\n",
            "\n",
            "\n",
            "Q1:Which method demonstrates both generalization and specificity?\n",
            "Q2:What is ROME based on?\n",
            "Best ans:  Rank-One Model Editing (ROME)is a method that rewrites MLP modules that output decisions. ROME demonstrates both generalization and specificity. ROMET is based on Causal Tracing.\n",
            "\n",
            "A0: Rome demonstrates both generalization and specificity.\n",
            "A1: Rome demonstrates both generalization and specificity. Rank-One Model Editing (ROME) is a method that generalizes and specificifies a model of a causal association.\n",
            "A2:  Rank-One Model Editing (ROME)is a method that rewrites MLP modules that output decisions. ROME demonstrates both generalization and specificity. ROMET is based on Causal Tracing.\n",
            "A3: The method Rank-One Model Editing (ROME) demonstrates both generalization and specificity.\n",
            "A4: \n",
            "A5: Rome\n",
            "A6: MODE\n",
            "A7: ROME\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n",
            "Q1:On what metrics do we observe strong correlation with causal analysis?\n",
            "Q2:What is ROME?\n",
            "Best ans:  Rank-One Model Editing (ROME)is a model editing method that demonstrates both generalization and specificity when targeted at various layers and tokens. ROME is an algorithm that rewrites MLP modules that output those states.\n",
            "\n",
            "A0: To confirm that factual associations are stored in the MLP modules that output those state, we test Rank-One Model Editing (ROME) on four metrics: Generalizability specificity\n",
            "A1:  Rank-One Model Editing (ROME)is a model editing method that demonstrates both generalization and specificity when targeted at various layers and tokens.\n",
            "A2:  Rank-One Model Editing (ROME)is a model editing method that demonstrates both generalization and specificity when targeted at various layers and tokens. ROME is an algorithm that rewrites MLP modules that output those states.\n",
            "A3: The layers at which edits generalize best correspond to the middle layers of the early site identified by 7 Rank-One Model Editing (ROME) demonstrates both generalization and specificity. ROME is a method that demonstrates generalization.\n",
            "A4: generalized\n",
            "A5: specialty\n",
            "A6: specificity\n",
            "A7: generalization and specificity.\n",
            "Text: In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test Rank-One Model Editing (ROME) s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "questions_df_filtered = copy.deepcopy(questions_df)\n",
        "questions_df_filtered['qs_sim'] = questions_df_filtered.apply(lambda x: np.dot(sentence_model.encode(x.question),sentence_model.encode(x.new_question)),axis=1)\n",
        "questions_df_filtered=questions_df_filtered.sort_values('qs_sim',ascending=False)\n",
        "idx = questions_df_filtered.index\n",
        "#print(questions_df_filtered.qs_sim)\n",
        "for i in range(len(questions_df)):\n",
        "       show_qs(questions_df_filtered,i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "section_n                                                                 1\n",
              "section_rank                                                            1.0\n",
              "text                       Carers of people with dementia have more symp...\n",
              "question                  How many criteria are there for clinical signi...\n",
              "question_ppl                                                       0.017122\n",
              "answer_1                  Three possible criteria for clinical signifi c...\n",
              "answer_2                  Three possible criteria for clinical signifi c...\n",
              "answer_3                  Three possible criteria for clinical signifi c...\n",
              "answer_4                  Three possible criteria for clinical signifi c...\n",
              "short_answer_1                                                          two\n",
              "short_answer_2                                                          one\n",
              "short_answer_3                                                         four\n",
              "short_answer_4                                                        Three\n",
              "generated_selected_ans    three possible criteria for clinical signifi c...\n",
              "selected_ans              Three possible criteria for clinical signifi c...\n",
              "new_question              How many criteria are there for clinical signi...\n",
              "Name: 9, dtype: object"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions_df.iloc[9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting pandasgui\n",
            "  Downloading pandasgui-0.2.13.tar.gz (215 kB)\n",
            "\u001b[K     |████████████████████████████████| 215 kB 28.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: IPython in /home/ubuntu/.local/lib/python3.8/site-packages (from pandasgui) (8.7.0)\n",
            "Collecting PyQt5\n",
            "  Downloading PyQt5-5.15.7-cp37-abi3-manylinux1_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 61.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting PyQt5-sip\n",
            "  Downloading PyQt5_sip-12.11.0-cp38-cp38-manylinux1_x86_64.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 86.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting PyQtWebEngine\n",
            "  Downloading PyQtWebEngine-5.15.6-cp37-abi3-manylinux1_x86_64.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 84.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting appdirs\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting astor\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from pandasgui) (1.23.5)\n",
            "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from pandasgui) (1.5.2)\n",
            "Requirement already satisfied: plotly in /home/ubuntu/.local/lib/python3.8/site-packages (from pandasgui) (5.11.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.0 MB 69.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pynput\n",
            "  Downloading pynput-1.7.6-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 15.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting qtstylish>=0.1.2\n",
            "  Downloading qtstylish-0.1.5.tar.gz (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 71.7 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from pandasgui) (45.2.0)\n",
            "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from pandasgui) (4.4.0)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.8.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (458 kB)\n",
            "\u001b[K     |████████████████████████████████| 458 kB 78.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (0.18.2)\n",
            "Requirement already satisfied: stack-data in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (0.6.2)\n",
            "Requirement already satisfied: traitlets>=5 in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (5.7.0)\n",
            "Requirement already satisfied: backcall in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (0.7.5)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (2.13.0)\n",
            "Requirement already satisfied: matplotlib-inline in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (3.0.36)\n",
            "Requirement already satisfied: decorator in /home/ubuntu/.local/lib/python3.8/site-packages (from IPython->pandasgui) (5.1.1)\n",
            "Collecting PyQt5-Qt5>=5.15.0\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.9 MB 165 kB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting PyQtWebEngine-Qt5>=5.15.0\n",
            "  Downloading PyQtWebEngine_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (67.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.5 MB 301 kB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->pandasgui) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->pandasgui) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from plotly->pandasgui) (8.1.0)\n",
            "Collecting evdev>=1.3; \"linux\" in sys_platform\n",
            "  Downloading evdev-1.6.0.tar.gz (26 kB)\n",
            "Requirement already satisfied: six in /home/ubuntu/.local/lib/python3.8/site-packages (from pynput->pandasgui) (1.16.0)\n",
            "Collecting python-xlib>=0.17; \"linux\" in sys_platform\n",
            "  Downloading python_xlib-0.32-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 79.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pillow in /home/ubuntu/.local/lib/python3.8/site-packages (from wordcloud->pandasgui) (9.3.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 69.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from jedi>=0.16->IPython->pandasgui) (0.8.3)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from stack-data->IPython->pandasgui) (1.2.0)\n",
            "Requirement already satisfied: pure-eval in /home/ubuntu/.local/lib/python3.8/site-packages (from stack-data->IPython->pandasgui) (0.2.2)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from stack-data->IPython->pandasgui) (2.2.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->IPython->pandasgui) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/ubuntu/.local/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->IPython->pandasgui) (0.2.5)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 78.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 87.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 72.6 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->wordcloud->pandasgui) (22.0)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting pyparsing>=2.2.1\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Building wheels for collected packages: pandasgui, qtstylish, evdev\n",
            "  Building wheel for pandasgui (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pandasgui: filename=pandasgui-0.2.13-py3-none-any.whl size=233705 sha256=1ee4fbd4254829c17e89636112f45021ccdb5ca2fab5a3ab0b638c447d6abef3\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/17/d6/78/9058f28b6208399e1ea3b0638a071aba64e21d3d384c42cc15\n",
            "  Building wheel for qtstylish (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for qtstylish: filename=qtstylish-0.1.5-py3-none-any.whl size=1029272 sha256=8cd025fbca293fc8324a2db180f157772d324b1711b59edd0686eb9fad1eb7a7\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/88/6f/75/b39bf9ba496bc5f025e3d66cdb1e1f7c2dae210f212b084bf7\n",
            "  Building wheel for evdev (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for evdev: filename=evdev-1.6.0-cp38-cp38-linux_x86_64.whl size=99473 sha256=4ba1087c77284a15557d7a9835cbb55032270d03d4b3a353155640329b1ca137\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/4b/94/09/d70322e50a9ee7e046d4f41b987ee33cd862932cdf03f7929c\n",
            "Successfully built pandasgui qtstylish evdev\n",
            "Installing collected packages: PyQt5-sip, PyQt5-Qt5, PyQt5, PyQtWebEngine-Qt5, PyQtWebEngine, appdirs, astor, pyarrow, evdev, python-xlib, pynput, qtstylish, fonttools, contourpy, kiwisolver, cycler, pyparsing, matplotlib, wordcloud, pandasgui\n",
            "\u001b[33m  WARNING: The scripts pylupdate5, pyrcc5 and pyuic5 are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script plasma_store is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script wordcloud_cli is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed PyQt5-5.15.7 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0 PyQtWebEngine-5.15.6 PyQtWebEngine-Qt5-5.15.2 appdirs-1.4.4 astor-0.8.1 contourpy-1.0.6 cycler-0.11.0 evdev-1.6.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2 pandasgui-0.2.13 pyarrow-10.0.1 pynput-1.7.6 pyparsing-3.0.9 python-xlib-0.32 qtstylish-0.1.5 wordcloud-1.8.2.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "this platform is not supported: ('failed to acquire X connection: Bad display name \"\"', DisplayNameError(''))\n\nTry one of the following resolutions:\n\n * Please make sure that you have an X server running, and that the DISPLAY environment variable is set correctly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mpip\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minstall pandasgui\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m \u001b[39mimport\u001b[39;00m show\n\u001b[1;32m      4\u001b[0m show(questions_df)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandasgui/__init__.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m logger\u001b[39m.\u001b[39maddHandler(sh)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Imports\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgui\u001b[39;00m \u001b[39mimport\u001b[39;00m show\n\u001b[1;32m     17\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mshow\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandasgui/gui.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfind_toolbar\u001b[39;00m \u001b[39mimport\u001b[39;00m FindToolbar\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjson_viewer\u001b[39;00m \u001b[39mimport\u001b[39;00m JsonViewer\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnavigator\u001b[39;00m \u001b[39mimport\u001b[39;00m Navigator\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfigure_viewer\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureViewer\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msettings_editor\u001b[39;00m \u001b[39mimport\u001b[39;00m SettingsEditor\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandasgui/widgets/navigator.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[39mreturn\u001b[39;00m win32api\u001b[39m.\u001b[39mGetKeyState(\u001b[39m0x01\u001b[39m) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpynput\u001b[39;00m \u001b[39mimport\u001b[39;00m mouse\n\u001b[1;32m     27\u001b[0m     \u001b[39mclass\u001b[39;00m \u001b[39mMouseState\u001b[39;00m(mouse\u001b[39m.\u001b[39mListener):\n\u001b[1;32m     28\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pynput/__init__.py:40\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     36\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m)[:\u001b[39m2\u001b[39m]),\n\u001b[1;32m     37\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n\u001b[0;32m---> 40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m keyboard\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m mouse\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pynput/keyboard/__init__.py:31\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpynput\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m backend, Events\n\u001b[0;32m---> 31\u001b[0m backend \u001b[39m=\u001b[39m backend(\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m KeyCode \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mKeyCode\n\u001b[1;32m     33\u001b[0m Key \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mKey\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pynput/_util/__init__.py:76\u001b[0m, in \u001b[0;36mbackend\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[39mif\u001b[39;00m module \u001b[39min\u001b[39;00m RESOLUTIONS:\n\u001b[1;32m     74\u001b[0m             resolutions\u001b[39m.\u001b[39mappend(RESOLUTIONS[module])\n\u001b[0;32m---> 76\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mthis platform is not supported: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     77\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(e) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m errors)) \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mTry one of the following resolutions:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     80\u001b[0m             \u001b[39m'\u001b[39m\u001b[39m * \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(s)\n\u001b[1;32m     81\u001b[0m             \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m resolutions))\n\u001b[1;32m     82\u001b[0m         \u001b[39mif\u001b[39;00m resolutions \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mImportError\u001b[0m: this platform is not supported: ('failed to acquire X connection: Bad display name \"\"', DisplayNameError(''))\n\nTry one of the following resolutions:\n\n * Please make sure that you have an X server running, and that the DISPLAY environment variable is set correctly"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/home/ubuntu/.local/lib/python3.8/site-packages/pynput/_util/__init__.py\u001b[0m(76)\u001b[0;36mbackend\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     74 \u001b[0;31m                \u001b[0mresolutions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESOLUTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     75 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 76 \u001b[0;31m    raise ImportError('this platform is not supported: {}'.format(\n",
            "\u001b[0m\u001b[0;32m     77 \u001b[0;31m        \u001b[0;34m'; '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     78 \u001b[0;31m            \u001b[0;34m'Try one of the following resolutions:\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install pandasgui\n",
        "from pandasgui import show\n",
        "i=10\n",
        "show(questions_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEPj3tmxFt7_",
        "outputId": "d88cd105-d09e-4f5b-800a-681a4f4cd89e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'What is COMET?'}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(format_inputs(questions_df.loc[i,'text'], questions_df.loc[i,'new_ans']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_eezkGSUFS0M",
        "outputId": "827e58d3-07a9-4f4c-a5f3-5a444b4d1d6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7a8ba0d9-e16c-4fc7-bd8a-75ff4155e044\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>section_n</th>\n",
              "      <th>section_rank</th>\n",
              "      <th>text</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_1</th>\n",
              "      <th>answer_2</th>\n",
              "      <th>answer_3</th>\n",
              "      <th>answer_4</th>\n",
              "      <th>question_ppl</th>\n",
              "      <th>selected_ans</th>\n",
              "      <th>new_ans</th>\n",
              "      <th>new_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.2797</td>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>Where does this paper come from?</td>\n",
              "      <td>Using COMET for Text Summarization</td>\n",
              "      <td>Using COMET for Text Summarization Evaluation</td>\n",
              "      <td>Annotated summarization outputs are used for p...</td>\n",
              "      <td>The COMET model is a multilingual evaluation m...</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>the comet model is a multilingual evaluation m...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is COMET?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.2797</td>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>Using COMET to evaluate text summarization sys...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>0.001738</td>\n",
              "      <td>we introduce a variant of the model – comes – ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is COMET?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.2797</td>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>Which machine translation metrics can be used ...</td>\n",
              "      <td>We introduce a variant of the COMET model – tr...</td>\n",
              "      <td>In this paper, we introduce a variant of the m...</td>\n",
              "      <td>In this paper, we introduce a variant of the m...</td>\n",
              "      <td>In this paper, we introduce a variant of the m...</td>\n",
              "      <td>0.002084</td>\n",
              "      <td>in this paper, we introduce a variant of the m...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is COMET?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.2797</td>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>What is the purpose of this paper?</td>\n",
              "      <td>An evaluation model for text summarization</td>\n",
              "      <td>Evaluation of Text Summarization Systems</td>\n",
              "      <td>Use COMET to evaluate text summarization systems.</td>\n",
              "      <td>Evaluation of Text Summarization Systems using...</td>\n",
              "      <td>0.005797</td>\n",
              "      <td>evaluation of text summarization systems</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is COMET?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2835</td>\n",
              "      <td>Since manual annotation for any generative ta...</td>\n",
              "      <td>Why are human judgments difficult to collect?</td>\n",
              "      <td>This paper presents a variant of the COMES2 mo...</td>\n",
              "      <td>COMES2 is an automatic model that uses the ann...</td>\n",
              "      <td>This paper presents a variant of the COMES2 mo...</td>\n",
              "      <td>One of the issues making research on summary e...</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>one of the issues making research on summary e...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the COMET metric?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2835</td>\n",
              "      <td>Since manual annotation for any generative ta...</td>\n",
              "      <td>What is the purpose of the COMET2 model?</td>\n",
              "      <td>A variant of the WMT model – COMES2 – that use...</td>\n",
              "      <td>COMET2 is a variant of the WMT model that uses...</td>\n",
              "      <td>COMET2 is a variant of the Metrics Shared Task...</td>\n",
              "      <td>COMET2 is a variant of the Metrics Shared Task...</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>comet2 is a variant of the wmt model that uses...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the COMET metric?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2835</td>\n",
              "      <td>Since manual annotation for any generative ta...</td>\n",
              "      <td>Which metrics are used to evaluate summary qua...</td>\n",
              "      <td>COMET metric by Rei et al. (2020)</td>\n",
              "      <td>COMET metric by Rei et al. (2020) is a metric ...</td>\n",
              "      <td>A variant of the model COMES2 is capable of pr...</td>\n",
              "      <td>COMET metric by Rei et al. (2020) is a metric ...</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>comet metric by rei et al. (2020) is a metric ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the COMET metric?'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>0.2835</td>\n",
              "      <td>COMET is a trained metric that, based on sema...</td>\n",
              "      <td>Comet uses what method of scoring?</td>\n",
              "      <td>the human perception of translation quality</td>\n",
              "      <td>based on semantic similarities</td>\n",
              "      <td>Semantic similarities between the translated a...</td>\n",
              "      <td>semantic similarities between the translated a...</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>semantic similarities between the translated a...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does COMET learn to o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>0.2835</td>\n",
              "      <td>COMET is a trained metric that, based on sema...</td>\n",
              "      <td>Why do you need to train your COMET model?</td>\n",
              "      <td>The COMET model is trained to output a score t...</td>\n",
              "      <td>The COMET model is trained to extract represen...</td>\n",
              "      <td>Pre-trained multilingual language models are u...</td>\n",
              "      <td>Pre-trained multilingual language models are u...</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>pre-trained multilingual language models are u...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does COMET learn to o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>0.2835</td>\n",
              "      <td>COMET is a trained metric that, based on sema...</td>\n",
              "      <td>How does COMET compare to the human perception...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does COMET learn to o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2848</td>\n",
              "      <td>SummEval4 is a recently proposed dataset with...</td>\n",
              "      <td>Where did CNN/DailyMail come from?</td>\n",
              "      <td>Nallapati et al., 2016 CNN/DailyMail corpus</td>\n",
              "      <td>The CNN/DailyMail corpus (Nallapati et al., 20...</td>\n",
              "      <td>CNN/DailyMail corpus (Nallapati et al., 2016)</td>\n",
              "      <td>The CNN/DailyMail corpus (Nallapati et al., 20...</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>cnn/dailymail corpus (nallapati et al., 2016) ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the 3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2848</td>\n",
              "      <td>SummEval4 is a recently proposed dataset with...</td>\n",
              "      <td>Why is SummEval4 dataset used?</td>\n",
              "      <td>A recently proposed dataset with human annotat...</td>\n",
              "      <td>It consists of 100 articles randomly sampled f...</td>\n",
              "      <td>SummEval4 is a recently proposed dataset with ...</td>\n",
              "      <td>For each system output, the authors collected ...</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>summeval4 is a recently proposed dataset with ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the 3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2848</td>\n",
              "      <td>SummEval4 is a recently proposed dataset with...</td>\n",
              "      <td>When was this dataset proposed?</td>\n",
              "      <td>2017</td>\n",
              "      <td>2016</td>\n",
              "      <td>Nallapati et al., 2016</td>\n",
              "      <td>The dataset was proposed in 2016.</td>\n",
              "      <td>0.001915</td>\n",
              "      <td>2016</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the 3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2848</td>\n",
              "      <td>SummEval4 is a recently proposed dataset with...</td>\n",
              "      <td>In which dimension of summary quality is SummE...</td>\n",
              "      <td>In SummEval4, each article is summarized by 17...</td>\n",
              "      <td>In SummEval4, each article is summarized by 17...</td>\n",
              "      <td>A recently proposed dataset with human annotat...</td>\n",
              "      <td>The authors collected expert judgments for Coh...</td>\n",
              "      <td>0.004925</td>\n",
              "      <td>in summeval4, each article is summarized by 17...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the 3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2848</td>\n",
              "      <td>SummEval4 is a recently proposed dataset with...</td>\n",
              "      <td>For each summary system, how many expert judgm...</td>\n",
              "      <td>Authors collected 3 expert judgments for each ...</td>\n",
              "      <td>Authors collected 3 expert judgments for Coher...</td>\n",
              "      <td>Authors collected 3 expert judgments for Coher...</td>\n",
              "      <td>For each summary system, the authors collected...</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>authors collected 3 expert judgments for coher...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the 3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2854</td>\n",
              "      <td>The COMET metric trained on MT data outputs a...</td>\n",
              "      <td>Does the proposed method improve on existing m...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>A modified version of the COMET method.</td>\n",
              "      <td>A modified version of the COMET method is prop...</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>yes</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2854</td>\n",
              "      <td>The COMET metric trained on MT data outputs a...</td>\n",
              "      <td>For what purpose does this work?</td>\n",
              "      <td>This work is about improving the performance o...</td>\n",
              "      <td>We experiment with both training from scratch ...</td>\n",
              "      <td>For the purpose of this paper, we propose a mo...</td>\n",
              "      <td>For the purpose of this work, we propose a mod...</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>for the purpose of this work, we propose a mod...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2854</td>\n",
              "      <td>The COMET metric trained on MT data outputs a...</td>\n",
              "      <td>Which metric is used for evaluating the qualit...</td>\n",
              "      <td>CoMet is used for training on text annotations...</td>\n",
              "      <td>Content: The COMET metric trained on MT data o...</td>\n",
              "      <td>Content: The COMET metric trained on MT data o...</td>\n",
              "      <td>COMET metric trained on MT data outputs a sing...</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>comet metric trained on mt data outputs a sing...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2854</td>\n",
              "      <td>The COMET metric trained on MT data outputs a...</td>\n",
              "      <td>Why do we need to change how the COMET metric ...</td>\n",
              "      <td>This modification changes the number of output...</td>\n",
              "      <td>This modification changes the number of output...</td>\n",
              "      <td>This modification changes the number of output...</td>\n",
              "      <td>The COMET metric trained on MT data outputs a ...</td>\n",
              "      <td>0.001986</td>\n",
              "      <td>this modification changes the number of output...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>Rei et al. would like to use it both for trai...</td>\n",
              "      <td>Does cross-validation improve performance?</td>\n",
              "      <td>In Rei et al.'s experiments, we used 80 articl...</td>\n",
              "      <td>During evaluation, we handle multiple referenc...</td>\n",
              "      <td>During evaluation, we handle multiple referenc...</td>\n",
              "      <td>During evaluation, we handle multiple referenc...</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>during evaluation, we handle multiple referenc...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does the scoring syst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>Rei et al. would like to use it both for trai...</td>\n",
              "      <td>how do they use the data?</td>\n",
              "      <td>Use the data for training and evaluation.</td>\n",
              "      <td>Use the data for training and evaluation</td>\n",
              "      <td>The data is used for training and evaluation.</td>\n",
              "      <td>The data is used both for training and evaluat...</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>the data is used for training and evaluation.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does the scoring syst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>Rei et al. would like to use it both for trai...</td>\n",
              "      <td>When training, how many articles are used?</td>\n",
              "      <td>80 articles are used in cross-validation.</td>\n",
              "      <td>Use 80 articles per subset for training, 10 fo...</td>\n",
              "      <td>80 articles are used in cross-validation. Duri...</td>\n",
              "      <td>During training, we use each reference and eac...</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>80 articles per subset for training, 10 for va...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does the scoring syst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>Rei et al. would like to use it both for trai...</td>\n",
              "      <td>What is the method of cross-validation used fo...</td>\n",
              "      <td>Use each reference and each expert annotation ...</td>\n",
              "      <td>During training, we use each reference and eac...</td>\n",
              "      <td>During training, we use each reference and eac...</td>\n",
              "      <td>We split the data into 10 subsets of 10 articl...</td>\n",
              "      <td>0.005417</td>\n",
              "      <td>during training, we use each reference and eac...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does the scoring syst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>Rei et al. would like to use it both for trai...</td>\n",
              "      <td>Which metrics are used to evaluate the perform...</td>\n",
              "      <td>In order to evaluate the model, we use the fol...</td>\n",
              "      <td>During evaluation, we handle multiple referenc...</td>\n",
              "      <td>Scoring system output with both out-of-the-box...</td>\n",
              "      <td>Scoring system output with both out-of-the-box...</td>\n",
              "      <td>0.007615</td>\n",
              "      <td>in order to evaluate the model, we use the fol...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What does the scoring syst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6</td>\n",
              "      <td>0.2873</td>\n",
              "      <td>Bhandari et al. (2020) produced the numerical...</td>\n",
              "      <td>what are some examples?</td>\n",
              "      <td>Stiennon et al.(2020) annotated a different su...</td>\n",
              "      <td>Stiennon et al.(2020) annotated a different su...</td>\n",
              "      <td>COMET_QE outperforms any other variant, almost...</td>\n",
              "      <td>Stiennon et al.(2020) annotated a different su...</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>stiennon et al.(2020) annotated a different su...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the best result of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>6</td>\n",
              "      <td>0.2873</td>\n",
              "      <td>Bhandari et al. (2020) produced the numerical...</td>\n",
              "      <td>Why do we use COMET_QE?</td>\n",
              "      <td>A reference-less COMET_QE outperforms any othe...</td>\n",
              "      <td>On this dataset, the COMET_QE variant outperfo...</td>\n",
              "      <td>This paper presents a new approach to the eval...</td>\n",
              "      <td>Compared to COMET_QE, the reference-less varia...</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>on this dataset, the comet_qe variant outperfo...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the best result of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>6</td>\n",
              "      <td>0.2873</td>\n",
              "      <td>Bhandari et al. (2020) produced the numerical...</td>\n",
              "      <td>Who produced numerical gold-standard scores by...</td>\n",
              "      <td>R. Bhandari et al.</td>\n",
              "      <td>R. Bhandari et al. (2020)</td>\n",
              "      <td>Bhandari et al</td>\n",
              "      <td>Bhandari et al. (2020)</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>bhandari et al. (2020)</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the best result of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>6</td>\n",
              "      <td>0.2873</td>\n",
              "      <td>Bhandari et al. (2020) produced the numerical...</td>\n",
              "      <td>What is the best correlation between the COMET...</td>\n",
              "      <td>Using the CNN/DailyMail dataset, the COMET_QE ...</td>\n",
              "      <td>Using the CNN/DailyMail dataset, the COMET_QE ...</td>\n",
              "      <td>Using the CNN/DailyMail dataset, the COMET_QE ...</td>\n",
              "      <td>Using the CNN/DailyMail dataset, the COMET_QE ...</td>\n",
              "      <td>0.000855</td>\n",
              "      <td>using the cnn/dailymail dataset, the comet_qe ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the best result of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>7</td>\n",
              "      <td>0.2866</td>\n",
              "      <td>One of the strengths of the COMET metric is i...</td>\n",
              "      <td>Who developed Multi_SummEval dataset?</td>\n",
              "      <td>Koloto et al.</td>\n",
              "      <td>Koto and colleagues</td>\n",
              "      <td>Koto et al</td>\n",
              "      <td>Koto et al., 2020</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>koto et al.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the strength of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>7</td>\n",
              "      <td>0.2866</td>\n",
              "      <td>One of the strengths of the COMET metric is i...</td>\n",
              "      <td>On which dataset did we train a COMET model?</td>\n",
              "      <td>Multi_SummEval</td>\n",
              "      <td>Koto et al., 2021 Multi_SummEval dataset</td>\n",
              "      <td>Multi_SummEval dataset</td>\n",
              "      <td>Multi_SummEval dataset (Koto et al., 2021)</td>\n",
              "      <td>0.003008</td>\n",
              "      <td>multi_summeval dataset (koto et al., 2021)</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the strength of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>7</td>\n",
              "      <td>0.2866</td>\n",
              "      <td>One of the strengths of the COMET metric is i...</td>\n",
              "      <td>Is multilinguality a strength of the COMET model?</td>\n",
              "      <td>Multilinguality is a strength of the COMET met...</td>\n",
              "      <td>It has seen over 30 language pairs during trai...</td>\n",
              "      <td>Multilinguality is a strength of the COMET met...</td>\n",
              "      <td>On the Multi_SummEval dataset (Koto et al., 20...</td>\n",
              "      <td>0.004863</td>\n",
              "      <td>multilinguality is a strength of the comet met...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the strength of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>7</td>\n",
              "      <td>0.2866</td>\n",
              "      <td>One of the strengths of the COMET metric is i...</td>\n",
              "      <td>Do summary evaluations survive translation?</td>\n",
              "      <td>The Multi_SummEval dataset (Koto et al., 2021)...</td>\n",
              "      <td>On the Multi_SummEval dataset, even the best p...</td>\n",
              "      <td>On the Multi_SummEval dataset, even the best p...</td>\n",
              "      <td>On the Multi_SummEval dataset, even the best p...</td>\n",
              "      <td>0.005573</td>\n",
              "      <td>on the multi_summeval dataset, even the best p...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the strength of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>7</td>\n",
              "      <td>0.2866</td>\n",
              "      <td>One of the strengths of the COMET metric is i...</td>\n",
              "      <td>How many language pairs were used in the Multi...</td>\n",
              "      <td>One of the strengths of the COMET metric is it...</td>\n",
              "      <td>In the Multi_SummEval dataset, the COMET model...</td>\n",
              "      <td>In the Multi_SummEval dataset, the COMET model...</td>\n",
              "      <td>With only two system outputs annotated, the si...</td>\n",
              "      <td>0.012023</td>\n",
              "      <td>in the multi_summeval dataset, the comet model...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the strength of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>8</td>\n",
              "      <td>0.2793</td>\n",
              "      <td>In Section 4.1, we propose the usage of cross...</td>\n",
              "      <td>Who proposed crossvalidation?</td>\n",
              "      <td>We propose the usage of crossvalidation in Sec...</td>\n",
              "      <td>We propose the usage of crossvalidation to ena...</td>\n",
              "      <td>We propose the usage of crossvalidation to ena...</td>\n",
              "      <td>We propose the usage of crossvalidation to ena...</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>we propose the usage of crossvalidation to ena...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the purpose of cro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>8</td>\n",
              "      <td>0.2793</td>\n",
              "      <td>In Section 4.1, we propose the usage of cross...</td>\n",
              "      <td>On what dataset does this work apply crossvali...</td>\n",
              "      <td>SummEval dataset.</td>\n",
              "      <td>On the SummEval dataset.</td>\n",
              "      <td>SummEval dataset</td>\n",
              "      <td>The SummEval dataset is used in this work.</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>summeval dataset.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the purpose of cro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>8</td>\n",
              "      <td>0.2793</td>\n",
              "      <td>In Section 4.1, we propose the usage of cross...</td>\n",
              "      <td>Where does this work take place?</td>\n",
              "      <td>SummEval dataset</td>\n",
              "      <td>On the SummEval dataset</td>\n",
              "      <td>On the SummEval dataset.</td>\n",
              "      <td>The work is done on the SummEval dataset.</td>\n",
              "      <td>0.002421</td>\n",
              "      <td>on the summeval dataset</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the purpose of cro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>8</td>\n",
              "      <td>0.2793</td>\n",
              "      <td>In Section 4.1, we propose the usage of cross...</td>\n",
              "      <td>Why do we use crossvalidation?</td>\n",
              "      <td>Using crossvalidation, we can train a model us...</td>\n",
              "      <td>A model can be trained and tested on different...</td>\n",
              "      <td>Different articles are used for training, vali...</td>\n",
              "      <td>We propose the usage of crossvalidation to ena...</td>\n",
              "      <td>0.003005</td>\n",
              "      <td>a model can be trained and tested on different...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the purpose of cro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>8</td>\n",
              "      <td>0.2793</td>\n",
              "      <td>In Section 4.1, we propose the usage of cross...</td>\n",
              "      <td>Which dataset is used for training and testing...</td>\n",
              "      <td>Different articles are used for training, vali...</td>\n",
              "      <td>SummEval dataset</td>\n",
              "      <td>On the SummEval dataset, different articles ar...</td>\n",
              "      <td>SummEval dataset is used for training and test...</td>\n",
              "      <td>0.005263</td>\n",
              "      <td>summeval dataset</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the purpose of cro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>9</td>\n",
              "      <td>0.2875</td>\n",
              "      <td>During training, we mostly follow the trainin...</td>\n",
              "      <td>We use what method for preprocessing?</td>\n",
              "      <td>Stanford CoreNLP tool</td>\n",
              "      <td>we de-tokenize and true-case system outputs wi...</td>\n",
              "      <td>De-tokenize and true-case system outputs with ...</td>\n",
              "      <td>de-tokenize and true-case system outputs with ...</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>de-tokenize and true-case system outputs with ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>9</td>\n",
              "      <td>0.2875</td>\n",
              "      <td>During training, we mostly follow the trainin...</td>\n",
              "      <td>Whose work is this paper an example of?</td>\n",
              "      <td>Stanford CoreNLP tool</td>\n",
              "      <td>Rei et al.</td>\n",
              "      <td>The paper is an example of Rei et al. (2021)</td>\n",
              "      <td>This paper is an example of Rei et al. (2021)</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>rei et al.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>9</td>\n",
              "      <td>0.2875</td>\n",
              "      <td>During training, we mostly follow the trainin...</td>\n",
              "      <td>When training, what do they use?</td>\n",
              "      <td>Tokenization and true-case system outputs with...</td>\n",
              "      <td>Tokenization and true-case system outputs with...</td>\n",
              "      <td>gradient accumulation to train with the effect...</td>\n",
              "      <td>Gradient accumulation to train with the effect...</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>gradient accumulation to train with the effect...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>9</td>\n",
              "      <td>0.2875</td>\n",
              "      <td>During training, we mostly follow the trainin...</td>\n",
              "      <td>How many training batches are employed in this...</td>\n",
              "      <td>This paper employs a batch size of 40.</td>\n",
              "      <td>The effective batch size of the training is 40.</td>\n",
              "      <td>In this paper, we employ a batch size of 40.</td>\n",
              "      <td>In this paper, we employ a batch size of 40. S...</td>\n",
              "      <td>0.000506</td>\n",
              "      <td>in this paper, we employ a batch size of 40.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>9</td>\n",
              "      <td>0.2875</td>\n",
              "      <td>During training, we mostly follow the trainin...</td>\n",
              "      <td>What is the training method used?</td>\n",
              "      <td>As a part of pre-processing, we de-tokenize an...</td>\n",
              "      <td>As a part of pre-processing, we de-tokenize an...</td>\n",
              "      <td>We monitor Pearson correlation on the validati...</td>\n",
              "      <td>We employ gradient accumulation to train with ...</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>we employ gradient accumulation to train with ...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>10</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>Table 5: System-level Kendall’s Tau correlati...</td>\n",
              "      <td>Whose datasets were used for this study?</td>\n",
              "      <td>Bhandari et al.</td>\n",
              "      <td>Bhandari et al. (2020) CNN/DailyMail corpus.</td>\n",
              "      <td>Bhandari et al. (2020)</td>\n",
              "      <td>Bhandari et al. (2020) CNN/DailyMail dataset</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>bhandari et al. (2020) cnn/dailymail corpus.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>10</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>Table 5: System-level Kendall’s Tau correlati...</td>\n",
              "      <td>On what corpus did we evaluate our model?</td>\n",
              "      <td>CNN/DailyMail</td>\n",
              "      <td>The CNN/DailyMail corpus.</td>\n",
              "      <td>On the CNN/DailyMail corpus.</td>\n",
              "      <td>The CNN/DailyMail corpus is used to evaluate o...</td>\n",
              "      <td>0.003838</td>\n",
              "      <td>the cnn/dailymail corpus.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>10</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>Table 5: System-level Kendall’s Tau correlati...</td>\n",
              "      <td>How many metrics are evaluated?</td>\n",
              "      <td>A total of 6 metrics are evaluated.</td>\n",
              "      <td>A total of 6 metrics are evaluated. The three ...</td>\n",
              "      <td>We evaluate on the subset of the test split of...</td>\n",
              "      <td>We evaluate on the subset of the test split of...</td>\n",
              "      <td>0.004689</td>\n",
              "      <td>a total of 6 metrics are evaluated.</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>10</td>\n",
              "      <td>0.2851</td>\n",
              "      <td>Table 5: System-level Kendall’s Tau correlati...</td>\n",
              "      <td>Which metrics have the highest correlations?</td>\n",
              "      <td>On the CNN/DailyMail corpus, we evaluate on th...</td>\n",
              "      <td>The three metrics with the highest correlation...</td>\n",
              "      <td>We evaluate on the subset of the test split of...</td>\n",
              "      <td>System-level Kendall’s Tau correlations on the...</td>\n",
              "      <td>0.013755</td>\n",
              "      <td>the three metrics with the highest correlation...</td>\n",
              "      <td></td>\n",
              "      <td>{'generated_text': 'What is the name of the me...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a8ba0d9-e16c-4fc7-bd8a-75ff4155e044')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a8ba0d9-e16c-4fc7-bd8a-75ff4155e044 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a8ba0d9-e16c-4fc7-bd8a-75ff4155e044');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   section_n  section_rank                                               text  \\\n",
              "0          0        0.2797  COMET is a recently proposed trainable neuralb...   \n",
              "1          0        0.2797  COMET is a recently proposed trainable neuralb...   \n",
              "2          0        0.2797  COMET is a recently proposed trainable neuralb...   \n",
              "3          0        0.2797  COMET is a recently proposed trainable neuralb...   \n",
              "4          1        0.2835   Since manual annotation for any generative ta...   \n",
              "5          1        0.2835   Since manual annotation for any generative ta...   \n",
              "6          1        0.2835   Since manual annotation for any generative ta...   \n",
              "7          2        0.2835   COMET is a trained metric that, based on sema...   \n",
              "8          2        0.2835   COMET is a trained metric that, based on sema...   \n",
              "9          2        0.2835   COMET is a trained metric that, based on sema...   \n",
              "10         3        0.2848   SummEval4 is a recently proposed dataset with...   \n",
              "11         3        0.2848   SummEval4 is a recently proposed dataset with...   \n",
              "12         3        0.2848   SummEval4 is a recently proposed dataset with...   \n",
              "13         3        0.2848   SummEval4 is a recently proposed dataset with...   \n",
              "14         3        0.2848   SummEval4 is a recently proposed dataset with...   \n",
              "15         4        0.2854   The COMET metric trained on MT data outputs a...   \n",
              "16         4        0.2854   The COMET metric trained on MT data outputs a...   \n",
              "17         4        0.2854   The COMET metric trained on MT data outputs a...   \n",
              "18         4        0.2854   The COMET metric trained on MT data outputs a...   \n",
              "19         5        0.2832   Rei et al. would like to use it both for trai...   \n",
              "20         5        0.2832   Rei et al. would like to use it both for trai...   \n",
              "21         5        0.2832   Rei et al. would like to use it both for trai...   \n",
              "22         5        0.2832   Rei et al. would like to use it both for trai...   \n",
              "23         5        0.2832   Rei et al. would like to use it both for trai...   \n",
              "24         6        0.2873   Bhandari et al. (2020) produced the numerical...   \n",
              "25         6        0.2873   Bhandari et al. (2020) produced the numerical...   \n",
              "26         6        0.2873   Bhandari et al. (2020) produced the numerical...   \n",
              "27         6        0.2873   Bhandari et al. (2020) produced the numerical...   \n",
              "28         7        0.2866   One of the strengths of the COMET metric is i...   \n",
              "29         7        0.2866   One of the strengths of the COMET metric is i...   \n",
              "30         7        0.2866   One of the strengths of the COMET metric is i...   \n",
              "31         7        0.2866   One of the strengths of the COMET metric is i...   \n",
              "32         7        0.2866   One of the strengths of the COMET metric is i...   \n",
              "33         8        0.2793   In Section 4.1, we propose the usage of cross...   \n",
              "34         8        0.2793   In Section 4.1, we propose the usage of cross...   \n",
              "35         8        0.2793   In Section 4.1, we propose the usage of cross...   \n",
              "36         8        0.2793   In Section 4.1, we propose the usage of cross...   \n",
              "37         8        0.2793   In Section 4.1, we propose the usage of cross...   \n",
              "38         9        0.2875   During training, we mostly follow the trainin...   \n",
              "39         9        0.2875   During training, we mostly follow the trainin...   \n",
              "40         9        0.2875   During training, we mostly follow the trainin...   \n",
              "41         9        0.2875   During training, we mostly follow the trainin...   \n",
              "42         9        0.2875   During training, we mostly follow the trainin...   \n",
              "43        10        0.2851   Table 5: System-level Kendall’s Tau correlati...   \n",
              "44        10        0.2851   Table 5: System-level Kendall’s Tau correlati...   \n",
              "45        10        0.2851   Table 5: System-level Kendall’s Tau correlati...   \n",
              "46        10        0.2851   Table 5: System-level Kendall’s Tau correlati...   \n",
              "\n",
              "                                             question  \\\n",
              "0                    Where does this paper come from?   \n",
              "1   Using COMET to evaluate text summarization sys...   \n",
              "2   Which machine translation metrics can be used ...   \n",
              "3                  What is the purpose of this paper?   \n",
              "4       Why are human judgments difficult to collect?   \n",
              "5            What is the purpose of the COMET2 model?   \n",
              "6   Which metrics are used to evaluate summary qua...   \n",
              "7                  Comet uses what method of scoring?   \n",
              "8          Why do you need to train your COMET model?   \n",
              "9   How does COMET compare to the human perception...   \n",
              "10                 Where did CNN/DailyMail come from?   \n",
              "11                     Why is SummEval4 dataset used?   \n",
              "12                    When was this dataset proposed?   \n",
              "13  In which dimension of summary quality is SummE...   \n",
              "14  For each summary system, how many expert judgm...   \n",
              "15  Does the proposed method improve on existing m...   \n",
              "16                   For what purpose does this work?   \n",
              "17  Which metric is used for evaluating the qualit...   \n",
              "18  Why do we need to change how the COMET metric ...   \n",
              "19         Does cross-validation improve performance?   \n",
              "20                          how do they use the data?   \n",
              "21         When training, how many articles are used?   \n",
              "22  What is the method of cross-validation used fo...   \n",
              "23  Which metrics are used to evaluate the perform...   \n",
              "24                            what are some examples?   \n",
              "25                            Why do we use COMET_QE?   \n",
              "26  Who produced numerical gold-standard scores by...   \n",
              "27  What is the best correlation between the COMET...   \n",
              "28              Who developed Multi_SummEval dataset?   \n",
              "29       On which dataset did we train a COMET model?   \n",
              "30  Is multilinguality a strength of the COMET model?   \n",
              "31        Do summary evaluations survive translation?   \n",
              "32  How many language pairs were used in the Multi...   \n",
              "33                      Who proposed crossvalidation?   \n",
              "34  On what dataset does this work apply crossvali...   \n",
              "35                   Where does this work take place?   \n",
              "36                     Why do we use crossvalidation?   \n",
              "37  Which dataset is used for training and testing...   \n",
              "38              We use what method for preprocessing?   \n",
              "39            Whose work is this paper an example of?   \n",
              "40                   When training, what do they use?   \n",
              "41  How many training batches are employed in this...   \n",
              "42                  What is the training method used?   \n",
              "43           Whose datasets were used for this study?   \n",
              "44          On what corpus did we evaluate our model?   \n",
              "45                    How many metrics are evaluated?   \n",
              "46       Which metrics have the highest correlations?   \n",
              "\n",
              "                                             answer_1  \\\n",
              "0                  Using COMET for Text Summarization   \n",
              "1   We introduce a variant of the model – COMES – ...   \n",
              "2   We introduce a variant of the COMET model – tr...   \n",
              "3          An evaluation model for text summarization   \n",
              "4   This paper presents a variant of the COMES2 mo...   \n",
              "5   A variant of the WMT model – COMES2 – that use...   \n",
              "6                   COMET metric by Rei et al. (2020)   \n",
              "7         the human perception of translation quality   \n",
              "8   The COMET model is trained to output a score t...   \n",
              "9   based on semantic similarities between the tra...   \n",
              "10        Nallapati et al., 2016 CNN/DailyMail corpus   \n",
              "11  A recently proposed dataset with human annotat...   \n",
              "12                                               2017   \n",
              "13  In SummEval4, each article is summarized by 17...   \n",
              "14  Authors collected 3 expert judgments for each ...   \n",
              "15                                                Yes   \n",
              "16  This work is about improving the performance o...   \n",
              "17  CoMet is used for training on text annotations...   \n",
              "18  This modification changes the number of output...   \n",
              "19  In Rei et al.'s experiments, we used 80 articl...   \n",
              "20          Use the data for training and evaluation.   \n",
              "21          80 articles are used in cross-validation.   \n",
              "22  Use each reference and each expert annotation ...   \n",
              "23  In order to evaluate the model, we use the fol...   \n",
              "24  Stiennon et al.(2020) annotated a different su...   \n",
              "25  A reference-less COMET_QE outperforms any othe...   \n",
              "26                                 R. Bhandari et al.   \n",
              "27  Using the CNN/DailyMail dataset, the COMET_QE ...   \n",
              "28                                      Koloto et al.   \n",
              "29                                     Multi_SummEval   \n",
              "30  Multilinguality is a strength of the COMET met...   \n",
              "31  The Multi_SummEval dataset (Koto et al., 2021)...   \n",
              "32  One of the strengths of the COMET metric is it...   \n",
              "33  We propose the usage of crossvalidation in Sec...   \n",
              "34                                  SummEval dataset.   \n",
              "35                                   SummEval dataset   \n",
              "36  Using crossvalidation, we can train a model us...   \n",
              "37  Different articles are used for training, vali...   \n",
              "38                              Stanford CoreNLP tool   \n",
              "39                              Stanford CoreNLP tool   \n",
              "40  Tokenization and true-case system outputs with...   \n",
              "41             This paper employs a batch size of 40.   \n",
              "42  As a part of pre-processing, we de-tokenize an...   \n",
              "43                                    Bhandari et al.   \n",
              "44                                      CNN/DailyMail   \n",
              "45                A total of 6 metrics are evaluated.   \n",
              "46  On the CNN/DailyMail corpus, we evaluate on th...   \n",
              "\n",
              "                                             answer_2  \\\n",
              "0       Using COMET for Text Summarization Evaluation   \n",
              "1   We introduce a variant of the model – COMES – ...   \n",
              "2   In this paper, we introduce a variant of the m...   \n",
              "3            Evaluation of Text Summarization Systems   \n",
              "4   COMES2 is an automatic model that uses the ann...   \n",
              "5   COMET2 is a variant of the WMT model that uses...   \n",
              "6   COMET metric by Rei et al. (2020) is a metric ...   \n",
              "7                      based on semantic similarities   \n",
              "8   The COMET model is trained to extract represen...   \n",
              "9   based on semantic similarities between the tra...   \n",
              "10  The CNN/DailyMail corpus (Nallapati et al., 20...   \n",
              "11  It consists of 100 articles randomly sampled f...   \n",
              "12                                               2016   \n",
              "13  In SummEval4, each article is summarized by 17...   \n",
              "14  Authors collected 3 expert judgments for Coher...   \n",
              "15                                                 No   \n",
              "16  We experiment with both training from scratch ...   \n",
              "17  Content: The COMET metric trained on MT data o...   \n",
              "18  This modification changes the number of output...   \n",
              "19  During evaluation, we handle multiple referenc...   \n",
              "20           Use the data for training and evaluation   \n",
              "21  Use 80 articles per subset for training, 10 fo...   \n",
              "22  During training, we use each reference and eac...   \n",
              "23  During evaluation, we handle multiple referenc...   \n",
              "24  Stiennon et al.(2020) annotated a different su...   \n",
              "25  On this dataset, the COMET_QE variant outperfo...   \n",
              "26                          R. Bhandari et al. (2020)   \n",
              "27  Using the CNN/DailyMail dataset, the COMET_QE ...   \n",
              "28                                Koto and colleagues   \n",
              "29           Koto et al., 2021 Multi_SummEval dataset   \n",
              "30  It has seen over 30 language pairs during trai...   \n",
              "31  On the Multi_SummEval dataset, even the best p...   \n",
              "32  In the Multi_SummEval dataset, the COMET model...   \n",
              "33  We propose the usage of crossvalidation to ena...   \n",
              "34                           On the SummEval dataset.   \n",
              "35                            On the SummEval dataset   \n",
              "36  A model can be trained and tested on different...   \n",
              "37                                   SummEval dataset   \n",
              "38  we de-tokenize and true-case system outputs wi...   \n",
              "39                                         Rei et al.   \n",
              "40  Tokenization and true-case system outputs with...   \n",
              "41    The effective batch size of the training is 40.   \n",
              "42  As a part of pre-processing, we de-tokenize an...   \n",
              "43       Bhandari et al. (2020) CNN/DailyMail corpus.   \n",
              "44                          The CNN/DailyMail corpus.   \n",
              "45  A total of 6 metrics are evaluated. The three ...   \n",
              "46  The three metrics with the highest correlation...   \n",
              "\n",
              "                                             answer_3  \\\n",
              "0   Annotated summarization outputs are used for p...   \n",
              "1   We introduce a variant of the model – COMES – ...   \n",
              "2   In this paper, we introduce a variant of the m...   \n",
              "3   Use COMET to evaluate text summarization systems.   \n",
              "4   This paper presents a variant of the COMES2 mo...   \n",
              "5   COMET2 is a variant of the Metrics Shared Task...   \n",
              "6   A variant of the model COMES2 is capable of pr...   \n",
              "7   Semantic similarities between the translated a...   \n",
              "8   Pre-trained multilingual language models are u...   \n",
              "9   based on semantic similarities between the tra...   \n",
              "10      CNN/DailyMail corpus (Nallapati et al., 2016)   \n",
              "11  SummEval4 is a recently proposed dataset with ...   \n",
              "12                             Nallapati et al., 2016   \n",
              "13  A recently proposed dataset with human annotat...   \n",
              "14  Authors collected 3 expert judgments for Coher...   \n",
              "15            A modified version of the COMET method.   \n",
              "16  For the purpose of this paper, we propose a mo...   \n",
              "17  Content: The COMET metric trained on MT data o...   \n",
              "18  This modification changes the number of output...   \n",
              "19  During evaluation, we handle multiple referenc...   \n",
              "20      The data is used for training and evaluation.   \n",
              "21  80 articles are used in cross-validation. Duri...   \n",
              "22  During training, we use each reference and eac...   \n",
              "23  Scoring system output with both out-of-the-box...   \n",
              "24  COMET_QE outperforms any other variant, almost...   \n",
              "25  This paper presents a new approach to the eval...   \n",
              "26                                     Bhandari et al   \n",
              "27  Using the CNN/DailyMail dataset, the COMET_QE ...   \n",
              "28                                         Koto et al   \n",
              "29                             Multi_SummEval dataset   \n",
              "30  Multilinguality is a strength of the COMET met...   \n",
              "31  On the Multi_SummEval dataset, even the best p...   \n",
              "32  In the Multi_SummEval dataset, the COMET model...   \n",
              "33  We propose the usage of crossvalidation to ena...   \n",
              "34                                   SummEval dataset   \n",
              "35                           On the SummEval dataset.   \n",
              "36  Different articles are used for training, vali...   \n",
              "37  On the SummEval dataset, different articles ar...   \n",
              "38  De-tokenize and true-case system outputs with ...   \n",
              "39       The paper is an example of Rei et al. (2021)   \n",
              "40  gradient accumulation to train with the effect...   \n",
              "41       In this paper, we employ a batch size of 40.   \n",
              "42  We monitor Pearson correlation on the validati...   \n",
              "43                             Bhandari et al. (2020)   \n",
              "44                       On the CNN/DailyMail corpus.   \n",
              "45  We evaluate on the subset of the test split of...   \n",
              "46  We evaluate on the subset of the test split of...   \n",
              "\n",
              "                                             answer_4  question_ppl  \\\n",
              "0   The COMET model is a multilingual evaluation m...      0.001733   \n",
              "1   We introduce a variant of the model – COMES – ...      0.001738   \n",
              "2   In this paper, we introduce a variant of the m...      0.002084   \n",
              "3   Evaluation of Text Summarization Systems using...      0.005797   \n",
              "4   One of the issues making research on summary e...      0.000133   \n",
              "5   COMET2 is a variant of the Metrics Shared Task...      0.000531   \n",
              "6   COMET metric by Rei et al. (2020) is a metric ...      0.000755   \n",
              "7   semantic similarities between the translated a...      0.000230   \n",
              "8   Pre-trained multilingual language models are u...      0.000584   \n",
              "9   based on semantic similarities between the tra...      0.000611   \n",
              "10  The CNN/DailyMail corpus (Nallapati et al., 20...      0.000369   \n",
              "11  For each system output, the authors collected ...      0.000401   \n",
              "12                  The dataset was proposed in 2016.      0.001915   \n",
              "13  The authors collected expert judgments for Coh...      0.004925   \n",
              "14  For each summary system, the authors collected...      0.006409   \n",
              "15  A modified version of the COMET method is prop...      0.000568   \n",
              "16  For the purpose of this work, we propose a mod...      0.000666   \n",
              "17  COMET metric trained on MT data outputs a sing...      0.000924   \n",
              "18  The COMET metric trained on MT data outputs a ...      0.001986   \n",
              "19  During evaluation, we handle multiple referenc...      0.000342   \n",
              "20  The data is used both for training and evaluat...      0.000801   \n",
              "21  During training, we use each reference and eac...      0.002532   \n",
              "22  We split the data into 10 subsets of 10 articl...      0.005417   \n",
              "23  Scoring system output with both out-of-the-box...      0.007615   \n",
              "24  Stiennon et al.(2020) annotated a different su...      0.000035   \n",
              "25  Compared to COMET_QE, the reference-less varia...      0.000156   \n",
              "26                             Bhandari et al. (2020)      0.000440   \n",
              "27  Using the CNN/DailyMail dataset, the COMET_QE ...      0.000855   \n",
              "28                                  Koto et al., 2020      0.000055   \n",
              "29         Multi_SummEval dataset (Koto et al., 2021)      0.003008   \n",
              "30  On the Multi_SummEval dataset (Koto et al., 20...      0.004863   \n",
              "31  On the Multi_SummEval dataset, even the best p...      0.005573   \n",
              "32  With only two system outputs annotated, the si...      0.012023   \n",
              "33  We propose the usage of crossvalidation to ena...      0.000120   \n",
              "34         The SummEval dataset is used in this work.      0.000977   \n",
              "35          The work is done on the SummEval dataset.      0.002421   \n",
              "36  We propose the usage of crossvalidation to ena...      0.003005   \n",
              "37  SummEval dataset is used for training and test...      0.005263   \n",
              "38  de-tokenize and true-case system outputs with ...      0.000111   \n",
              "39      This paper is an example of Rei et al. (2021)      0.000195   \n",
              "40  Gradient accumulation to train with the effect...      0.000273   \n",
              "41  In this paper, we employ a batch size of 40. S...      0.000506   \n",
              "42  We employ gradient accumulation to train with ...      0.000996   \n",
              "43       Bhandari et al. (2020) CNN/DailyMail dataset      0.000711   \n",
              "44  The CNN/DailyMail corpus is used to evaluate o...      0.003838   \n",
              "45  We evaluate on the subset of the test split of...      0.004689   \n",
              "46  System-level Kendall’s Tau correlations on the...      0.013755   \n",
              "\n",
              "                                         selected_ans new_ans  \\\n",
              "0   the comet model is a multilingual evaluation m...           \n",
              "1   we introduce a variant of the model – comes – ...           \n",
              "2   in this paper, we introduce a variant of the m...           \n",
              "3            evaluation of text summarization systems           \n",
              "4   one of the issues making research on summary e...           \n",
              "5   comet2 is a variant of the wmt model that uses...           \n",
              "6   comet metric by rei et al. (2020) is a metric ...           \n",
              "7   semantic similarities between the translated a...           \n",
              "8   pre-trained multilingual language models are u...           \n",
              "9   based on semantic similarities between the tra...           \n",
              "10  cnn/dailymail corpus (nallapati et al., 2016) ...           \n",
              "11  summeval4 is a recently proposed dataset with ...           \n",
              "12                                               2016           \n",
              "13  in summeval4, each article is summarized by 17...           \n",
              "14  authors collected 3 expert judgments for coher...           \n",
              "15                                                yes           \n",
              "16  for the purpose of this work, we propose a mod...           \n",
              "17  comet metric trained on mt data outputs a sing...           \n",
              "18  this modification changes the number of output...           \n",
              "19  during evaluation, we handle multiple referenc...           \n",
              "20      the data is used for training and evaluation.           \n",
              "21  80 articles per subset for training, 10 for va...           \n",
              "22  during training, we use each reference and eac...           \n",
              "23  in order to evaluate the model, we use the fol...           \n",
              "24  stiennon et al.(2020) annotated a different su...           \n",
              "25  on this dataset, the comet_qe variant outperfo...           \n",
              "26                             bhandari et al. (2020)           \n",
              "27  using the cnn/dailymail dataset, the comet_qe ...           \n",
              "28                                        koto et al.           \n",
              "29         multi_summeval dataset (koto et al., 2021)           \n",
              "30  multilinguality is a strength of the comet met...           \n",
              "31  on the multi_summeval dataset, even the best p...           \n",
              "32  in the multi_summeval dataset, the comet model...           \n",
              "33  we propose the usage of crossvalidation to ena...           \n",
              "34                                  summeval dataset.           \n",
              "35                            on the summeval dataset           \n",
              "36  a model can be trained and tested on different...           \n",
              "37                                   summeval dataset           \n",
              "38  de-tokenize and true-case system outputs with ...           \n",
              "39                                         rei et al.           \n",
              "40  gradient accumulation to train with the effect...           \n",
              "41       in this paper, we employ a batch size of 40.           \n",
              "42  we employ gradient accumulation to train with ...           \n",
              "43       bhandari et al. (2020) cnn/dailymail corpus.           \n",
              "44                          the cnn/dailymail corpus.           \n",
              "45                a total of 6 metrics are evaluated.           \n",
              "46  the three metrics with the highest correlation...           \n",
              "\n",
              "                                         new_question  \n",
              "0                {'generated_text': 'What is COMET?'}  \n",
              "1                {'generated_text': 'What is COMET?'}  \n",
              "2                {'generated_text': 'What is COMET?'}  \n",
              "3                {'generated_text': 'What is COMET?'}  \n",
              "4     {'generated_text': 'What is the COMET metric?'}  \n",
              "5     {'generated_text': 'What is the COMET metric?'}  \n",
              "6     {'generated_text': 'What is the COMET metric?'}  \n",
              "7   {'generated_text': 'What does COMET learn to o...  \n",
              "8   {'generated_text': 'What does COMET learn to o...  \n",
              "9   {'generated_text': 'What does COMET learn to o...  \n",
              "10  {'generated_text': 'What is the name of the 3 ...  \n",
              "11  {'generated_text': 'What is the name of the 3 ...  \n",
              "12  {'generated_text': 'What is the name of the 3 ...  \n",
              "13  {'generated_text': 'What is the name of the 3 ...  \n",
              "14  {'generated_text': 'What is the name of the 3 ...  \n",
              "15  {'generated_text': 'What is the name of the tr...  \n",
              "16  {'generated_text': 'What is the name of the tr...  \n",
              "17  {'generated_text': 'What is the name of the tr...  \n",
              "18  {'generated_text': 'What is the name of the tr...  \n",
              "19  {'generated_text': 'What does the scoring syst...  \n",
              "20  {'generated_text': 'What does the scoring syst...  \n",
              "21  {'generated_text': 'What does the scoring syst...  \n",
              "22  {'generated_text': 'What does the scoring syst...  \n",
              "23  {'generated_text': 'What does the scoring syst...  \n",
              "24  {'generated_text': 'What is the best result of...  \n",
              "25  {'generated_text': 'What is the best result of...  \n",
              "26  {'generated_text': 'What is the best result of...  \n",
              "27  {'generated_text': 'What is the best result of...  \n",
              "28  {'generated_text': 'What is the strength of th...  \n",
              "29  {'generated_text': 'What is the strength of th...  \n",
              "30  {'generated_text': 'What is the strength of th...  \n",
              "31  {'generated_text': 'What is the strength of th...  \n",
              "32  {'generated_text': 'What is the strength of th...  \n",
              "33  {'generated_text': 'What is the purpose of cro...  \n",
              "34  {'generated_text': 'What is the purpose of cro...  \n",
              "35  {'generated_text': 'What is the purpose of cro...  \n",
              "36  {'generated_text': 'What is the purpose of cro...  \n",
              "37  {'generated_text': 'What is the purpose of cro...  \n",
              "38  {'generated_text': 'What is the name of the to...  \n",
              "39  {'generated_text': 'What is the name of the to...  \n",
              "40  {'generated_text': 'What is the name of the to...  \n",
              "41  {'generated_text': 'What is the name of the to...  \n",
              "42  {'generated_text': 'What is the name of the to...  \n",
              "43  {'generated_text': 'What is the name of the me...  \n",
              "44  {'generated_text': 'What is the name of the me...  \n",
              "45  {'generated_text': 'What is the name of the me...  \n",
              "46  {'generated_text': 'What is the name of the me...  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cckPav6fknB",
        "outputId": "69ac53ff-cf8e-4847-8c3a-77ff6bb4f499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Where does this paper come from?\n",
            "Using COMET for Text Summarization\n",
            "\n",
            "Using COMET to evaluate text summarization systems?\n",
            "we introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. we evaluate its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.\n",
            "\n",
            "Which machine translation metrics can be used to evaluate text summarization systems?\n",
            "in this paper, we introduce a variant of the model – COMET – trained on the annotated summarization outputs that uses MT data for pre-training. we evaluate its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.\n",
            "\n",
            "What is the purpose of this paper?\n",
            "use COMET to evaluate text summarization systems.\n",
            "\n",
            "Why are human judgments difficult to collect?\n",
            "one of the issues making research on summary evaluation metrics difficult is lack of a standardized framework for collecting human judgments.\n",
            "\n",
            "What is the purpose of the COMET2 model?\n",
            "COMET2 is a variant of the WMT model that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality.\n",
            "\n",
            "Which metrics are used to evaluate summary quality of WMT models?\n",
            "COMET metric by Rei et al. (2020) is a metric used to evaluate summary quality of WMT models.\n",
            "\n",
            "For which task is the summation evaluation resource used?\n",
            "the summation evaluation resource is used for the following tasks: question answering, similarity between summary and reference embeddings.\n",
            "\n",
            "Summaries of what kind of data are evaluated?\n",
            "summaries of system output are evaluated based on question answering, similarity between summary and reference embeddings.\n",
            "\n",
            "Why do we need to evaluate summaries?\n",
            "summaries can be used to evaluate the quality of a system. system output can be evaluated by evaluating the quality and reliability of summary output. so, the final answer is yes.\n",
            "\n",
            "Besides question answering, what other metrics were proposed for this task?\n",
            "the following metrics have been proposed for this task: Similarity between summary and reference embeddings:\n",
            "\n",
            "What are the most common metrics used for evaluating summary evaluation?\n",
            "the most common metrics used for evaluating summary evaluation are question answering, similarity between summary and reference embeddings.\n",
            "\n",
            "Comet uses what method of scoring?\n",
            "semantic similarities between the translated and reference texts\n",
            "\n",
            "Why do you need to train your COMET model?\n",
            "Pre-trained multilingual language models are used to extract representations for each of the input sequences, which are then pooled and concatenated before being processed with a stack of feed-forward layers that outputs a single numerical value.\n",
            "\n",
            "How does COMET compare to the human perception of translation quality?\n",
            "based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(questions_df)):\n",
        "  print(questions_df.question[i])\n",
        "  print(questions_df.selected_ans[i])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2957
        },
        "id": "_P3tIqb2gCd4",
        "outputId": "029767d6-ac67-42d5-fee1-1e34ec1f0083"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9ab65206-8702-4869-abe6-16e40b52b0f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_1</th>\n",
              "      <th>answer_2</th>\n",
              "      <th>answer_3</th>\n",
              "      <th>answer_4</th>\n",
              "      <th>question_ppl</th>\n",
              "      <th>selected_ans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>Where does this paper come from?</td>\n",
              "      <td>Using COMET for Text Summarization</td>\n",
              "      <td>Using COMET for Text Summarization Evaluation</td>\n",
              "      <td>Annotated summarization outputs are used for p...</td>\n",
              "      <td>The COMET model is a multilingual evaluation m...</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>Using COMET for Text Summarization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>Using COMET to evaluate text summarization sys...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>We introduce a variant of the model – COMES – ...</td>\n",
              "      <td>0.001738</td>\n",
              "      <td>we introduce a variant of the model – COMES – ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>Which machine translation metrics can be used ...</td>\n",
              "      <td>We introduce a variant of the COMET model – tr...</td>\n",
              "      <td>In this paper, we introduce a variant of the m...</td>\n",
              "      <td>In this paper, we introduce a variant of the m...</td>\n",
              "      <td>In this paper, we introduce a variant of the m...</td>\n",
              "      <td>0.002084</td>\n",
              "      <td>in this paper, we introduce a variant of the m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
              "      <td>What is the purpose of this paper?</td>\n",
              "      <td>An evaluation model for text summarization</td>\n",
              "      <td>Evaluation of Text Summarization Systems</td>\n",
              "      <td>Use COMET to evaluate text summarization systems.</td>\n",
              "      <td>Evaluation of Text Summarization Systems using...</td>\n",
              "      <td>0.005797</td>\n",
              "      <td>use COMET to evaluate text summarization systems.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Since manual annotation for any generative ta...</td>\n",
              "      <td>Why are human judgments difficult to collect?</td>\n",
              "      <td>This paper presents a variant of the COMES2 mo...</td>\n",
              "      <td>COMES2 is an automatic model that uses the ann...</td>\n",
              "      <td>This paper presents a variant of the COMES2 mo...</td>\n",
              "      <td>One of the issues making research on summary e...</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>one of the issues making research on summary e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Since manual annotation for any generative ta...</td>\n",
              "      <td>What is the purpose of the COMET2 model?</td>\n",
              "      <td>A variant of the WMT model – COMES2 – that use...</td>\n",
              "      <td>COMET2 is a variant of the WMT model that uses...</td>\n",
              "      <td>COMET2 is a variant of the Metrics Shared Task...</td>\n",
              "      <td>COMET2 is a variant of the Metrics Shared Task...</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>COMET2 is a variant of the WMT model that uses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Since manual annotation for any generative ta...</td>\n",
              "      <td>Which metrics are used to evaluate summary qua...</td>\n",
              "      <td>COMET metric by Rei et al. (2020)</td>\n",
              "      <td>COMET metric by Rei et al. (2020) is a metric ...</td>\n",
              "      <td>A variant of the model COMES2 is capable of pr...</td>\n",
              "      <td>COMET metric by Rei et al. (2020) is a metric ...</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>COMET metric by Rei et al. (2020) is a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>For a comprehensive survey on the summary eva...</td>\n",
              "      <td>For which task is the summation evaluation res...</td>\n",
              "      <td>The summation evaluation resource is used for ...</td>\n",
              "      <td>In this task, a variety of metrics were propos...</td>\n",
              "      <td>The summation evaluation resource is used for ...</td>\n",
              "      <td>In this task, a variety of metrics were propos...</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>the summation evaluation resource is used for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>For a comprehensive survey on the summary eva...</td>\n",
              "      <td>Summaries of what kind of data are evaluated?</td>\n",
              "      <td>System output (Papineni et al., 2002; Lin, 2004).</td>\n",
              "      <td>Summary evaluation is based on question answer...</td>\n",
              "      <td>In this task, summaries of system output are e...</td>\n",
              "      <td>Summaries of system output are evaluated based...</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>summaries of system output are evaluated based...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>For a comprehensive survey on the summary eva...</td>\n",
              "      <td>Why do we need to evaluate summaries?</td>\n",
              "      <td>Summaries can be used to evaluate the quality ...</td>\n",
              "      <td>Summaries can be used to evaluate the quality ...</td>\n",
              "      <td>Summaries can be used to evaluate the quality ...</td>\n",
              "      <td>Summaries can be used to evaluate the quality ...</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>summaries can be used to evaluate the quality ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>For a comprehensive survey on the summary eva...</td>\n",
              "      <td>Besides question answering, what other metrics...</td>\n",
              "      <td>The following metrics were proposed for this t...</td>\n",
              "      <td>The following metrics have been proposed for t...</td>\n",
              "      <td>The following metrics were proposed for this t...</td>\n",
              "      <td>Similarity between summary and reference embed...</td>\n",
              "      <td>0.006097</td>\n",
              "      <td>the following metrics have been proposed for t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>For a comprehensive survey on the summary eva...</td>\n",
              "      <td>What are the most common metrics used for eval...</td>\n",
              "      <td>Koto et al., 2002; Lin, 2004. Over the years, ...</td>\n",
              "      <td>The most common metrics used for evaluating su...</td>\n",
              "      <td>The most common metrics used for evaluating su...</td>\n",
              "      <td>The most common metrics used for evaluating su...</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>the most common metrics used for evaluating su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>COMET is a trained metric that, based on sema...</td>\n",
              "      <td>Comet uses what method of scoring?</td>\n",
              "      <td>the human perception of translation quality</td>\n",
              "      <td>based on semantic similarities</td>\n",
              "      <td>Semantic similarities between the translated a...</td>\n",
              "      <td>semantic similarities between the translated a...</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>semantic similarities between the translated a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>COMET is a trained metric that, based on sema...</td>\n",
              "      <td>Why do you need to train your COMET model?</td>\n",
              "      <td>The COMET model is trained to output a score t...</td>\n",
              "      <td>The COMET model is trained to extract represen...</td>\n",
              "      <td>Pre-trained multilingual language models are u...</td>\n",
              "      <td>Pre-trained multilingual language models are u...</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>Pre-trained multilingual language models are u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>COMET is a trained metric that, based on sema...</td>\n",
              "      <td>How does COMET compare to the human perception...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>based on semantic similarities between the tra...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ab65206-8702-4869-abe6-16e40b52b0f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ab65206-8702-4869-abe6-16e40b52b0f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ab65206-8702-4869-abe6-16e40b52b0f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              context  \\\n",
              "0   COMET is a recently proposed trainable neuralb...   \n",
              "1   COMET is a recently proposed trainable neuralb...   \n",
              "2   COMET is a recently proposed trainable neuralb...   \n",
              "3   COMET is a recently proposed trainable neuralb...   \n",
              "4    Since manual annotation for any generative ta...   \n",
              "5    Since manual annotation for any generative ta...   \n",
              "6    Since manual annotation for any generative ta...   \n",
              "7    For a comprehensive survey on the summary eva...   \n",
              "8    For a comprehensive survey on the summary eva...   \n",
              "9    For a comprehensive survey on the summary eva...   \n",
              "10   For a comprehensive survey on the summary eva...   \n",
              "11   For a comprehensive survey on the summary eva...   \n",
              "12   COMET is a trained metric that, based on sema...   \n",
              "13   COMET is a trained metric that, based on sema...   \n",
              "14   COMET is a trained metric that, based on sema...   \n",
              "\n",
              "                                             question  \\\n",
              "0                    Where does this paper come from?   \n",
              "1   Using COMET to evaluate text summarization sys...   \n",
              "2   Which machine translation metrics can be used ...   \n",
              "3                  What is the purpose of this paper?   \n",
              "4       Why are human judgments difficult to collect?   \n",
              "5            What is the purpose of the COMET2 model?   \n",
              "6   Which metrics are used to evaluate summary qua...   \n",
              "7   For which task is the summation evaluation res...   \n",
              "8       Summaries of what kind of data are evaluated?   \n",
              "9               Why do we need to evaluate summaries?   \n",
              "10  Besides question answering, what other metrics...   \n",
              "11  What are the most common metrics used for eval...   \n",
              "12                 Comet uses what method of scoring?   \n",
              "13         Why do you need to train your COMET model?   \n",
              "14  How does COMET compare to the human perception...   \n",
              "\n",
              "                                             answer_1  \\\n",
              "0                  Using COMET for Text Summarization   \n",
              "1   We introduce a variant of the model – COMES – ...   \n",
              "2   We introduce a variant of the COMET model – tr...   \n",
              "3          An evaluation model for text summarization   \n",
              "4   This paper presents a variant of the COMES2 mo...   \n",
              "5   A variant of the WMT model – COMES2 – that use...   \n",
              "6                   COMET metric by Rei et al. (2020)   \n",
              "7   The summation evaluation resource is used for ...   \n",
              "8   System output (Papineni et al., 2002; Lin, 2004).   \n",
              "9   Summaries can be used to evaluate the quality ...   \n",
              "10  The following metrics were proposed for this t...   \n",
              "11  Koto et al., 2002; Lin, 2004. Over the years, ...   \n",
              "12        the human perception of translation quality   \n",
              "13  The COMET model is trained to output a score t...   \n",
              "14  based on semantic similarities between the tra...   \n",
              "\n",
              "                                             answer_2  \\\n",
              "0       Using COMET for Text Summarization Evaluation   \n",
              "1   We introduce a variant of the model – COMES – ...   \n",
              "2   In this paper, we introduce a variant of the m...   \n",
              "3            Evaluation of Text Summarization Systems   \n",
              "4   COMES2 is an automatic model that uses the ann...   \n",
              "5   COMET2 is a variant of the WMT model that uses...   \n",
              "6   COMET metric by Rei et al. (2020) is a metric ...   \n",
              "7   In this task, a variety of metrics were propos...   \n",
              "8   Summary evaluation is based on question answer...   \n",
              "9   Summaries can be used to evaluate the quality ...   \n",
              "10  The following metrics have been proposed for t...   \n",
              "11  The most common metrics used for evaluating su...   \n",
              "12                     based on semantic similarities   \n",
              "13  The COMET model is trained to extract represen...   \n",
              "14  based on semantic similarities between the tra...   \n",
              "\n",
              "                                             answer_3  \\\n",
              "0   Annotated summarization outputs are used for p...   \n",
              "1   We introduce a variant of the model – COMES – ...   \n",
              "2   In this paper, we introduce a variant of the m...   \n",
              "3   Use COMET to evaluate text summarization systems.   \n",
              "4   This paper presents a variant of the COMES2 mo...   \n",
              "5   COMET2 is a variant of the Metrics Shared Task...   \n",
              "6   A variant of the model COMES2 is capable of pr...   \n",
              "7   The summation evaluation resource is used for ...   \n",
              "8   In this task, summaries of system output are e...   \n",
              "9   Summaries can be used to evaluate the quality ...   \n",
              "10  The following metrics were proposed for this t...   \n",
              "11  The most common metrics used for evaluating su...   \n",
              "12  Semantic similarities between the translated a...   \n",
              "13  Pre-trained multilingual language models are u...   \n",
              "14  based on semantic similarities between the tra...   \n",
              "\n",
              "                                             answer_4  question_ppl  \\\n",
              "0   The COMET model is a multilingual evaluation m...      0.001733   \n",
              "1   We introduce a variant of the model – COMES – ...      0.001738   \n",
              "2   In this paper, we introduce a variant of the m...      0.002084   \n",
              "3   Evaluation of Text Summarization Systems using...      0.005797   \n",
              "4   One of the issues making research on summary e...      0.000133   \n",
              "5   COMET2 is a variant of the Metrics Shared Task...      0.000531   \n",
              "6   COMET metric by Rei et al. (2020) is a metric ...      0.000755   \n",
              "7   In this task, a variety of metrics were propos...      0.000309   \n",
              "8   Summaries of system output are evaluated based...      0.000315   \n",
              "9   Summaries can be used to evaluate the quality ...      0.001162   \n",
              "10  Similarity between summary and reference embed...      0.006097   \n",
              "11  The most common metrics used for evaluating su...      0.008529   \n",
              "12  semantic similarities between the translated a...      0.000230   \n",
              "13  Pre-trained multilingual language models are u...      0.000584   \n",
              "14  based on semantic similarities between the tra...      0.000611   \n",
              "\n",
              "                                         selected_ans  \n",
              "0                  Using COMET for Text Summarization  \n",
              "1   we introduce a variant of the model – COMES – ...  \n",
              "2   in this paper, we introduce a variant of the m...  \n",
              "3   use COMET to evaluate text summarization systems.  \n",
              "4   one of the issues making research on summary e...  \n",
              "5   COMET2 is a variant of the WMT model that uses...  \n",
              "6              COMET metric by Rei et al. (2020) is a  \n",
              "7   the summation evaluation resource is used for ...  \n",
              "8   summaries of system output are evaluated based...  \n",
              "9   summaries can be used to evaluate the quality ...  \n",
              "10  the following metrics have been proposed for t...  \n",
              "11  the most common metrics used for evaluating su...  \n",
              "12  semantic similarities between the translated a...  \n",
              "13  Pre-trained multilingual language models are u...  \n",
              "14  based on semantic similarities between the tra...  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "ZC3GdMbpkVwP",
        "outputId": "382e1258-a891-4d7f-eda8-0bbbc38437bc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b99c46a0e716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mans_input_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ans_input_string' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "for cur in papers_list:\n",
        "  print(cur)\n",
        "  cur = replace_abbreviations(abrv_dict,cur,only_first=True)\n",
        "  input_string = \"generate question: \" + cur\n",
        "  #input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "  output = get_output_from_prompt(model,input_string,generator_args)\n",
        "  print(f'Total text shape is {tokenizer.encode(cur, return_tensors=\"pt\",truncation=False).shape}')\n",
        "  for i in output:\n",
        "    print(f'Qs:{i}')\n",
        "    #input_string = i[0]+\" /n \" + cur\n",
        "    #ans_output = get_output_from_prompt(QA_model,input_string,answers_generator_args)\n",
        "  similarity_matrix = find_similarity(output)\n",
        "  plot_similarity_matrix(output)\n",
        "  output_filtered = filter_questions(output, similarity_matrix,similarity_thrs=0.7,n_thrs=7)\n",
        "  for i in output_filtered:\n",
        "    print(f'Qs:{replace_abbreviations(abrv_dict,i,only_first=True)}')\n",
        "    ans_input_string = \"answer to the question, step by step: \"+i+\" </s> context: \" + cur\n",
        "    ans_output = get_output_from_prompt(model,ans_input_string,answers_generator_args)\n",
        "    for ans in ans_output:\n",
        "      print('----Ans:--',replace_abbreviations(abrv_dict,ans,only_first=True))\n",
        "    '''dist_input_string = \"generate distractor to the question, step by step: \"+i+\" </s> context: \" + cur\n",
        "    dist_output = get_output_from_prompt(model,dist_input_string,answers_generator_args)\n",
        "    for dist in dist_output:\n",
        "      print('----Distractor:--',dist)'''\n",
        "  similarity_matrix = find_similarity(output_filtered)\n",
        "  plot_similarity_matrix(output_filtered)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG52zyvnfGCu",
        "outputId": "b3a28be8-8adb-44c6-c8c5-e73242776116"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  warnings.warn(\n",
            "<ipython-input-46-a2ac9b874259>:31: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  ppl = [np.exp(np.array(log_likelihoods).mean() / np.array(len(cur_output.split())).mean()) for log_likelihoods,cur_output in zip(res['sequences_scores'],output)]\n"
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "    \"max_new_tokens\":20,\n",
        "#\"max_length\": 256,\n",
        "\"num_beams\": 36,\n",
        "\"length_penalty\":-0.5,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.95,\n",
        "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
        "'num_beam_groups':18, \n",
        "\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':21\n",
        "}\n",
        "import torch\n",
        "sigmoid_func = torch.nn.Sigmoid()\n",
        "def get_output_from_prompt(model,prompt,args):\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "  \n",
        "  res = model.generate(input_ids, **args)\n",
        "  output = tokenizer.batch_decode(res['sequences'], skip_special_tokens=True)\n",
        "  output = [item.split(\"<sep>\") for item in output]\n",
        "  if all([len(cur_sen)==1 for cur_sen in output]):  \n",
        "      output = [cur_sen[0] for cur_sen in output]\n",
        "  #p = sigmoid_func(res['sequences_scores'])\n",
        "  #print(p)\n",
        "  #ppl = [Pi**-(1/len(cur_output.split())) for Pi,cur_output in zip(p,output)]\n",
        "  #np.exp(np.array(log_likelihoods).mean() / np.array(seq_lens).mean())\n",
        "  ppl = [np.exp(np.array(log_likelihoods).mean() / np.array(len(cur_output.split())).mean()) for log_likelihoods,cur_output in zip(res['sequences_scores'],output)]\n",
        "  sorted_idx  = np.argsort(ppl)[::-1]\n",
        "  return [output[id] for id in sorted_idx],\\\n",
        "          [ppl[id] for id in sorted_idx],\\\n",
        "          [res['sequences_scores'][id] for id in sorted_idx]\n",
        "\n",
        "input_string = \"generate question: \"+roelfsema_paper_conclusion\n",
        "texts,ppl,scores  = get_output_from_prompt(model,input_string,generator_args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDEHV2tTUv3N",
        "outputId": "12b48091-2568-4e84-fad3-65d4bb5293bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Learning in the brain is a genuine understanding of learning in machines and the brain',\n",
              "  'Learning in the brain is a genuine understanding of learning in machines and the brain.',\n",
              "  'Learning in the brain is a genuine understanding of learning in machines',\n",
              "  'Intelligent learning is starting to bridge the gap between neural and motor neurons',\n",
              "  'It is a genuine understanding of the brain and its processing stages.',\n",
              "  'Intelligent machine learning',\n",
              "  'An understanding of biologically plausible training',\n",
              "  'Does the present work on biologically plausible training begin?',\n",
              "  'Human neural networks',\n",
              "  'Human neural networks have been developed for this purpose',\n",
              "  'No longer are we learning in machines and neurons?',\n",
              "  'Human neural networks can be used for neural networks',\n",
              "  'No longer are we learning in machines and neurons.',\n",
              "  'Neurons are involved with learning in machines',\n",
              "  'No longer are we learning?',\n",
              "  'Scepticism in machine learning',\n",
              "  'New research on bio-imaging',\n",
              "  'New research on bio-imagings',\n",
              "  'Scepticism',\n",
              "  '',\n",
              "  ''],\n",
              " [0.0070327159403667005,\n",
              "  0.006726712070660552,\n",
              "  0.0036728079231538252,\n",
              "  0.002767608520828352,\n",
              "  0.0014406607712875402,\n",
              "  0.0009859644499894493,\n",
              "  0.0002758208989920734,\n",
              "  0.00017505652341704868,\n",
              "  0.00016917697083408334,\n",
              "  0.00011275263277165744,\n",
              "  0.00010753797786264648,\n",
              "  0.00010496363515363486,\n",
              "  8.728246727365972e-05,\n",
              "  3.229514623172829e-05,\n",
              "  2.064933792064652e-05,\n",
              "  1.6818380748077015e-05,\n",
              "  3.9780354918886107e-07,\n",
              "  2.1436051540900494e-08,\n",
              "  3.5595304848483565e-11,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [tensor(-74.3577),\n",
              "  tensor(-75.0250),\n",
              "  tensor(-67.2816),\n",
              "  tensor(-76.5670),\n",
              "  tensor(-78.5118),\n",
              "  tensor(-20.7657),\n",
              "  tensor(-49.1746),\n",
              "  tensor(-77.8536),\n",
              "  tensor(-26.0537),\n",
              "  tensor(-81.8128),\n",
              "  tensor(-82.2390),\n",
              "  tensor(-82.4571),\n",
              "  tensor(-84.1172),\n",
              "  tensor(-72.3842),\n",
              "  tensor(-53.9391),\n",
              "  tensor(-43.9722),\n",
              "  tensor(-58.9492),\n",
              "  tensor(-70.6328),\n",
              "  tensor(-24.0588),\n",
              "  tensor(-6.5034),\n",
              "  tensor(-6.5034)])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts,ppl,scores "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7SkihMWYsYU",
        "outputId": "91f79f80-4796-4dc6-e6af-3c406c1eb6c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([20, 19, 17, 10, 12, 15, 13,  9,  0,  1,  2,  3, 16,  5, 14, 18,  4,\n",
              "        6, 11,  7,  8])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu9oAkG7gs_R",
        "outputId": "839f5867-d9bb-4947-cf80-ca843821562d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['We conclude that the present',\n",
              " 'Insights from',\n",
              " 'The present and related work',\n",
              " 'Learning in the brain is',\n",
              " 'Our findings suggest that the',\n",
              " 'Machine Learning',\n",
              " 'This study demonstrates that',\n",
              " 'Recommendations',\n",
              " 'Towards an understanding',\n",
              " 'A new study demonstrates',\n",
              " 'Intelligent learning is starting to',\n",
              " 'It is possible to conclude']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9A-29IKgZQ1",
        "outputId": "f122472d-6ed4-4de2-acde-953f0ad342bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 3806,   822,    10,   242,     3,     9,  3452,  3719,    30,     8,\n",
              "          9251,  5002,  1438,   217,  1793,   235,     3,    15,    17,   491,\n",
              "             5,    11,   358,  3911,    41, 15182,    77,    35,    23,     3,\n",
              "            15,    17,   491,     5,     6,  4407,   117,  6741,     6,  4406,\n",
              "           137,  2035,     8,   203,     6,     3,     9,  1196,    13, 15905,\n",
              "           130,  4382,    21,    48,  2491,     3,   104,     3,   390,    30,\n",
              "           822, 18243,     6,  1126,   485,   344,  9251,    11,  2848, 25078,\n",
              "            26,    53,     7,     5,     1]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GqEq_tMyfa5S",
        "outputId": "da58675c-645b-4ca2-a97a-49186705f89e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are the most common metrics used for evaluating summary evaluation? - System output'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "fhVYN2WNqM-W",
        "outputId": "56018a70-5da2-414f-bbad-a0f47f9eac6a"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-650319e39ed6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (output, similarity_matrix,similarity_thrs=0.5,n_thrs=7)\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "(output, similarity_matrix,similarity_thrs=0.5,n_thrs=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbAZAuSKp0Id",
        "outputId": "6212da9b-def7-4a6e-e329-8cb881c4bb90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['What is the new learning scheme called?']\n",
            "['Why do we need reinforcement learning?']\n",
            "['Which learning scheme is equivalent to error backpropagation?']\n",
            "['A reinforcement learning scheme for deep networks with an arbitrary number of layers.']\n",
            "['BrainProp: A Reinforcement Learning Scheme']\n",
            "['Where can reinforcement learning be applied to deep networks?']\n",
            "['Describe an approach to reinforcement learning for deep networks.']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filter_questions(output, similarity_matrix,similarity_thrs=0.5,n_thrs=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "goJscQZG7R0j",
        "outputId": "82400e41-4a84-47ac-bc36-30541df1cbc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4593a6851340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pdb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_similarity_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3412dd1b9e70>\u001b[0m in \u001b[0;36mplot_similarity_matrix\u001b[0;34m(questions_list)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestions_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'visible'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'showticklabels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'visible'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'showticklabels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/plotly/express/_imshow.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(img, zmin, zmax, origin, labels, x, y, animation_frame, facet_col, facet_col_wrap, facet_col_spacing, facet_row_spacing, color_continuous_scale, color_continuous_midpoint, range_color, title, template, width, height, aspect, contrast_rescaling, binary_string, binary_backend, binary_compression_level, binary_format, text_auto)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xaxis\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautorange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reversed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;34m\"px.imshow only accepts 2D single-channel, RGB or RGBA images. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"An image of shape %s was provided. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: px.imshow only accepts 2D single-channel, RGB or RGBA images. An image of shape () was provided. Alternatively, 3- or 4-D single or multichannel datasets can be visualized using the `facet_col` or/and `animation_frame` arguments."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.8/dist-packages/plotly/express/_imshow.py\u001b[0m(530)\u001b[0;36mimshow\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    528 \u001b[0;31m            \u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xaxis\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautorange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reversed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    529 \u001b[0;31m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 530 \u001b[0;31m        raise ValueError(\n",
            "\u001b[0m\u001b[0;32m    531 \u001b[0;31m            \u001b[0;34m\"px.imshow only accepts 2D single-channel, RGB or RGBA images. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    532 \u001b[0;31m            \u001b[0;34m\"An image of shape %s was provided. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m<ipython-input-5-3412dd1b9e70>\u001b[0m(65)\u001b[0;36mplot_similarity_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     63 \u001b[0;31m    \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     64 \u001b[0;31m  \u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestions_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 65 \u001b[0;31m  \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     66 \u001b[0;31m  \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'visible'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'showticklabels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     67 \u001b[0;31m  \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'visible'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'showticklabels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> similarity_matrix\n",
            "0.0\n",
            "ipdb> questions_list\n",
            "[]\n",
            "ipdb> q\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.8/bdb.py\", line 359, in set_quit\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pdb on\n",
        "plot_similarity_matrix(output_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP5AWH0BLFjQ",
        "outputId": "b0f5a357-2dda-469e-8ca7-a8a71a0032fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['How can we predict crowdfunding success?Options:A By analyzing the semantic features of project posts.B By using a text analytics framework.C By focusing on both project-specific aspects and semantics of project descriptions.D By examining the features correlated to funding success.Answer:C']\n",
            "['How can we predict crowdfunding success?Options:A By analyzing the semantic features of project posts.B By using a text analytics framework.C By focusing on both project-specific aspects and semantics of project descriptions.D By examining the features correlated to funding success.Answer:A']\n",
            "['According to this paper, which of the following features is highly correlated with crowdfunding success?Options:A Buzzwords.B Feelings.C Project category.D Project description.Answer:B']\n",
            "['According to this paper, which of the following features is highly correlated with crowdfunding success?Options:A Buzzwords.B Feelings.C Project category.D Project description.Answer:A']\n",
            "['According to this paper, which of the following features is highly correlated with crowdfunding success?Options:A Buzzwords.B Feelings words.C Project category.D Images.Answer:A']\n",
            "['According to this paper, which of the following features is highly correlated with crowdfunding success?Options:A Buzzwords.B Feelings words.C Project category.D Images.Answer:A.']\n",
            "['If you want to increase your crowdfunding success chances, what should you do?Options:A Update your project information.B Use buzzwords in your project description.C Make sure that your project has more feelings words.D Make sure your project’s post contains more feelings word.Answer:C']\n",
            "['If you want to increase your crowdfunding success chances, what should you do?Options:A Update your project information.B Use buzzwords in your project description.C Make sure that your project has more feelings words.D Make sure your project’s post contains more feelings word.Answer:A']\n",
            "['If you want to increase your crowdfunding success chances, what should you do?Options:A Update your project information.B Use buzzwords in your project description.C Make sure that your project has more feelings words.D Make sure your project’s description contains more buzzword.Answer:C']\n",
            "['According to this paper, which of the following features is highly correlated with crowdfunding success?Options:A Buzzwords.B Feelings.C Project category.D Project description.Answer:An']\n"
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 21,\n",
        "#\"length_penalty\":-3.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.82,\n",
        "'diversity_penalty':1.5,\n",
        "'num_beam_groups':7,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "#'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':10\n",
        "}\n",
        "\n",
        "#input_string = \"ask question and answer: \" + results + \" </s>\"\n",
        "#input_string = \"generate multiple choice question: \" + results + \" </s>\"\n",
        "input_string = \"generate multiple choice question in the template of question followed by its answer and 3 wrong distractors, from the following text: \" + results + \" </s>\"\n",
        "#input_string = \"generate questions: \" + results\n",
        "#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "res = model.generate(input_ids, **generator_args)\n",
        "output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "output = [item.split(\"<sep>\") for item in output]\n",
        "for i in output:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goidDTAQoSzP",
        "outputId": "62926ddb-25c2-481e-e8b6-888fbd4dcd5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qs:['How is mindfulness defined?']\n",
            "----Ans:-- ['The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner.']\n",
            "----Ans:-- ['Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner.']\n",
            "----Ans:-- ['Most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner.']\n",
            "Qs:['Which two interventions comprise eight weekly sessions averaging 2.5 hours?']\n",
            "----Ans:-- ['MBIS and CBT']\n",
            "----Ans:-- ['CBT and mindfulness-based stress reduction (MBSR)']\n",
            "----Ans:-- ['MBIS and CBT.']\n",
            "Qs:['What are the two standard interventions that comprise eight weekly sessions averaging 2.5 hours?']\n",
            "----Ans:-- [\"The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment.\"]\n",
            "----Ans:-- [\"The study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment.\"]\n",
            "----Ans:-- ['Mindfulness-based stress reduction (MBSR) and mindfulness-based cognitive therapy (MBCT) are two standard interventions that comprise eight weekly sessions averaging 2.5 hours.']\n",
            "Qs:['Who was the author of this article?']\n",
            "----Ans:-- [\"author's name\"]\n",
            "----Ans:-- ['Author(s): Garroway']\n",
            "----Ans:-- ['Judith C. Rybarczyk']\n",
            "Qs:['Which two types of interventions do they compare?']\n",
            "----Ans:-- ['MBIS and CBT']\n",
            "----Ans:-- ['CBT and mindfulness-based interventions']\n",
            "----Ans:-- ['CBT and mindfulness-based interventions for seniors (MBIS)']\n",
            "Qs:['What are two types of interventions discussed in this article?']\n",
            "----Ans:-- ['Mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention']\n",
            "----Ans:-- ['Cognitive behavioral therapy (CBT) and mindfulness-based intervention for seniors (MBIS)']\n",
            "----Ans:-- ['Mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention.']\n",
            "Qs:['Why do older adults have more anxiety and depression than younger adults?']\n",
            "----Ans:-- [\"The study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment.\"]\n",
            "----Ans:-- [\"The increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critical to improve the elderly population's quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them.\"]\n",
            "----Ans:-- [\"The elderly population is continuously challenged with maintaining and advancing its well-being. The increased vulnerability to emotional distress in old age highlights the critical need for effective short-term interventions that can quickly improve seniors' health and quality of life.\"]\n"
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 10,\n",
        "\"length_penalty\":-0.5,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.97,\n",
        "'diversity_penalty':1.8, #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
        "'num_beam_groups':10, \n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':7\n",
        "}\n",
        "answers_generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 10,\n",
        "\"length_penalty\":0.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.96,\n",
        "'diversity_penalty':0.9,\n",
        "'num_beam_groups':10,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':3\n",
        "}\n",
        "def get_output_from_prompt(prompt,args):\n",
        "  input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "  res = model.generate(input_ids, **args)\n",
        "  output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "  output = [item.split(\"<sep>\") for item in output]\n",
        "  return output\n",
        "#\n",
        "#input_string = \"ask question and answer: \" + results + \" </s>\"\n",
        "#input_string = \"generate questions: \" + results\n",
        "input_string = \"generate questions and answer: \" + mindfulness_paper\n",
        "#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "output = get_output_from_prompt(input_string,generator_args)\n",
        "for i in output:\n",
        "  print(f'Qs:{i}')\n",
        "  input_string = \"answer to the question, step by step: \"+i[0]+\" </s> context: \" + mindfulness_paper\n",
        "  ans_output = get_output_from_prompt(input_string,answers_generator_args)\n",
        "  for ans in ans_output:\n",
        "    print('----Ans:--',ans)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HGnQPjOCyYS",
        "outputId": "707743d6-37b2-407f-a737-af7c663d345b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qs:['How is mindfulness defined?']\n",
            "----Ans:-- ['The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner.']\n",
            "----Ans:-- ['Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner.']\n",
            "----Ans:-- ['Most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner.']\n",
            "Qs:['Which two interventions comprise eight weekly sessions averaging 2.5 hours?']\n",
            "----Ans:-- ['MBIS and CBT']\n",
            "----Ans:-- ['CBT and mindfulness-based stress reduction (MBSR)']\n",
            "----Ans:-- ['MBIS and CBT.']\n",
            "Qs:['What are the two standard interventions that comprise eight weekly sessions averaging 2.5 hours?']\n",
            "----Ans:-- [\"The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment.\"]\n",
            "----Ans:-- [\"The study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment.\"]\n",
            "----Ans:-- ['Mindfulness-based stress reduction (MBSR) and mindfulness-based cognitive therapy (MBCT) are two standard interventions that comprise eight weekly sessions averaging 2.5 hours.']\n",
            "Qs:['Who was the author of this article?']\n",
            "----Ans:-- [\"author's name\"]\n",
            "----Ans:-- ['Author(s): Garroway']\n",
            "----Ans:-- ['Judith C. Rybarczyk']\n",
            "Qs:['Which two types of interventions do they compare?']\n",
            "----Ans:-- ['MBIS and CBT']\n",
            "----Ans:-- ['CBT and mindfulness-based interventions']\n",
            "----Ans:-- ['CBT and mindfulness-based interventions for seniors (MBIS)']\n",
            "Qs:['What are two types of interventions discussed in this article?']\n",
            "----Ans:-- ['Mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention']\n",
            "----Ans:-- ['Cognitive behavioral therapy (CBT) and mindfulness-based intervention for seniors (MBIS)']\n",
            "----Ans:-- ['Mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention.']\n",
            "Qs:['Why do older adults have more anxiety and depression than younger adults?']\n",
            "----Ans:-- [\"The study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors' attitudes toward seeking mental health treatment.\"]\n",
            "----Ans:-- [\"The increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critical to improve the elderly population's quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them.\"]\n",
            "----Ans:-- [\"The elderly population is continuously challenged with maintaining and advancing its well-being. The increased vulnerability to emotional distress in old age highlights the critical need for effective short-term interventions that can quickly improve seniors' health and quality of life.\"]\n"
          ]
        }
      ],
      "source": [
        "answers_generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 10,\n",
        "\"length_penalty\":0.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.6,\n",
        "'diversity_penalty':0.9,\n",
        "'num_beam_groups':10,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':3\n",
        "}\n",
        "\n",
        "for i in output:\n",
        "  print(f'Qs:{i}')\n",
        "  input_string = \"answer to the question, step by step: \"+i[0]+\" </s> context: \" + mindfulness_paper\n",
        "  ans_output = get_output_from_prompt(input_string,answers_generator_args)\n",
        "  for ans in ans_output:\n",
        "    print('----Ans:--',ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtJuT_0Y6NAa",
        "outputId": "98956ad2-7e04-4b12-93d7-403ea9303006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRUE OR FLASE Qs:['True']\n",
            "TRUE OR FLASE Qs:['False']\n",
            "TRUE OR FLASE Qs:['True']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "input_string = \"generate true or false question from the following text: \" + mindfulness_paper\n",
        "output = get_output_from_prompt(input_string,generator_args)\n",
        "for i in output:\n",
        "  print(f'TRUE OR FLASE Qs:{i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "jXYHoMLQpHiC",
        "outputId": "e224fe77-ec09-4d71-ddb8-edbf44a2fd07"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7dfd9e30d35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mqs_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0manswers_generator_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mans_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mans_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<sep>\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`num_beam_groups` has to be smaller or equal to `num_beams`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_group_beam_gen_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdo_sample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1438\u001b[0m                 \u001b[0;34m\"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`."
          ]
        }
      ],
      "source": [
        "answers_generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 40,\n",
        "'do_sample':True,\n",
        "\"length_penalty\":0.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[tokenizer(\"False\")['input_ids']]],#token of `?`\n",
        "'top_p' :0.5,\n",
        "'diversity_penalty':0.99,\n",
        "'num_beam_groups':5,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':15\n",
        "}\n",
        "input_string = \"False answer to 'What datasets did they use?' </s> context: \" + results+ '</s>'\n",
        "#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "qs_res = model.generate(input_ids, **answers_generator_args)\n",
        "ans_output = tokenizer.batch_decode(qs_res, skip_special_tokens=True)\n",
        "ans_output = [item.split(\"<sep>\") for item in ans_output]\n",
        "for ans in ans_output:\n",
        "  print('----Ans:--',ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8PYt_LGzpMM5",
        "outputId": "73e6ff07-d2f8-4176-f06a-4b7304585b7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qs:['What is the purpose of this paper?']\n",
            "----Ans:-- ['To investigate the relationship between crowdfunding success and buzzwords.']\n",
            "----Ans:-- ['We developed a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['This paper investigates the relationship between crowdfunding success and buzzwords.']\n",
            "Qs:['What is the purpose of this study?']\n",
            "----Ans:-- ['We developed a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['To improve the accuracy of crowdfunding success predictions.']\n",
            "----Ans:-- ['Identifying and Predicting Crowdfunding Success']\n",
            "Qs:['Which features are highly correlated with crowdfunding success?']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['The purpose of this paper is to:']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies.']\n",
            "Qs:['Which features are highly correlated to funding success?']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['In recent years, crowdfunding platforms such as Kickstarter and Indiegogo have been offering entrepreneurs the possibility to present their projects and attract funders, and thus raise the funds necessary for their projects.']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies.']\n",
            "Qs:['Which features are highly correlated with funding success?']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['In recent years, crowdfunding platforms such as Kickstarter and Indiegogo have been offering entrepreneurs the possibility to present their projects and attract funders, and thus raise the funds necessary for their projects.']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies.']\n",
            "Qs:['How do they predict crowdfunding success?']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['We developed a model based on semantic features only and achieved similar accuracy level as previous studies.']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success']\n",
            "Qs:['The purpose of this paper is to:']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for predicting crowdfunding success.']\n",
            "Qs:['Which features are highly correlated to funding success compared to other parameters?']\n",
            "----Ans:-- ['A text analytics framework and a prediction model for analyzing and predicting crowdfunding success.']\n",
            "----Ans:-- ['To the best of our knowledge, this study is the first that investigates the relationship between funding success and buzzwords.']\n",
            "----Ans:-- ['The purpose of this paper is to:']\n",
            "Qs:['Which features are highly correlated with funding success compared to other parameters?']\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-52c22620a4ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;31m#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0mqs_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0manswers_generator_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mans_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0mans_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<sep>\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mans_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1662\u001b[0m             )\n\u001b[1;32m   1663\u001b[0m             \u001b[0;31m# 12. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m             return self.group_beam_search(\n\u001b[0m\u001b[1;32m   1665\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgroup_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3385\u001b[0m             \u001b[0;31m# do one decoder step on all beams of all sentences in batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3386\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   3388\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                 )\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1041\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 25,\n",
        "\"length_penalty\":-1.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.96,\n",
        "'diversity_penalty':1.2,\n",
        "'num_beam_groups':5,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':10\n",
        "}\n",
        "short_generator_args = {\n",
        "\"max_length\": 100,\n",
        "\"num_beams\": 25,\n",
        "\"length_penalty\":-5,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.96,\n",
        "#'diversity_penalty':0.95,\n",
        "#'num_beam_groups':5,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':5\n",
        "}\n",
        "answers_generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 20,\n",
        "\"length_penalty\":0.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.96,\n",
        "'diversity_penalty':0.9,\n",
        "'num_beam_groups':5,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':3\n",
        "}\n",
        "#\n",
        "#input_string = \"ask question and answer: \" + results + \" </s>\"\n",
        "#input_string = \"generate questions: \" + results\n",
        "input_string = \"generate questions and answer: \" + results\n",
        "#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "res = model.generate(input_ids, **generator_args)\n",
        "output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "output = [item.split(\"<sep>\") for item in output]\n",
        "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "res = model.generate(input_ids, **short_generator_args)\n",
        "output_short = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "output_short = [item.split(\"<sep>\") for item in output_short]\n",
        "for i in [*output,*output_short]:\n",
        "  print(f'Qs:{i}')\n",
        "  input_string = \"answer to the question, step by step: \"+i[0]+\" </s> context: \" + results\n",
        "  #input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "  input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "  qs_res = model.generate(input_ids, **answers_generator_args)\n",
        "  ans_output = tokenizer.batch_decode(qs_res, skip_special_tokens=True)\n",
        "  ans_output = [item.split(\"<sep>\") for item in ans_output]\n",
        "  for ans in ans_output:\n",
        "    print('----Ans:--',ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SLr7oqyoRPB",
        "outputId": "ccab3cf3-7a6c-4fd3-ad74-370876aa29f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  3,  2,  3,  2,  1,  0,  0],\n",
              "        [ 0,  3,  2,  1,  0,  0,  0,  0],\n",
              "        [ 0,  3,  2, 18,  2,  3,  2,  1]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_string = [\"answer to the question, step by step: \"+i[0]+\" </s> context: \" + results for i in output_short]\n",
        "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "qs_res = model.generate(input_ids, **answers_generator_args)\n",
        "qs_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW0wP3uPo3gv",
        "outputId": "11837818-0d83-41ce-fa92-46932e883762"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['What is the purpose of this paper?'],\n",
              " ['What is the purpose of this study?'],\n",
              " ['What was the purpose of this study?'],\n",
              " ['What is the purpose of this work?'],\n",
              " ['What is the purpose of this research?']]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IOh9k6LmwpW"
      },
      "outputs": [],
      "source": [
        "input_string = \"answer to the question:What is the purpose of this paper? </s> context: \" + results\n",
        "#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
        "res = model.generate(input_ids, **generator_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoOhAkdld5iU",
        "outputId": "6b5049ff-fb6b-42ce-8040-b969668d5b08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Prediction of crowdfunding success']\n",
            "['Improve the accuracy of crowdfunding success predictions']\n",
            "['Improve the accuracy of crowdfunding success prediction']\n",
            "['To analyze and predict crowdfunding success using semantic features']\n",
            "['Prediction Model for Crowdfunding Success']\n",
            "['To explore the relationship between crowdfunding success and buzzwords']\n",
            "['to improve the accuracy of crowdfunding success predictions']\n",
            "['To analyze and predict crowdfunding success using semantic features.']\n",
            "['To develop a model for predicting crowdfunding success']\n",
            "['To develop a model for predicting crowdfunding success.']\n"
          ]
        }
      ],
      "source": [
        "output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "output = [item.split(\"<sep>\") for item in output]\n",
        "for i in output:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp-MKmpCRlKY",
        "outputId": "816c391f-3801-4d42-86e8-e9cc8dc450d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.model_max_length "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8uXP8nkM7EG",
        "outputId": "1ef39c4a-9e4f-4159-a2a3-47d67795c7d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['How does the introduction of semantics into funding success prediction improve accuracy?',\n",
              " 'How does the introduction of semantics in funding success predict funding success?',\n",
              " 'How does the introduction of semantics into funding success prediction enhance accuracy?',\n",
              " 'How does the introduction of semantics into funding success predict accuracy?']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.batch_decode(res, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73zH1j8Wfv8E"
      },
      "source": [
        "# GPT-neo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIQLlJmZfz0M"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer,AutoModel,T5Tokenizer,T5ForConditionalGeneration,GPTNeoForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8b_e2ZRgNVu",
        "outputId": "0773b8a2-b931-416e-8ce9-f90cedec6e47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The present']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n",
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The results']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n",
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The study']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n",
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The findings']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n",
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The research']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n",
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The purpose']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n",
            "Qs:['generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors related to mental and physical health. The aim']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors,']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors that']\n",
            "----Ans:-- ['answer to the question, step by step: generate questions and answer: Aging can be experienced as a complex and challenging process, considering the increased exposure and proximity to death alongside the physical, emotional, and cognitive decline characterizing old age. Thus, interventions are critically needed to improve the elderly population\\'s quality of life and ease its pain. Such interventions must be made accessible, as individuals in this age group often rebuff recommendations to utilize them. The current study will examine the influence of a mindfulness-based intervention for seniors (MBIS) and a cognitive behavioral therapy (CBT) intervention on various psychological measures and seniors\\' attitudes toward seeking mental health treatment. Mindfulness has been defined in several ways, all sourced in the Buddhist tradition. The most accepted Western definition defines mindfulness as the maintenance of prolonged attention on human experiences (such as thoughts, emotions, and bodily sensations) in the present, \"from moment to moment,\" and in an accepting, non-judgmental manner. This definition incorporates three components that are also referenced in the bulk of Buddhist literature on the topic of mindfulness: (1) attentive awareness; (2) current experience, and (3) acceptance. The past 30 years have been marked by extensive literature pointing to the significant advantages of meditation on various factors including']\n"
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 10,\n",
        "\"length_penalty\":-0.5,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.97,\n",
        "'diversity_penalty':1.8, #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
        "'num_beam_groups':10, \n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':7\n",
        "}\n",
        "answers_generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 10,\n",
        "\"length_penalty\":0.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids':[[58]],#token of `?`\n",
        "'top_p' :0.96,\n",
        "'diversity_penalty':0.9,\n",
        "'num_beam_groups':10,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':3\n",
        "}\n",
        "def get_output_from_prompt(prompt,args):\n",
        "  input_ids = tokenizer.encode(input_string, return_tensors=\"pt\",max_length=255)\n",
        "  res = model.generate(input_ids, **args)\n",
        "  output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "  output = [item.split(\"<sep>\") for item in output]\n",
        "  return output\n",
        "#\n",
        "#input_string = \"ask question and answer: \" + results + \" </s>\"\n",
        "#input_string = \"generate questions: \" + results\n",
        "input_string = \"generate questions and answer: \" + mindfulness_paper\n",
        "#input_string = \"multitask-qa-qg: \" + intro + \" </s>\"\n",
        "output = get_output_from_prompt(input_string,generator_args)\n",
        "for i in output:\n",
        "  print(f'Qs:{i}')\n",
        "  input_string = \"answer to the question, step by step: \"+i[0]+\" </s> context: \" + mindfulness_paper\n",
        "  ans_output = get_output_from_prompt(input_string,answers_generator_args)\n",
        "  for ans in ans_output:\n",
        "    print('----Ans:--',ans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of5Z4Em1fs3U"
      },
      "source": [
        "# few shots learning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtK53r5vnjMu"
      },
      "outputs": [],
      "source": [
        "instruction = 'Followed are texts and multipile choice question with one answer and 3 distractors followed by each text:\\n' \n",
        "q1 = \"Text: According to Andrew, it never would have happened if he had not had a flat tire on Highway 10 last night at about 7:30. He was on his way to attend a three-day sales meeting when he had the flat. tyre. Unfortunately, he did not have a spare, so he pushed the car off the road, locked it up, and managed to thumb a ride back to Pine Grove. It was after eleven o'clock when he finally got home, and it was then that his real problems started. When Andrew left home at about 5:30, he had told his wife not to expect him back until Thursday or Friday. Knowing that his wife was nervous about staying in the house alone at night, Andrew took the precaution of checking all the windows in the house to be sure they were locked, so that he could report to his wife that the house was secure. He convinced his wife that the house was burglar-proof, and that she would be perfectly safe, providing she bolted the front door as soon as he drove away. Andrew's only thought as he made his way in the dark to his front door was how surprised his wife was going to be to see him, since he was not supposed to be back until Thursday or Friday. He had forgotten about the bolt on the front door. When he turned his key in the lock and the door wouldn't _ he remembered the bolt. And he remembered that he had carefully locked all of the windows. Although Andrew didn't know it at the time, a next-door neighbor had seen him approaching the house and had watched him go up the steps to the front door. In the dark, it was impossible for the neighbor to recognize Andrew, and, besides, the neighbor knew that Andrew had gone out-of-town for a three-day meeting. As a matter of fact, Andrew had asked the neighbor to keep an eye on the house while he was gone. Finding that he couldn't get in, Andrew began pounding on the front door to get his wife to open the door. According to Andrew, however, his wife is a very sound sleeper, and he knew it was going to be hard to wake her up. In the meantime, because of all the noise he had been making, the neighbor was convinced that somebody was trying to break into the house; so she called the police. When we talked to Andrew at the country jail this morning, he said that he still didn't understand how the police managed to circle the house without his seeing them. He stated that he had decided the only way to get in was to break one of the dining room windows, and that he was about, to hurl his briefcase into the window to break it when two of the officers grabbed him from behind. Andrew could not make the officers believe that he lived there; so they took him off to jail. Apparently, he did succeed in convincing them that they ought to wake up the woman in the house to check his story. But there was no answer when they knocked at the door. He tried to explain to them that his wife was a very sound sleeper, but they concluded there was nobody in the house. \\n \" +\\\n",
        "\"Question: Why did the police officers take Andrew off to jail? \\n (A) It was too late for them to contact Andrew's wife. \\n (B) Andrew did not explain clearly why he broke into the house. \\n (C) They thought it unnecessary to check Andrew's story. \\n (D) They concluded that Andrew's story was a complete invention. \\n Correct Answer is D. \"\n",
        "q2 = \"Text: School is out for the summer in some areas, and for many children, that means weeks of sleeping in late, trips to the swimming pool or hours spent in front of the TV. These months of inactivity--- away from the daily tiring things of school--- are something that young people look forward to, but it can have a detrimental influence on the knowledge they have. Studies have shown that children lose a lot in math skills during the summer months. Children who spend little time learning experience a much greater amount of learning loss than those who often take part in summer programs. So when school starts again in the fall, teachers often have to spend several weeks reteaching children material that they have forgotten. Experts suggest that parents make sure the summer months are a time of continued learning. For example, they suggest that lunch time is good time to improve a child's spoken skills, simply through conversation. A trip to the supermarket can be an opportunity to improve a child's math skills by having him or her add up the cost of food. And there is no better way to prevent learning loss than by having a child read every day. Fortunately, there are many programs that can help parents. The best place to start is at your local library. Most libraries have summer reading programs that are filled with fun activities. A. strong musical education also improves learning. Find out summer music camps in your area. We think that children should have fun during the summer break, but it should not be a vacation from learning. We strongly suggest parents should take advantage of the many educational opportunities available for their children during the coming weeks. \\n \"+\\\n",
        "\"Question: Which of the following is true? \\n (A) It isn't good for children to have fun in summer. \\n (B) Children will get bored with reading programs. \\n (C) Summer programs can be helpful for children. \\n (D) Teachers needn't help children review lessons.  \\n Correct Answer is C.\" \n",
        "q3 = \"Text: Nelson Mandela was a figure of international fame, and many details of his life and career were public knowledge. But here are four things you may not have known about the late South African leader. 1. He was a boxing fan. In his youth, Nelson Mandela enjoyed boxing and long-distance running. Even during the 27 years he spent in prison, he would exercise every morning. \"I did not enjoy the violence of boxing so much as the science of it. I was curious by how one moved one's body to protect oneself, how one used a strategy both to attack and retreat, how one paced oneself over a match,\" he wrote in his autobiography Long Walk to Freedom. 2. His original name was not Nelson. Rolihlahla Mandela was nine years old when a teacher at the primary Methodist school where he was studying in Qunu, South Africa, gave him an English name \"Nelson\" in accordance with the custom to give all school children Christian names. 3. He forgot his glasses when he was released from prison. Mr. Mandela's release on 11 February 1990 followed years of political pressure against apartheid . Mr. Mandela's reading glasses stayed behind in prison Mr. Mandela and his then-wife Winnie were taken to the centre of Cape Town to address a huge and exciting crowd. But when he pulled out the text of his speech, he realized he had forgotten his glasses and had to borrow Winnie's. 4. He had his own law firm, but it took him years to get a law degree. Mr. Mandela studied law on and off for 50 years from 1939, failing about half the courses he took. A two-year diploma in law on top of his university degree allowed him to practice, and in August 1952, he and Oliver Tambo set up South Africa's first black law firm, Mandela and Tambo, in Johannesburg. He kept on studying hard to finally secure a law degree while in prison in 1989. \\n \"+\\\n",
        "\"Question: Why did Nelson Mandela love boxing? \\n (A) Because he wanted to be a boxer. \\n (B) Because he enjoyed the violence of boxing. \\n (C) Because he appreciated the strategy in boxing. \\n (D) Because he had nothing else to do in the prison. \\n Correct Answer is C.\"\n",
        "\n",
        "q4 = \"Text: New research at Washington University School of Medicine in St. Louis shows that people who struggle with mood problems or addiction can safely quit smoking and that kicking the habit is associated with improved mental health. Cavazos-Rehg, an assistant professor of psychiatry, found that quitting or significantly cutting back on cigarette smoking was linked to improved mental health outcomes. Quitting altogether or reducing by half the number of cigarettes smoked daily was associated with lower risk for mood disorders like depression, as well as a lower likelihood of alcohol and drug problems. \"We don't know if their mental health improves first and then they are more motivated to quit smoking or if quitting smoking leads to an improvement in mental health,\" Cavazos-Rehg said. \"But either way, our findings show a strong link between quitting and a better psychiatric outlook.\" In addition, she believes the serious health risks associated with smoking make it important for doctors to work with their patients to quit, regardless of other psychiatric problems. \"About half of all smokers die from related to smoking, so we need to remember that as complicated as it can be to treat mental health issues, smoking cigarettes also causes very serious illnesses that can lead to death,\" she explained. Cavazos-Rehg and her team analyzed questionnaires gathered as part of the National Epidemiologic Study on Alcohol and Related Conditions. In the early 2000s, just under 35,000 people were surveyed. As part of the study, they answered questions about drinking, smoking and mental health in two interviews conducted three years apart. The researchers focused on data from 4,800 daily smokers. Those who had addiction or other psychiatric problems at the time of the first survey were less likely to have those same problems three years later if they had quit smoking. And those who hadn't had psychiatric problems at the initial survey were less likely to develop those problems later if they already had quit. \"We really need to spread the word and encourage doctors and patients to tackle these problems,\" Cavazos-Rehg said. \"When a patient is ready to focus on other mental health issues, it may be an ideal time to address smoking cessation, too. \\n\"+\\\n",
        "\"Which type of writing is the article likely to be? \\n (A) Science fiction. \\n (B) A health report \\n (C) An education report. \\n (D) A news report. \\n Correct Answer is B.\"\n",
        "\n",
        "\"high11185.txt\"\n",
        "\"Owning a smartphone may not be as smart as you think.They may let you surf the Internet,listen to music and snap photos wherever you are...but they also turn you into a workaholic,it seems.A study suggests that,by giving you access to emails at all times.the all-singing.all-dancing mobile phone adds as much as two hours to your working day. Researchers found that Britons work an additional 460hours a year on average as they are able to respond to emails on their mobiles.The study by technology retailer Pixmania,reveals the average UK working day is between nine and ten hours,but a further two hours is spent responding to or sending work emails,or making work calls.Almost one in ten admits spending up to three hours outside their normal working day checking work emails.Some workers confess they are on call almost 24 hours a day,with nine out of ten saying they take work emails and calls outside their normal working hours.Nearly two-thirds say they often check work emails just before they go to bed and as soon as they wake up,while over a third have replied to one in the middle of the night. Ghadi Hobeika,marketing director of Pixmania,said:\"The ability to access literally millions of apps,keep in contact via social networks and take photos and video as well as text and call has made smartphones valuable for many people.However,there are _ .Many companies expect their employees to be on call 24 hours a day,seven days a week,and smartphones mean that people literally cannot get away from work.The more constantly in contact we become. The more is expected of us in a work capacity.\"\"\n",
        "\"C\"\n",
        "\"Ghadi may agree that _ .\"\n",
        "[ \"employees are supposed to be on call 24 hours a day\", \"the ability to access many apps made smartphones worthless\", \"smartphones might turn a person into a workaholic\", \"people literally cannot get away from work without smartphones\" ]\n",
        "\"high11185.txt\"\n",
        "\"Owning a smartphone may not be as smart as you think.They may let you surf the Internet,listen to music and snap photos wherever you are...but they also turn you into a workaholic,it seems.A study suggests that,by giving you access to emails at all times.the all-singing.all-dancing mobile phone adds as much as two hours to your working day. Researchers found that Britons work an additional 460hours a year on average as they are able to respond to emails on their mobiles.The study by technology retailer Pixmania,reveals the average UK working day is between nine and ten hours,but a further two hours is spent responding to or sending work emails,or making work calls.Almost one in ten admits spending up to three hours outside their normal working day checking work emails.Some workers confess they are on call almost 24 hours a day,with nine out of ten saying they take work emails and calls outside their normal working hours.Nearly two-thirds say they often check work emails just before they go to bed and as soon as they wake up,while over a third have replied to one in the middle of the night. Ghadi Hobeika,marketing director of Pixmania,said:\"The ability to access literally millions of apps,keep in contact via social networks and take photos and video as well as text and call has made smartphones valuable for many people.However,there are _ .Many companies expect their employees to be on call 24 hours a day,seven days a week,and smartphones mean that people literally cannot get away from work.The more constantly in contact we become. The more is expected of us in a work capacity.\"\"\n",
        "\"A\"\n",
        "\"What is the main idea of this passage?\"\n",
        "[ \"Smartphones are lengthening working hours.\", \"Smartphones are becoming valuable for many people.\", \"Britons work art additional 460 hours a year on average.\", \"Smartphones are more beneficial to our life than we think.\" ]\n",
        "\"high22994.txt\"\n",
        "\"I've done many things that would make any parent proud. I've taken part in sports, community work and school events, but I am most proud to be a good big brother. I have to watch over four sisters and a brother, plus myself. That's quitea task, but one I feel I have gotten good at. My brother needs more attention than most siblings. When he was nine, he was hit by a pick-up truck on the way to the bus stop and lost use of the lower two-thirds of his body. He's been in a wheelchair for six years. My brother does many things, but sometimes he tries to do things he used to do but can't now. I' m always there with words of encouragement. I now watch my brother struggle to do as many things as he can within and beyond his limits. Should he fall, I will be the person to pick him up. I also watch as my younger sisters grow up into intelligent, beautiful young women. Whenever my brother or sisters need a helping hand or words of encouragement, I'm on the job. That's what I, the big brother, am for.\"\n",
        "\"D\"\n",
        "\"How many people are probably there in the author's family? _ .\"\n",
        "[ \"5\", \"6\", \"7\", \"8\" ]\n",
        "\"high22994.txt\"\n",
        "\"I've done many things that would make any parent proud. I've taken part in sports, community work and school events, but I am most proud to be a good big brother. I have to watch over four sisters and a brother, plus myself. That's quitea task, but one I feel I have gotten good at. My brother needs more attention than most siblings. When he was nine, he was hit by a pick-up truck on the way to the bus stop and lost use of the lower two-thirds of his body. He's been in a wheelchair for six years. My brother does many things, but sometimes he tries to do things he used to do but can't now. I' m always there with words of encouragement. I now watch my brother struggle to do as many things as he can within and beyond his limits. Should he fall, I will be the person to pick him up. I also watch as my younger sisters grow up into intelligent, beautiful young women. Whenever my brother or sisters need a helping hand or words of encouragement, I'm on the job. That's what I, the big brother, am for.\"\n",
        "\"B\"\n",
        "\"The author's brother needs more attention because _ .\"\n",
        "[ \"he is the youngest child of the family\", \"he lost the functions of his lower body\", \"he was disabled by a careless bus driver\", \"he needs the words of encouragement\" ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9PT_F3Drg6g"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "622c2a624f8843ac93b1307b025c0cf4",
            "77d8bee59fea4993988b7d67cb794b1d",
            "d72095e7c6ff4ffea4c3ae56bd17b320",
            "65f97c32a6274e7c8e5351999e3924dc",
            "3e87f939bb7e49bda23ccb7953ff595e",
            "29b3ef2d63b94d158e7c0435de82ba9f",
            "979ee0fcf33b4487821ea80287faeb65",
            "254566af90754f97972ee1c26259ac16",
            "41e0def5a00e45799ba61088654dcac5",
            "3ce5fac4351340d4bbc779880b6b62a6",
            "a7fac09af6c24f1ea35a62201c15f19d"
          ]
        },
        "id": "i80tPvTirfnc",
        "outputId": "f0a63cf8-472c-4b2d-82b0-5f2a48600bbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset race (/root/.cache/huggingface/datasets/race/middle/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "622c2a624f8843ac93b1307b025c0cf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"race\",'middle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "4ZBNZ27fsDZX",
        "outputId": "95da4d87-e73a-41cc-a683-bdddcb48ebce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  {\\n    \"answer\": \"A\",\\n    \"article\": \"\"Schoolgirls have been wearing such short skirts at Paget High School in Branston that they\\'ve been ordered to wear trousers ins...\",\\n    \"example_id\": \"high132.txt\",\\n    \"options\": [\"short skirts give people the impression of sexualisation\", \"short skirts are too expensive for parents to afford\", \"the headmaster doesn\\'t like girls wearing short skirts\", \"the girls wearing short skirts will be at the risk of being laughed at\"],\\n    \"question\": \"The girls at Paget High School are not allowed to wear skirts in that    _  .\"\\n}'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def concat_qs(data,with_qs = True ):\n",
        "  prompt = 'Text:'+data['article']+ '\\n' +\" Question:\"\n",
        "  if with_qs:\n",
        "    prompt+=\" \" + data['question'] + \" \\n (A) \"+data['options'][0] +\" \\n (B) \"+data['options'][1]+\" \\n (C) \"+data['options'][2]+\" \\n (D) \"+data['options'][3]+\" \\n Correct answer is \"+data['answer']+'. \\n '\n",
        "  return prompt\n",
        "'''  {\n",
        "    \"answer\": \"A\",\n",
        "    \"article\": \"\\\"Schoolgirls have been wearing such short skirts at Paget High School in Branston that they've been ordered to wear trousers ins...\",\n",
        "    \"example_id\": \"high132.txt\",\n",
        "    \"options\": [\"short skirts give people the impression of sexualisation\", \"short skirts are too expensive for parents to afford\", \"the headmaster doesn't like girls wearing short skirts\", \"the girls wearing short skirts will be at the risk of being laughed at\"],\n",
        "    \"question\": \"The girls at Paget High School are not allowed to wear skirts in that    _  .\"\n",
        "}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdpT-IHhtSJH"
      },
      "outputs": [],
      "source": [
        "instruction = 'Followed are texts and multipile choice question with one answer and 3 distractors followed by each text: \\n ' +\\\n",
        " concat_qs(dataset['train'][0])+ concat_qs(dataset['train'][20]) #+concat_qs(dataset['train'][40])+concat_qs(dataset['train'][60])+concat_qs(dataset['train'][1000])+concat_qs(dataset['train'][3000])\n",
        "\n",
        "\n",
        "exm_prompt = instruction + concat_qs(dataset['train'][65],False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk9dFG11ves1",
        "outputId": "cd778c18-ebdf-4fa6-f13e-94e3edb8310e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Followed are texts and multipile choice question with one answer and 3 distractors followed by each text: Text:\"I planted a seed. Finally grow fruits. Today is a great day. Pick off the star for you. Pick off the moon for you. Let it rise for you every day. Become candles burning myself. Just light you up, hey!... You are my little little apple. How much I love you, still no enough.\" This words are from the popular song You Are My Little Dear Apple. Bae Seul-Ki acted as the leading dancer in the MV of the song. She loves dancing. She became crazy about hip-hop when she was a school girl. Bai Seul-Ki was born on September 27, 1986. She is a South Korean singer and dancer. She is 168cm tall. She loves cooking. Her favourite food is spicy and salty. She like pink and red most. There are five members in her family---father, mother, two younger brothers and herself. She isn't married. After her father and mother broke up, she lived with her mother and new daddy. She enjoys being alone. Question: Bae Seul-Ki _ in the MV of the song according to the passage. (A) sang (B) danced (C) cried (D) laughed Correct answer is B. Text:Alice needs some money for a Mother's Day present. She tells her father about her problem. \"I'll pay you to do some housework. You can clean up the yard,\" her father says. \"You can also wash my car and Mum's. Both of them need to be washed because they're really dirty.\" Alice works on the yard and washes her father's car. \"Too bad! You didn't have time to wash Mum's car,\" Dad says when he pays her. The next morning Alice wakes up early. First she washes her mother's car. Then she goes to the flower shop. She also buys her mother's favourite bread on her way home. After breakfast Alice asks her mother to go to the garage. She opens the gate. \"Wow!\" Alice's mother says. \"My car is _.\" \"Open the door,\" Alice says. Alice's mother opens the door. There on the seat she sees a bunch of flowers. \"Happy Mother's Day!\" Alice says. Question: What is this story about? (A) It is about what Alice buys for her mother on Mother's Day. (B) It is about what Alice does for her mother for Mother's Day. (C) It is about why Alice's father gives her money to buy presents. (D) It is about what Alice does at home each week for her mother. Correct answer is B. Text:My name's Mary. This is my family tree. These are my parents. Their names are Bill and Grace Brown. Those are my grandparents. Their names are Hery and Linda Brown. This is my uncle. His name is John. That boy is my brother. His name is Tony. This is Susan. She is my uncle's daughter.,. (2,10) Question:\n",
            "Len: torch.Size([707])\n",
            "[\"What is Susan's name?\"]\n",
            "[\"What's Susan's name?\"]\n",
            "[\"What is Susan's brother's name?\"]\n",
            "[\"What's Mary's brother's name?\"]\n",
            "[\"How many people are there in Mary's family?\"]\n"
          ]
        }
      ],
      "source": [
        "generator_args = {\n",
        "\"max_length\": 256,\n",
        "\"num_beams\": 5,\n",
        "\"length_penalty\":-1.2,\n",
        "#\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
        "\"no_repeat_ngram_size\": 3,\n",
        "#'force_words_ids': ['(A)','(B)','(C)','(D)'],#[[58]],#token of `?`\n",
        "'top_p' :0.96,\n",
        "#'diversity_penalty':1.2,\n",
        "#'num_beam_groups':5,\n",
        "#\"return_dict_in_generate\" :True,\n",
        "'output_scores':True,\n",
        "\"early_stopping\": True,\n",
        "'num_return_sequences':5\n",
        "}\n",
        "input_ids = tokenizer.encode(exm_prompt, return_tensors=\"pt\")\n",
        "print(f'{tokenizer.decode(input_ids[0],skip_special_tokens=True)}')\n",
        "print(f'Len: {input_ids[0].shape}')\n",
        "res = model.generate(input_ids, **generator_args)\n",
        "output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "output = [item.split(\"<sep>\") for item in output]\n",
        "for i in output:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "iDzDUSevwt8f",
        "outputId": "a96374c1-2d16-4538-879c-99ce4ffa81d2"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2f5e184b0ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'(A)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'(B)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'(C)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'(D)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2520\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2604\u001b[0m                 )\n\u001b[1;32m   2605\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2606\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2607\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2608\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2797\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2798\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "tokenizer([['(A)'],['(B)'],['(C)'],['(D)']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "SFSZiIc0StHY",
        "outputId": "718ec42b-8c7a-469b-a2ff-f3d04d6e93c0"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6edc5cc5bfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"question-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'google/flan-t5-small'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google/flan-t5-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m<ipython-input-15-6edc5cc5bfa2>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"question-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'google/flan-t5-small'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'google/flan-t5-small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> q\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"question-generation\",model = 'google/flan-t5-small',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "oZy5F8sjSv2W",
        "outputId": "a944a9d5-bb65-45fe-cb66-d20756e2463a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-37a8137d986e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/question_generation/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mqg_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs_for_qg_from_answers_prepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mqg_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs_for_qg_from_answers_hl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mqg_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqg_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/question_generation/pipelines.py\u001b[0m in \u001b[0;36m_prepare_inputs_for_qg_from_answers_hl\u001b[0;34m(self, sents, answers)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0manswer_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mans_start_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: substring not found"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/content/question_generation/pipelines.py\u001b[0m(142)\u001b[0;36m_prepare_inputs_for_qg_from_answers_hl\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    140 \u001b[0;31m                \u001b[0manswer_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    141 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 142 \u001b[0;31m                \u001b[0mans_start_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    143 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    144 \u001b[0;31m                \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> qg_examples = self._prepare_inputs_for_qg_from_answers_prepend(inputs, answers)\n",
            "ipdb> qg_inputs = [example['source_text'] for example in qg_examples]\n",
            "ipdb> qg_inputs \n",
            "['answer: <pad> Forrest Gump (Hanks) context: [] </s>', 'answer: <pad> Alabama context: [] </s>', 'answer: <pad> Alabama context: [] </s>', 'answer: <pad> Alabama context: [] </s>']\n",
            "ipdb> q\n"
          ]
        }
      ],
      "source": [
        "nlp(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sthmL6SuFkvm",
        "outputId": "7c082817-6ec3-4990-c257-f1b24133acc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ],
      "source": [
        "%pdb on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHB0dDqTb-o"
      },
      "source": [
        "If you want to use the t5-base model, then pass the path through model parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_050CddNTWeU"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "px6v-bKOTy87",
        "outputId": "fbc302e3-66bf-42cd-c8a3-874896c459d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': '42',\n",
              "  'question': 'What is the answer to life, universe and everything?'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "yiYInRhOT2Fn",
        "outputId": "51d944ed-3ed1-4de7-e706-5e6cb3f7e207"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'Robert Zemeckis', 'question': 'Who directed Forrest Gump?'},\n",
              " {'answer': 'Eric Roth', 'question': 'Who directed Forrest Gump?'},\n",
              " {'answer': '1986',\n",
              "  'question': \"In what year was Winston Groom's novel published?\"},\n",
              " {'answer': 'Alabama', 'question': 'Where is Forrest Gump from?'},\n",
              " {'answer': 'differs substantially from the novel',\n",
              "  'question': 'How does Forrest Gump compare to the novel?'}]"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "XB_UuedZT3Lj",
        "outputId": "ca013472-11e8-4577-af1d-d8f1b080bb40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'gravitation', 'question': 'What is another name for gravity?'},\n",
              " {'answer': 'Earth',\n",
              "  'question': 'On what planet does gravity give weight to physical objects?'},\n",
              " {'answer': 'galaxies', 'question': 'What do the stars in the Universe form?'},\n",
              " {'answer': 'infinite range', 'question': 'What is the range of gravity?'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egRuC8QFUy0v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uiG6_NQVCIz"
      },
      "source": [
        "## Multitask QA-QG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_PKMG28VhxM"
      },
      "source": [
        "### small-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAkVmsH9VEIu"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"multitask-qa-qg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dRiLecTVk8E"
      },
      "source": [
        "#### QG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "GKA65C51VLGu",
        "outputId": "c164bac6-6857-43f1-b895-e42103c87c57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'Python',\n",
              "  'question': 'What is an interpreted, high-level, general-purpose programming language?'},\n",
              " {'answer': 'Guido van Rossum', 'question': 'Who created Python?'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "jAAxbcwzVXlV",
        "outputId": "c58e2a5e-5267-408d-b45e-634f93cb4c90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'gravitation',\n",
              "  'question': 'What is a natural phenomenon called when all things with mass or energy are brought toward one another?'},\n",
              " {'answer': 'Earth',\n",
              "  'question': 'On what planet does gravity give weight to physical objects?'},\n",
              " {'answer': 'galaxies', 'question': 'What did the stars group together into?'},\n",
              " {'answer': 'infinite range',\n",
              "  'question': 'What kind of range does Gravity have?'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "a81EN_WWVpae",
        "outputId": "a33e9833-9c04-4347-f841-433860c2a097"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'Robert Zemeckis', 'question': 'Who directed Forrest Gump?'},\n",
              " {'answer': 'Eric Roth', 'question': 'Who wrote Forrest Gump?'},\n",
              " {'answer': '1986', 'question': 'In what year was Forrest Gump based?'},\n",
              " {'answer': 'Alabama', 'question': 'Where is Forrest Gump from?'},\n",
              " {'answer': 'differs substantially from the novel',\n",
              "  'question': 'What does Forrest Gump differ from the novel?'}]"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9c2CkhhVsxs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuOL3X_XV28R"
      },
      "source": [
        "#### QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xe-v3I8aV4En",
        "outputId": "e43c25ed-e310-4875-af90-7a3f29847361"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": "{\"type\":\"string\"}",
            "text/plain": [
              "'Guido van Rossum'"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp({\n",
        "  \"question\": \"Who created Python ?\",\n",
        "  \"context\": text\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZchZFXPuWI62",
        "outputId": "baef6806-e2e1-4ea7-ba9b-3077d638ac1a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": "{\"type\":\"string\"}",
            "text/plain": [
              "'Eric Roth'"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp({\n",
        "    \"question\": \"Who wrote Forrest Gump ?\",\n",
        "     \"context\": text4\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARBRwBj5WVga"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEk-EU9UWaBr"
      },
      "source": [
        "### base-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "cf72d967d81444f9b93a1cbe2f5e4eb6",
            "8df06c887e5f4629b5392c068dda1204",
            "48a87f26c0e145c79bcacb7a66f2e03c",
            "f05b8fb4c7764f9caff4eba3d866c5e8",
            "57af1dc901794180a0c3a0ad598a0fec",
            "fbbeaf73ff094cc29a62694fd00289da",
            "ec840a9a88d647eda8090fba3768d36a",
            "dcdd610bc53549109eff5ee720030702",
            "c75db10073d6493aa9834c789537dc4a",
            "ca0feacb455b4d00a9db55caa335f7e9",
            "11da8c1e1d024b6e8cbb6de0efa47380",
            "f07cb8f47ccb41728ed60b9ba64ddee8",
            "19fdf701402e4eae9925288271e9b451",
            "5bff9fcea04d458dad3de823aa714873",
            "d46d560becfc4da281f51e67a12b29b1",
            "2f1d5aeea19c46448381385e567245bd",
            "8a65f1aedc7749ccb66db78aa9852c45",
            "34d93b3fe1704ed38b5dceee3f777daa",
            "f10552ead0394059a49b553a6d2ffbc9",
            "0221a5ef69194cf6973ea823ff202c65",
            "31f933d9c20845b8b7b7aba903878f64",
            "8dd03805a6fb4413a5225f0b38233cae",
            "fd48fb6d8ff6497c893e6b988746cec6",
            "3dfad120fef440b9aed00ee86df227fa",
            "0788251f4ea64c8084da30213e5bae4c",
            "06402537a8fd42bda097759eb76519cb",
            "d1b088bfd7c84df4bfe125d64152da2e",
            "de987e3495914bd3bf05fca5f0411595",
            "90ffeb50fe374fec91476463f9748738",
            "4791b0873f354c758e626837ebfeb5ff",
            "9760bd142c844c6d8c0c7a63ad22c020",
            "7180636e594e4c69ad73b22cbba633c0",
            "85e2b7c6a3964c7eb1ffc8fc74737600",
            "864ec70ff2ca47a6ac48d4bb67126b46",
            "9e12cbbe986d498290844708378f93a0",
            "9890ecc31a2c4bdaa7c9f2aeb8dcde4a",
            "35e37f582bad448a9f95e73c5375433b",
            "c01125ec848344959c88f7e090723d6f",
            "82c653628f4c4bb8bf2b6bd49c7ae102",
            "34aab39b61db49ada48366e68faedc5a",
            "1e3fec599654493284a8e9febd1734de",
            "ce175483b07742398ee572bd32346000",
            "665614a60eab4caba1c152d2cd71a33a",
            "c800ebd19e4141c3952a5e64061710a0",
            "055e538c3245473bb9ccc507492ae084",
            "4039b52964244324b7ad4dcb72f766dc",
            "920b1612f1bf469a81770461ec07cf0e",
            "a8889b2e8071450383475710e240fd81"
          ]
        },
        "id": "qx1KjJzaWa-a",
        "outputId": "0c7e1230-e132-4a6e-ad9a-537d30133ec6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf72d967d81444f9b93a1cbe2f5e4eb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=629.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c75db10073d6493aa9834c789537dc4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a65f1aedc7749ccb66db78aa9852c45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=31.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0788251f4ea64c8084da30213e5bae4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=65.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85e2b7c6a3964c7eb1ffc8fc74737600",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=90.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e3fec599654493284a8e9febd1734de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891612585.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"multitask-qa-qg\", model=\"valhalla/t5-base-qa-qg-hl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL5Cgm2XWikl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCA0sjtWldK"
      },
      "source": [
        "#### QG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "1Q89FhL1WldS",
        "outputId": "6a1c87f7-4601-4412-e21a-dded713ba188"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'Python',\n",
              "  'question': 'What programming language was created by Guido van Rossum?'},\n",
              " {'answer': 'Guido van Rossum', 'question': 'Who created Python?'},\n",
              " {'answer': '1991', 'question': 'Who created Python?'}]"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "Xwby-LJpWldj",
        "outputId": "5e464b23-0609-4e51-92e7-18a440ffd2dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': 'gravitation', 'question': 'What is another name for gravity?'},\n",
              " {'answer': 'Earth',\n",
              "  'question': 'On what planet does gravity give weight to physical objects?'},\n",
              " {'answer': 'galaxies', 'question': 'What do the stars form into?'},\n",
              " {'answer': 'weaker',\n",
              "  'question': \"Gravity's effects become what as objects get further away?\"}]"
            ]
          },
          "execution_count": 16,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "iTO5g4glWldp",
        "outputId": "908e10a6-8d0d-4134-97f5-f142c7116bf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answer': '1994', 'question': 'When was Forrest Gump released?'},\n",
              " {'answer': '1986',\n",
              "  'question': \"In what year was Winston Groom's novel based on Forrest Gump?\"},\n",
              " {'answer': 'Alabama', 'question': 'Where is Forrest Gump from?'},\n",
              " {'answer': 'novel',\n",
              "  'question': 'Forrest Gump differs substantially from what?'}]"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YIZtVu8Wldw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh46cnicWld1"
      },
      "source": [
        "#### QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "67nbW-dgWld2",
        "outputId": "348f1bbf-eccf-4b10-f455-71a389411dce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": "{\"type\":\"string\"}",
            "text/plain": [
              "'Guido van Rossum'"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp({\n",
        "  \"question\": \"Who created Python ?\",\n",
        "  \"context\": text\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2Ew7vRA2Wld6",
        "outputId": "950d9949-1ed8-4041-8c47-80eb025dd1f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": "{\"type\":\"string\"}",
            "text/plain": [
              "'Eric Roth'"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp({\n",
        "    \"question\": \"Who wrote Forrest Gump ?\",\n",
        "     \"context\": text4\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CkkYm6aWld-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Wq-2nuXDvl"
      },
      "source": [
        "## End-to-End QG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS4XfQh0X3W8"
      },
      "source": [
        "### small model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "596c754de829409fbafc768d6157d1ba",
            "f5420a7a84324b6aab69555cf2c4c28e",
            "96554632324f4f4d8f51add8ded21c58",
            "9e10200df4fb47de8d11d919506fd420",
            "d47a34125e0445d69e2e8b42709e3be7",
            "c23e78ce9eed4d2f8bd7453fb76f1813",
            "0b0663698db5484294f8cf0f4c9a7b8f",
            "198e22ecb91e4d0bbe659227e0263471",
            "93dc353d29ac415abaf193f9e9da83ab",
            "b3f78454700b44f690716acb5ba9405e",
            "e87a4d67ef534786b8be863eb55a8240",
            "86c324181fa74c2483ef80064f23248e",
            "9691c795b5534a09a3db391d13dc6337",
            "feaadea50b75486d9605c3f1b063bee2",
            "447f364655974be8b923f26261fa4635",
            "12412cb52fa844e1aec2799b6df43f52",
            "0ff56e11acfb42138ba524ebcdc664cb",
            "5ab4c3dbcd834437a3ea4b512b875f65",
            "4ef7f6001ec44d11af53c23c1a4e5368",
            "dbbd825fd25949408c836238b0b8119e",
            "40a77051f36349d6a6e4f94554331839",
            "d440bbd4fe414f568d3d5136279a6b79",
            "eb0f17ee32b04ba3844f609066bf7484",
            "9fe094b25570406db96cb643f5320b09",
            "2f5f23d75292466985974cbc025d9058",
            "e770c99833df4c838ba632cb5af35920",
            "7abd8e9099dc4ba2be5593940e3d69ee",
            "b1eb7e8892604cf995c8a7749904f166",
            "74075048891040a2b7da864cef5a3f23",
            "656c13ab65bf40f691c14d3d7c9dd6d6",
            "57fef133ea4b4c1f85388b9487b0032f",
            "413fab342b03411cbcb97b58b0450086",
            "68a1ab12e17a4ba5b87f3ad55f6e488e",
            "e3b79fc7017c4135afae4c47b8e5e7cb",
            "2b8c1735915c4c0b82e7faa92b45c281",
            "fcf7a63396ae408b8e2570aaa69a7ef1",
            "2a1fcedd1d4f4f0099ec72c1acb30419",
            "1dadb55aade34302b54d6a4e5b82207a",
            "fefa681970db4c259c2e1aeb93abd6d5",
            "29e5c0b0b02f44dea7be3589d2327e0b",
            "b69b3fcb5e4d48b192f6aa7f45f01889",
            "9ac10c77ff6b48978edc68ae9bc33cb9",
            "faf5ec950167410f8dc89363f0c3c627",
            "b08383598a1848118730f57ab4a44215",
            "9fd4d4e6bcb444aeb115b9cec2b5252f",
            "4b41626100c34fc081dd1cefb0fc0211",
            "c469ebd9fe344f92938478390737c523",
            "e0342c84e1404c93972fdb8cad71b363"
          ]
        },
        "id": "mzJjXzpmXG7t",
        "outputId": "41677aa0-9bdf-4d23-a879-bdd87e34f1a5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "596c754de829409fbafc768d6157d1ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1348.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93dc353d29ac415abaf193f9e9da83ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ff56e11acfb42138ba524ebcdc664cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=31.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f5f23d75292466985974cbc025d9058",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1786.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68a1ab12e17a4ba5b87f3ad55f6e488e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=124.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b69b3fcb5e4d48b192f6aa7f45f01889",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242013376.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"e2e-qg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "LMdylLZhXOED",
        "outputId": "055ff255-a788-45f4-b0ab-563d5075e115"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 256, but you input_length is only 54. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Who created Python?',\n",
              " 'When was Python first released?',\n",
              " \"What is Python's design philosophy?\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "Jd_2-fvYXaXb",
        "outputId": "cbc7807b-3a7d-4f21-ae13-1e7959e01330"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 256, but you input_length is only 161. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What is the Latin word for gravitas?',\n",
              " 'What does gravity give weight to on Earth?',\n",
              " \"The Moon's gravity causes what?\",\n",
              " 'Gravity has an infinite range, but its effects become weaker as objects get further away?']"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "bpDIQV3RXeAU",
        "outputId": "8751cd92-0893-4e33-8151-c35efb67caf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 256, but you input_length is only 135. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Who directed the 1994 American comedy-drama film Forrest Gump?',\n",
              " 'What is the name of the 1986 film that stars Tom Hanks, Robin Wright, Gary Sinise, Mykelti Williamson, and Sally Field?',\n",
              " 'Who is the author of the 1994 film forrest gump? What is Forrest a film that is based on?']"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDo01sKdXjfG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNBlsV_eX9mm"
      },
      "source": [
        "### base-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "354f7a87bb71411795bc63d11189094b",
            "eb9fb93a24f242f987e1fd6993c717e0",
            "277f08f8db154f29a23c087fa32b4cdf",
            "7600126083074eac9d68a285ca1b51ce",
            "224671f61e8a473a87070572a74d0081",
            "d8178120b6e04b43b17163b3f35b8c2a",
            "b860bd97046a4f5b93a5550a5b9a61b6",
            "bd34099fedfa436c9567678aebd8e1d1",
            "6032c576323540e1b6f47f2ff0f1d6b2",
            "39a08aba8f474fc0854e94febee2b4e0",
            "19a943f30f884893badaafb1aa5a48a7",
            "2dc45394f108468ea977eb797dae4682",
            "6e5c0103307b4c7a84f10e496b92b741",
            "8c498364dd904bbebb68a5277396d9b3",
            "1fa1810f1d794a50bd776f681394410c",
            "4906ff20a11a497ea7d84bf030581c5e",
            "ad01de75faf54ac595c540342bfbd16c",
            "dc1a584f4d8a40eca39bd2efed426eec",
            "1dc95b92e9db4eeb82851c679f709f8c",
            "b3e04bba6f5348c1b6a0d5c818c09e2a",
            "19f6ee3c487c405c9ea9e0bfadf31906",
            "5ba3a07444d049ef99947ab336b20c07",
            "97caa06c0b324d3aa829abdc14af6b5a",
            "afb1ace8c86e4106a53dd4f0c26f7ec7",
            "088383be40254e9e80f242bada211795",
            "0f6d8f6cde8e4b55a26d6e5594a1b412",
            "88824aa767d04222ad3c66ab8a80d4c1",
            "d180068e0b894d7bb0cf2b86772c39bb",
            "c035151659df42a581cfc9bed5f59b43",
            "9cecc5ca989a41c39d9836fd29581459",
            "724cdb6f5b78444e87b10f383f85923a",
            "f8b5c6f268f14f17af6666fa6b03fea1",
            "135b9811faba4960a3cebf8ad49f3f05",
            "50eb6e47233340e5af9d0c809e24b30e",
            "1a00e3b126494f6aa42a1739944cecb1",
            "849ccde622f941c482e6c179763348a4",
            "c38d04838c9c4c47b9b8d90e7297a931",
            "61a0cb3a3bb445b28bc29322179aa54f",
            "f56e1ca817f14b7ab8a2f40b2826f930",
            "9748513b6c524dd3972b48041b0ac989",
            "7e88959c5b7240818378e8ff75f63708",
            "e34ea489ea824b81bc5bbaf07aec79ca",
            "5db14e649c2c4ef095c31f2ed4d71094",
            "591ae711a57f4fa2b8f3d67a8f94d507",
            "8c12a663bbd342979337e2e2ba029219",
            "2abf9fbf4e2f4d87bb73fc4eb2bebdc9",
            "69b4598ae36348448d6bc8b31f6fee41",
            "d9840f3af0a94581824ce3459773fcb7"
          ]
        },
        "id": "B5oUJ5_0X_Je",
        "outputId": "d7d73f47-e92f-4e5f-dd30-85dc6c7d1a62"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "354f7a87bb71411795bc63d11189094b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1350.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6032c576323540e1b6f47f2ff0f1d6b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad01de75faf54ac595c540342bfbd16c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=31.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "088383be40254e9e80f242bada211795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1786.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "135b9811faba4960a3cebf8ad49f3f05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=195.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e88959c5b7240818378e8ff75f63708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891608946.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"e2e-qg\", model=\"valhalla/t5-base-e2e-qg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "8lY3vzbgYCm3",
        "outputId": "e3544994-dc8a-4734-f406-f85c71e080d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 256, but you input_length is only 54. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Who created Python?',\n",
              " 'When was Python first released?',\n",
              " \"What is Python's design philosophy?\"]"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "pWzd3JFQYI2O",
        "outputId": "9a0f5db8-e337-47ce-a6da-d0c056bf3187"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 256, but you input_length is only 161. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What is the Latin word for gravitation?',\n",
              " 'What does gravity give to physical objects on Earth?',\n",
              " \"The Moon's gravity causes what?\",\n",
              " 'Gravity has an infinite range, but its effects become weaker as objects get further away?']"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "KHnw2ZlNYKNz",
        "outputId": "dc902645-aee4-44d9-d3a5-a53d0ff2425a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your max_length is set to 256, but you input_length is only 135. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Who directed Forrest Gump?',\n",
              " 'Who wrote the book of the same name?',\n",
              " \"What is the name of the film based on Winston Groom's novel?\",\n",
              " 'Which actor stars in the film?']"
            ]
          },
          "execution_count": 11,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMY7ZLdYYUkL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8uiG6_NQVCIz",
        "Y_PKMG28VhxM",
        "8dRiLecTVk8E",
        "SuOL3X_XV28R",
        "gEk-EU9UWaBr",
        "IwCA0sjtWldK",
        "qh46cnicWld1",
        "z_Wq-2nuXDvl",
        "xS4XfQh0X3W8",
        "CNBlsV_eX9mm"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00995f81b8264f359381d0d0118f956a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87be85b1ed784d30b487083cf846c1d0",
              "IPY_MODEL_c48be6368fc848caa740cdf52fe0d39d",
              "IPY_MODEL_ffe99f3b2a454586966c1cd0e96f0e54"
            ],
            "layout": "IPY_MODEL_627ad910a3d2481db45f1a9bb90a1bfe"
          }
        },
        "010205a92d6444178afb7027270cd583": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0221a5ef69194cf6973ea823ff202c65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049495d9315e4e2fb11bdda61e044e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "055e538c3245473bb9ccc507492ae084": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06402537a8fd42bda097759eb76519cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90ffeb50fe374fec91476463f9748738",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4791b0873f354c758e626837ebfeb5ff",
            "value": 65
          }
        },
        "0788251f4ea64c8084da30213e5bae4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06402537a8fd42bda097759eb76519cb",
              "IPY_MODEL_d1b088bfd7c84df4bfe125d64152da2e"
            ],
            "layout": "IPY_MODEL_de987e3495914bd3bf05fca5f0411595"
          }
        },
        "088383be40254e9e80f242bada211795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f6d8f6cde8e4b55a26d6e5594a1b412",
              "IPY_MODEL_88824aa767d04222ad3c66ab8a80d4c1"
            ],
            "layout": "IPY_MODEL_d180068e0b894d7bb0cf2b86772c39bb"
          }
        },
        "0b0663698db5484294f8cf0f4c9a7b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd0bb5768da4cde8948551235d9be0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f6d8f6cde8e4b55a26d6e5594a1b412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c035151659df42a581cfc9bed5f59b43",
            "max": 1786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cecc5ca989a41c39d9836fd29581459",
            "value": 1786
          }
        },
        "0ff56e11acfb42138ba524ebcdc664cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ab4c3dbcd834437a3ea4b512b875f65",
              "IPY_MODEL_4ef7f6001ec44d11af53c23c1a4e5368"
            ],
            "layout": "IPY_MODEL_dbbd825fd25949408c836238b0b8119e"
          }
        },
        "1041e11ae58f4846a39e16ddf45b7477": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbd78e57aab1415087eec7b2fa4202fe",
            "placeholder": "​",
            "style": "IPY_MODEL_bde77fab7cfd4d399f79add1b210a0f4",
            "value": " 2.15k/2.15k [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "11da8c1e1d024b6e8cbb6de0efa47380": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46d560becfc4da281f51e67a12b29b1",
            "placeholder": "​",
            "style": "IPY_MODEL_2f1d5aeea19c46448381385e567245bd",
            "value": " 792k/792k [00:04&lt;00:00, 192kB/s]"
          }
        },
        "12412cb52fa844e1aec2799b6df43f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "135b9811faba4960a3cebf8ad49f3f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50eb6e47233340e5af9d0c809e24b30e",
              "IPY_MODEL_1a00e3b126494f6aa42a1739944cecb1"
            ],
            "layout": "IPY_MODEL_849ccde622f941c482e6c179763348a4"
          }
        },
        "198e22ecb91e4d0bbe659227e0263471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19a943f30f884893badaafb1aa5a48a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa1810f1d794a50bd776f681394410c",
            "placeholder": "​",
            "style": "IPY_MODEL_4906ff20a11a497ea7d84bf030581c5e",
            "value": " 792k/792k [00:44&lt;00:00, 18.0kB/s]"
          }
        },
        "19f6ee3c487c405c9ea9e0bfadf31906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19fdf701402e4eae9925288271e9b451": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a00e3b126494f6aa42a1739944cecb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f56e1ca817f14b7ab8a2f40b2826f930",
            "placeholder": "​",
            "style": "IPY_MODEL_9748513b6c524dd3972b48041b0ac989",
            "value": " 195/195 [00:38&lt;00:00, 5.09B/s]"
          }
        },
        "1dadb55aade34302b54d6a4e5b82207a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "1dc95b92e9db4eeb82851c679f709f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97caa06c0b324d3aa829abdc14af6b5a",
            "placeholder": "​",
            "style": "IPY_MODEL_afb1ace8c86e4106a53dd4f0c26f7ec7",
            "value": " 31.0/31.0 [00:03&lt;00:00, 9.59B/s]"
          }
        },
        "1e3fec599654493284a8e9febd1734de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce175483b07742398ee572bd32346000",
              "IPY_MODEL_665614a60eab4caba1c152d2cd71a33a"
            ],
            "layout": "IPY_MODEL_c800ebd19e4141c3952a5e64061710a0"
          }
        },
        "1fa1810f1d794a50bd776f681394410c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "224671f61e8a473a87070572a74d0081": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22818ab4c92b42d581825afa9b544a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22bfa2b6304f41b485d4f60a33beec8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_292e7911379a42dc8d8f6373ac86145f",
            "max": 1786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cd0bb5768da4cde8948551235d9be0f",
            "value": 1786
          }
        },
        "254566af90754f97972ee1c26259ac16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276fc7a3ed994b44a78b1fe682384ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc32deac3449452183e2e275907e5c03",
            "placeholder": "​",
            "style": "IPY_MODEL_47481f637f3742c0bf022fdea29ee39e",
            "value": "Downloading: 100%"
          }
        },
        "277f08f8db154f29a23c087fa32b4cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b860bd97046a4f5b93a5550a5b9a61b6",
            "placeholder": "​",
            "style": "IPY_MODEL_bd34099fedfa436c9567678aebd8e1d1",
            "value": " 1.35k/1.35k [00:01&lt;00:00, 779B/s]"
          }
        },
        "292e7911379a42dc8d8f6373ac86145f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b3ef2d63b94d158e7c0435de82ba9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e5c0b0b02f44dea7be3589d2327e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1fcedd1d4f4f0099ec72c1acb30419": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2abf9fbf4e2f4d87bb73fc4eb2bebdc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2b8c1735915c4c0b82e7faa92b45c281": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fefa681970db4c259c2e1aeb93abd6d5",
            "placeholder": "​",
            "style": "IPY_MODEL_29e5c0b0b02f44dea7be3589d2327e0b",
            "value": " 124/124 [00:14&lt;00:00, 8.63B/s]"
          }
        },
        "2dc45394f108468ea977eb797dae4682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1d5aeea19c46448381385e567245bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f5f23d75292466985974cbc025d9058": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e770c99833df4c838ba632cb5af35920",
              "IPY_MODEL_7abd8e9099dc4ba2be5593940e3d69ee"
            ],
            "layout": "IPY_MODEL_b1eb7e8892604cf995c8a7749904f166"
          }
        },
        "31f933d9c20845b8b7b7aba903878f64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34aab39b61db49ada48366e68faedc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34d93b3fe1704ed38b5dceee3f777daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f933d9c20845b8b7b7aba903878f64",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dd03805a6fb4413a5225f0b38233cae",
            "value": 31
          }
        },
        "354f7a87bb71411795bc63d11189094b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb9fb93a24f242f987e1fd6993c717e0",
              "IPY_MODEL_277f08f8db154f29a23c087fa32b4cdf"
            ],
            "layout": "IPY_MODEL_7600126083074eac9d68a285ca1b51ce"
          }
        },
        "35e37f582bad448a9f95e73c5375433b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a08aba8f474fc0854e94febee2b4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e5c0103307b4c7a84f10e496b92b741",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c498364dd904bbebb68a5277396d9b3",
            "value": 791656
          }
        },
        "3ce5fac4351340d4bbc779880b6b62a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d7f637b90c94df9943de38974066ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dfad120fef440b9aed00ee86df227fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e87f939bb7e49bda23ccb7953ff595e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4039b52964244324b7ad4dcb72f766dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "40a77051f36349d6a6e4f94554331839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413fab342b03411cbcb97b58b0450086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41e0def5a00e45799ba61088654dcac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4319d8ca6ee74b67b52b0c6bfcc1bb4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a0a349583a407e86a2252685f4aeee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "447f364655974be8b923f26261fa4635": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "458ef3b2aa934cdf9c5367afcf6d5609": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461c35f738894b10b3a6cbc956941f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4319d8ca6ee74b67b52b0c6bfcc1bb4e",
            "placeholder": "​",
            "style": "IPY_MODEL_b21d9ba0530b4109b7aa9c5cef5de67e",
            "value": "Downloading: 100%"
          }
        },
        "47481f637f3742c0bf022fdea29ee39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4791b0873f354c758e626837ebfeb5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "48a87f26c0e145c79bcacb7a66f2e03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec840a9a88d647eda8090fba3768d36a",
            "placeholder": "​",
            "style": "IPY_MODEL_dcdd610bc53549109eff5ee720030702",
            "value": " 629/629 [00:43&lt;00:00, 14.4B/s]"
          }
        },
        "4906ff20a11a497ea7d84bf030581c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b41626100c34fc081dd1cefb0fc0211": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4c451f3095b243f797b32e0700d11e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3123861517461883dad2f0b1df8610": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4eee7dad4502441ead4d3416d920879b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef7f6001ec44d11af53c23c1a4e5368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb0f17ee32b04ba3844f609066bf7484",
            "placeholder": "​",
            "style": "IPY_MODEL_9fe094b25570406db96cb643f5320b09",
            "value": " 31.0/31.0 [00:03&lt;00:00, 9.58B/s]"
          }
        },
        "50eb6e47233340e5af9d0c809e24b30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c38d04838c9c4c47b9b8d90e7297a931",
            "max": 195,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61a0cb3a3bb445b28bc29322179aa54f",
            "value": 195
          }
        },
        "57af1dc901794180a0c3a0ad598a0fec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57fef133ea4b4c1f85388b9487b0032f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "591ae711a57f4fa2b8f3d67a8f94d507": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "596c754de829409fbafc768d6157d1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5420a7a84324b6aab69555cf2c4c28e",
              "IPY_MODEL_96554632324f4f4d8f51add8ded21c58"
            ],
            "layout": "IPY_MODEL_9e10200df4fb47de8d11d919506fd420"
          }
        },
        "5ab4c3dbcd834437a3ea4b512b875f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40a77051f36349d6a6e4f94554331839",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d440bbd4fe414f568d3d5136279a6b79",
            "value": 31
          }
        },
        "5ba3a07444d049ef99947ab336b20c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "5bff9fcea04d458dad3de823aa714873": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "5c3eb3c7b038419e980e85eb151df6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e7277e0372406b8aec312fd969307f",
            "max": 1359,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f00acf8a444844ab871c326c9556aea0",
            "value": 1359
          }
        },
        "5db14e649c2c4ef095c31f2ed4d71094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69b4598ae36348448d6bc8b31f6fee41",
            "placeholder": "​",
            "style": "IPY_MODEL_d9840f3af0a94581824ce3459773fcb7",
            "value": " 892M/892M [00:32&lt;00:00, 27.1MB/s]"
          }
        },
        "6032c576323540e1b6f47f2ff0f1d6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39a08aba8f474fc0854e94febee2b4e0",
              "IPY_MODEL_19a943f30f884893badaafb1aa5a48a7"
            ],
            "layout": "IPY_MODEL_2dc45394f108468ea977eb797dae4682"
          }
        },
        "60e7277e0372406b8aec312fd969307f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a0cb3a3bb445b28bc29322179aa54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "622c2a624f8843ac93b1307b025c0cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77d8bee59fea4993988b7d67cb794b1d",
              "IPY_MODEL_d72095e7c6ff4ffea4c3ae56bd17b320",
              "IPY_MODEL_65f97c32a6274e7c8e5351999e3924dc"
            ],
            "layout": "IPY_MODEL_3e87f939bb7e49bda23ccb7953ff595e"
          }
        },
        "627ad910a3d2481db45f1a9bb90a1bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6302a8d0355d4f678ba15a6e831fa367": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "656c13ab65bf40f691c14d3d7c9dd6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "65f97c32a6274e7c8e5351999e3924dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce5fac4351340d4bbc779880b6b62a6",
            "placeholder": "​",
            "style": "IPY_MODEL_a7fac09af6c24f1ea35a62201c15f19d",
            "value": " 3/3 [00:00&lt;00:00,  9.33it/s]"
          }
        },
        "665614a60eab4caba1c152d2cd71a33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_920b1612f1bf469a81770461ec07cf0e",
            "placeholder": "​",
            "style": "IPY_MODEL_a8889b2e8071450383475710e240fd81",
            "value": " 892M/892M [00:32&lt;00:00, 27.4MB/s]"
          }
        },
        "68a1ab12e17a4ba5b87f3ad55f6e488e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3b79fc7017c4135afae4c47b8e5e7cb",
              "IPY_MODEL_2b8c1735915c4c0b82e7faa92b45c281"
            ],
            "layout": "IPY_MODEL_fcf7a63396ae408b8e2570aaa69a7ef1"
          }
        },
        "69b4598ae36348448d6bc8b31f6fee41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e232bf110c848978295f77475938326": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d7f637b90c94df9943de38974066ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_4eee7dad4502441ead4d3416d920879b",
            "value": " 792k/792k [00:00&lt;00:00, 11.1MB/s]"
          }
        },
        "6e39bde94ab742a49159399e5467466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_461c35f738894b10b3a6cbc956941f94",
              "IPY_MODEL_5c3eb3c7b038419e980e85eb151df6b0",
              "IPY_MODEL_951c85d99bb9439d8e2700331a6a5900"
            ],
            "layout": "IPY_MODEL_78bd7877d76f40b68cf2ec43085eecc6"
          }
        },
        "6e5c0103307b4c7a84f10e496b92b741": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7180636e594e4c69ad73b22cbba633c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "724cdb6f5b78444e87b10f383f85923a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74075048891040a2b7da864cef5a3f23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7600126083074eac9d68a285ca1b51ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d8bee59fea4993988b7d67cb794b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b3ef2d63b94d158e7c0435de82ba9f",
            "placeholder": "​",
            "style": "IPY_MODEL_979ee0fcf33b4487821ea80287faeb65",
            "value": "100%"
          }
        },
        "78bd7877d76f40b68cf2ec43085eecc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79e201af2bbe44a2bad6fa0d47500ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7abd8e9099dc4ba2be5593940e3d69ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57fef133ea4b4c1f85388b9487b0032f",
            "placeholder": "​",
            "style": "IPY_MODEL_413fab342b03411cbcb97b58b0450086",
            "value": " 1.79k/1.79k [00:01&lt;00:00, 1.11kB/s]"
          }
        },
        "7e88959c5b7240818378e8ff75f63708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e34ea489ea824b81bc5bbaf07aec79ca",
              "IPY_MODEL_5db14e649c2c4ef095c31f2ed4d71094"
            ],
            "layout": "IPY_MODEL_591ae711a57f4fa2b8f3d67a8f94d507"
          }
        },
        "82c653628f4c4bb8bf2b6bd49c7ae102": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "849ccde622f941c482e6c179763348a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e2b7c6a3964c7eb1ffc8fc74737600": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_864ec70ff2ca47a6ac48d4bb67126b46",
              "IPY_MODEL_9e12cbbe986d498290844708378f93a0"
            ],
            "layout": "IPY_MODEL_9890ecc31a2c4bdaa7c9f2aeb8dcde4a"
          }
        },
        "864ec70ff2ca47a6ac48d4bb67126b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e37f582bad448a9f95e73c5375433b",
            "max": 90,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c01125ec848344959c88f7e090723d6f",
            "value": 90
          }
        },
        "86c324181fa74c2483ef80064f23248e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87be85b1ed784d30b487083cf846c1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2e74f9f05c94ab5a6bfa81292b43da1",
            "placeholder": "​",
            "style": "IPY_MODEL_79e201af2bbe44a2bad6fa0d47500ec5",
            "value": "Downloading: 100%"
          }
        },
        "8829e41bbca54ad28066cbf3e8d0bcd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88824aa767d04222ad3c66ab8a80d4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_724cdb6f5b78444e87b10f383f85923a",
            "placeholder": "​",
            "style": "IPY_MODEL_f8b5c6f268f14f17af6666fa6b03fea1",
            "value": " 1.79k/1.79k [00:01&lt;00:00, 1.09kB/s]"
          }
        },
        "8a65f1aedc7749ccb66db78aa9852c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34d93b3fe1704ed38b5dceee3f777daa",
              "IPY_MODEL_f10552ead0394059a49b553a6d2ffbc9"
            ],
            "layout": "IPY_MODEL_0221a5ef69194cf6973ea823ff202c65"
          }
        },
        "8c12a663bbd342979337e2e2ba029219": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c498364dd904bbebb68a5277396d9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8dd03805a6fb4413a5225f0b38233cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8df06c887e5f4629b5392c068dda1204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57af1dc901794180a0c3a0ad598a0fec",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbbeaf73ff094cc29a62694fd00289da",
            "value": 629
          }
        },
        "90ffeb50fe374fec91476463f9748738": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "920b1612f1bf469a81770461ec07cf0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dc353d29ac415abaf193f9e9da83ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3f78454700b44f690716acb5ba9405e",
              "IPY_MODEL_e87a4d67ef534786b8be863eb55a8240"
            ],
            "layout": "IPY_MODEL_86c324181fa74c2483ef80064f23248e"
          }
        },
        "9443cce1956346cebcb42bfea6f0429b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a5b4cc5935410288f11ef197bdc223",
            "placeholder": "​",
            "style": "IPY_MODEL_d66bc4ef74eb411ba1915bcde1c3bfa0",
            "value": "Downloading: 100%"
          }
        },
        "944b0ebb16b248ef8720ef22a807b130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "951c85d99bb9439d8e2700331a6a5900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbf3c7defad6444581ee24eed49fef0e",
            "placeholder": "​",
            "style": "IPY_MODEL_049495d9315e4e2fb11bdda61e044e37",
            "value": " 1.36k/1.36k [00:00&lt;00:00, 22.2kB/s]"
          }
        },
        "965033b332ee4c48bd233b2e64da3f78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96554632324f4f4d8f51add8ded21c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0663698db5484294f8cf0f4c9a7b8f",
            "placeholder": "​",
            "style": "IPY_MODEL_198e22ecb91e4d0bbe659227e0263471",
            "value": " 1.35k/1.35k [00:01&lt;00:00, 823B/s]"
          }
        },
        "9691c795b5534a09a3db391d13dc6337": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9748513b6c524dd3972b48041b0ac989": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9760bd142c844c6d8c0c7a63ad22c020": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979ee0fcf33b4487821ea80287faeb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97caa06c0b324d3aa829abdc14af6b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9890ecc31a2c4bdaa7c9f2aeb8dcde4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a496f7a3b614863bc493ea2f2fa4bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ac10c77ff6b48978edc68ae9bc33cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fd4d4e6bcb444aeb115b9cec2b5252f",
            "max": 242013376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b41626100c34fc081dd1cefb0fc0211",
            "value": 242013376
          }
        },
        "9cecc5ca989a41c39d9836fd29581459": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "9e10200df4fb47de8d11d919506fd420": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e12cbbe986d498290844708378f93a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c653628f4c4bb8bf2b6bd49c7ae102",
            "placeholder": "​",
            "style": "IPY_MODEL_34aab39b61db49ada48366e68faedc5a",
            "value": " 90.0/90.0 [00:36&lt;00:00, 2.47B/s]"
          }
        },
        "9fd4d4e6bcb444aeb115b9cec2b5252f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fe094b25570406db96cb643f5320b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0ec9c4d11db4a6bb878c189916c16be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9443cce1956346cebcb42bfea6f0429b",
              "IPY_MODEL_fc7b6d0a32104609bfb7a0ae2f6db2d0",
              "IPY_MODEL_1041e11ae58f4846a39e16ddf45b7477"
            ],
            "layout": "IPY_MODEL_458ef3b2aa934cdf9c5367afcf6d5609"
          }
        },
        "a6a5b4cc5935410288f11ef197bdc223": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7fac09af6c24f1ea35a62201c15f19d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8889b2e8071450383475710e240fd81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abb16877e3da40418070a9d454ed3efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd051383b0c64e99ae015006faa88566",
              "IPY_MODEL_be42ae6aee554318bb07fe3a03499062",
              "IPY_MODEL_6e232bf110c848978295f77475938326"
            ],
            "layout": "IPY_MODEL_b7be025a8f6741a6aaed9893d0565452"
          }
        },
        "ad01de75faf54ac595c540342bfbd16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc1a584f4d8a40eca39bd2efed426eec",
              "IPY_MODEL_1dc95b92e9db4eeb82851c679f709f8c"
            ],
            "layout": "IPY_MODEL_b3e04bba6f5348c1b6a0d5c818c09e2a"
          }
        },
        "afb1ace8c86e4106a53dd4f0c26f7ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b08383598a1848118730f57ab4a44215": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1eb7e8892604cf995c8a7749904f166": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b21d9ba0530b4109b7aa9c5cef5de67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2e74f9f05c94ab5a6bfa81292b43da1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e04bba6f5348c1b6a0d5c818c09e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f78454700b44f690716acb5ba9405e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9691c795b5534a09a3db391d13dc6337",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_feaadea50b75486d9605c3f1b063bee2",
            "value": 791656
          }
        },
        "b69b3fcb5e4d48b192f6aa7f45f01889": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ac10c77ff6b48978edc68ae9bc33cb9",
              "IPY_MODEL_faf5ec950167410f8dc89363f0c3c627"
            ],
            "layout": "IPY_MODEL_b08383598a1848118730f57ab4a44215"
          }
        },
        "b7be025a8f6741a6aaed9893d0565452": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b860bd97046a4f5b93a5550a5b9a61b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd051383b0c64e99ae015006faa88566": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed102c7f999a4927bd833d637b2ea867",
            "placeholder": "​",
            "style": "IPY_MODEL_9a496f7a3b614863bc493ea2f2fa4bb3",
            "value": "Downloading: 100%"
          }
        },
        "bd34099fedfa436c9567678aebd8e1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bde77fab7cfd4d399f79add1b210a0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be42ae6aee554318bb07fe3a03499062": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_965033b332ee4c48bd233b2e64da3f78",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d3123861517461883dad2f0b1df8610",
            "value": 791656
          }
        },
        "c01125ec848344959c88f7e090723d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "c035151659df42a581cfc9bed5f59b43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c23e78ce9eed4d2f8bd7453fb76f1813": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "c38d04838c9c4c47b9b8d90e7297a931": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c8d5facdcf48c4994bc1e7954bd57f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c469ebd9fe344f92938478390737c523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48be6368fc848caa740cdf52fe0d39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3c8d5facdcf48c4994bc1e7954bd57f",
            "max": 2950910673,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8829e41bbca54ad28066cbf3e8d0bcd9",
            "value": 2950910673
          }
        },
        "c75db10073d6493aa9834c789537dc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca0feacb455b4d00a9db55caa335f7e9",
              "IPY_MODEL_11da8c1e1d024b6e8cbb6de0efa47380"
            ],
            "layout": "IPY_MODEL_f07cb8f47ccb41728ed60b9ba64ddee8"
          }
        },
        "c800ebd19e4141c3952a5e64061710a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0feacb455b4d00a9db55caa335f7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19fdf701402e4eae9925288271e9b451",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bff9fcea04d458dad3de823aa714873",
            "value": 791656
          }
        },
        "cc32deac3449452183e2e275907e5c03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce175483b07742398ee572bd32346000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055e538c3245473bb9ccc507492ae084",
            "max": 891612585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4039b52964244324b7ad4dcb72f766dc",
            "value": 891612585
          }
        },
        "cf72d967d81444f9b93a1cbe2f5e4eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8df06c887e5f4629b5392c068dda1204",
              "IPY_MODEL_48a87f26c0e145c79bcacb7a66f2e03c"
            ],
            "layout": "IPY_MODEL_f05b8fb4c7764f9caff4eba3d866c5e8"
          }
        },
        "d180068e0b894d7bb0cf2b86772c39bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b088bfd7c84df4bfe125d64152da2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9760bd142c844c6d8c0c7a63ad22c020",
            "placeholder": "​",
            "style": "IPY_MODEL_7180636e594e4c69ad73b22cbba633c0",
            "value": " 65.0/65.0 [00:38&lt;00:00, 1.71B/s]"
          }
        },
        "d440bbd4fe414f568d3d5136279a6b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d456038ce21944bb84e592fba181484e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d46d560becfc4da281f51e67a12b29b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47a34125e0445d69e2e8b42709e3be7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d66bc4ef74eb411ba1915bcde1c3bfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d72095e7c6ff4ffea4c3ae56bd17b320": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_254566af90754f97972ee1c26259ac16",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e0def5a00e45799ba61088654dcac5",
            "value": 3
          }
        },
        "d8178120b6e04b43b17163b3f35b8c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d9840f3af0a94581824ce3459773fcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbbd825fd25949408c836238b0b8119e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc1a584f4d8a40eca39bd2efed426eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f6ee3c487c405c9ea9e0bfadf31906",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ba3a07444d049ef99947ab336b20c07",
            "value": 31
          }
        },
        "dcdd610bc53549109eff5ee720030702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de987e3495914bd3bf05fca5f0411595": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0342c84e1404c93972fdb8cad71b363": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e34ea489ea824b81bc5bbaf07aec79ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c12a663bbd342979337e2e2ba029219",
            "max": 891608946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2abf9fbf4e2f4d87bb73fc4eb2bebdc9",
            "value": 891608946
          }
        },
        "e3b79fc7017c4135afae4c47b8e5e7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a1fcedd1d4f4f0099ec72c1acb30419",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dadb55aade34302b54d6a4e5b82207a",
            "value": 124
          }
        },
        "e770c99833df4c838ba632cb5af35920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74075048891040a2b7da864cef5a3f23",
            "max": 1786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_656c13ab65bf40f691c14d3d7c9dd6d6",
            "value": 1786
          }
        },
        "e77f7a984b4e4814af128964fc509eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6302a8d0355d4f678ba15a6e831fa367",
            "placeholder": "​",
            "style": "IPY_MODEL_22818ab4c92b42d581825afa9b544a17",
            "value": " 1.79k/1.79k [00:00&lt;00:00, 47.0kB/s]"
          }
        },
        "e87a4d67ef534786b8be863eb55a8240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_447f364655974be8b923f26261fa4635",
            "placeholder": "​",
            "style": "IPY_MODEL_12412cb52fa844e1aec2799b6df43f52",
            "value": " 792k/792k [00:20&lt;00:00, 39.4kB/s]"
          }
        },
        "eb0f17ee32b04ba3844f609066bf7484": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9fb93a24f242f987e1fd6993c717e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_224671f61e8a473a87070572a74d0081",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8178120b6e04b43b17163b3f35b8c2a",
            "value": 1350
          }
        },
        "ec840a9a88d647eda8090fba3768d36a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed102c7f999a4927bd833d637b2ea867": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00acf8a444844ab871c326c9556aea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f05b8fb4c7764f9caff4eba3d866c5e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07cb8f47ccb41728ed60b9ba64ddee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10552ead0394059a49b553a6d2ffbc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd48fb6d8ff6497c893e6b988746cec6",
            "placeholder": "​",
            "style": "IPY_MODEL_3dfad120fef440b9aed00ee86df227fa",
            "value": " 31.0/31.0 [00:01&lt;00:00, 19.5B/s]"
          }
        },
        "f5420a7a84324b6aab69555cf2c4c28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47a34125e0445d69e2e8b42709e3be7",
            "max": 1348,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c23e78ce9eed4d2f8bd7453fb76f1813",
            "value": 1348
          }
        },
        "f56e1ca817f14b7ab8a2f40b2826f930": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b5c6f268f14f17af6666fa6b03fea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f93c5540b7cd4730b1a1d044aa954f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_276fc7a3ed994b44a78b1fe682384ef1",
              "IPY_MODEL_22bfa2b6304f41b485d4f60a33beec8a",
              "IPY_MODEL_e77f7a984b4e4814af128964fc509eef"
            ],
            "layout": "IPY_MODEL_010205a92d6444178afb7027270cd583"
          }
        },
        "faf5ec950167410f8dc89363f0c3c627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c469ebd9fe344f92938478390737c523",
            "placeholder": "​",
            "style": "IPY_MODEL_e0342c84e1404c93972fdb8cad71b363",
            "value": " 242M/242M [00:09&lt;00:00, 24.4MB/s]"
          }
        },
        "fbbeaf73ff094cc29a62694fd00289da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "fbd78e57aab1415087eec7b2fa4202fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf3c7defad6444581ee24eed49fef0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc7b6d0a32104609bfb7a0ae2f6db2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d456038ce21944bb84e592fba181484e",
            "max": 2145,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_944b0ebb16b248ef8720ef22a807b130",
            "value": 2145
          }
        },
        "fcf7a63396ae408b8e2570aaa69a7ef1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd48fb6d8ff6497c893e6b988746cec6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feaadea50b75486d9605c3f1b063bee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "fefa681970db4c259c2e1aeb93abd6d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe99f3b2a454586966c1cd0e96f0e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c451f3095b243f797b32e0700d11e10",
            "placeholder": "​",
            "style": "IPY_MODEL_43a0a349583a407e86a2252685f4aeee",
            "value": " 2.95G/2.95G [01:05&lt;00:00, 61.8MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
