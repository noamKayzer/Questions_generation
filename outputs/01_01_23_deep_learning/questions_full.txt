(0)TAKEN?True RQUGE:5.1397
Q1:what has been widely adopted by the computervision community?
Q2:What was out of favor at the time of ConvNet's success?
Best ans: neural networks
['A0: During the period when neural networks were out of favour, Convolutional neural network (ConvNet) achieved many successes during the period. The computervision community has recently been widely adopted by the ConvNet. The final answer: Convolutionally neural network.', 'A1: A convnet is a neural network. Convolutional networks were out of favour during the period when neural networks were in favour. Convnet has recently been widely adopted by the computervision community. The final answer: ConvNet.', 'A2: The Convolutional Neural Network (ConvNet) achieved many successes during the period when neural networks were out of favour. The ConvNet has recently been widely adopted by the computervision community. The final answer: Convolution. Neural network.', 'A3: Computer vision is the study and analysis of visual images. Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. ConvNet has recently been widely adopted by the computervision community. The final answer: Convolution.', 'A4: convnet', 'A5: CNN', 'A6: the computervision community', 'A7: neural networks']
Text:Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community.



(1)TAKEN?True RQUGE:5.0816
Q1:A multilayer Neural Network is a type of which kind of machine learned system?
Q2:What type of network generates the captions in the figure?
Best ans: Recurrent neural network
['A0: neural network', 'A1: Language model', 'A2: Recurrent neural network', 'A3: recurrent neural network', 'A4: network', 'A5: deep', 'A6: deep learning', 'A7: language model']
Text: Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a recurrent neural network taking, as extra input, the representation extracted by a deep convolution neural network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(2)TAKEN?True RQUGE:5.0814
Q1:In what year was interest in deep feedforward networks revived?
Q2:When was interest in deep feedforward networks revived?
Best ans: 2006
['A0: Research group brought together by the Canadian Institute for Advanced Research revived interest in deep feedforward networks around 2006', 'A1: 2006. Interest in deep feedforward networks was revived around 2006 (refs 31–34)', 'A2: Around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research. The answer: around 2006.', 'A3: 2006 was the year that interest in deep feedforward networks was revived. So, the final answer is 2006.', 'A4: ', 'A5: 2006', 'A6: ', 'A7: ']
Text:Multilayer networks can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(3)TAKEN?True RQUGE:5.0379
Q1:Why do we need to compute the error derivative?
Q2:Why does each module in the stack transform its input?
Best ans: to increase both the selectivity and the invariance of the representation.
['A0: Raw pixels could not possibly distinguish Samoyeds from white dogs.', 'A1: Raw pixels could not possibly distinguish Samoyeds from white dogs', 'A2: The error derivative is a function of the weight wjk on the connection from unit j in the layer below.', 'A3: We need to compute the error derivative with respect to the output of each unit.', 'A4: to increase both the selectivity and the invariance of the representation.', 'A5: with multiple non-linear layers', 'A6: each hidden layer we compute the error derivative with respect to the output of each unit.', 'A7: to increase both the selectivity and the invariance of the representation']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(4)TAKEN?True RQUGE:4.959
Q1:How does deep learning differ from conventional machine learning techniques?
Q2:What is the key aspect of deep learning?
Best ans: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.
['A0: A deep learning system is designed to learn from data using general purpose learning procedures. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data.', 'A1: This is a list of machine-learning applications. Deep learning is based on the idea that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A2: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A3: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A4: they are learned from data using a general-purpose learning procedure.', 'A5: requires very little engineering by hand', 'A6: required careful engineering and considerable domain expertise to design a feature extractor', 'A7: they are learned from data using a general-purpose learning procedure']
Text: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(5)TAKEN?False RQUGE:4.959
Q1:What is the key aspect of deep learning?
Q2:What is the key aspect of deep learning?
Best ans: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.
['A0: Human engineers are not human. Deep learning is a class of techniques called deep neural networks. Deep neural networks are learned from data using a general-purpose learning procedure to discover intricate structures in high-dimensional data.', 'A1: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A2: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A3: Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A4: not designed', 'A5: no engineering by hand', 'A6: requires very little engineering by hand', 'A7: layers of features are not designed by human engineers']
Text: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(6)TAKEN?True RQUGE:4.9272
Q1:What is the name of the method that most practitioners use to adjust the weights?
Q2:What is the procedure called that most practitioners use to adjust the weights?
Best ans: Stochastic gradient descent (SGD) is a method that most practitioners use to adjust the weights.
['A0: stochastic gradient descent SGD is a method that most practitioners use to adjust the weights.', 'A1: In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors and computing the average gradient for those examples, and adjusting the weights accordingly.', 'A2: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. SGD is a method that most practitioners use to adjust the weights. SDD is based on the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A3: Stochastic gradient descent (SGD) is a method that most practitioners use to adjust the weights.', 'A4: algorithmic', 'A5: algorithm', 'A6: SGD', 'A7: neural network']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(7)TAKEN?False RQUGE:4.903
Q1:Who discovered multilayer networks?
Q2:Who discovered the idea that gradient descent could be done?
Best ans: several different groups during the 1970s and 1980s.
['A0: the Canadian Institute for Advanced Research', 'A1: several different groups during the 1970s and 1980s.', 'A2: A group of researchers brought together by the Canadian Institute for Advanced Research.', 'A3: A group of researchers brought together by the Canadian Institute for Advanced Research', 'A4: multiple', 'A5: different', 'A6: various', 'A7: various groups']
Text:Multilayer networks can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition comunities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(8)TAKEN?True RQUGE:4.8722
Q1:Why are Recurrent Neural Networks better than conventional Recurrent Neural Networks 
Q2:What are Recurrent Neural Networks better than conventional Recurrent Neural Networks?
Best ans: Recurrent neural networks are better than conventional Recurrent Neural Networks for tasks that involve sequential inputs, such as speech and language.
['A0: Recurrent neural networks are better than conventional Recurrent Neural Networks for tasks that involve sequential inputs, such as speech and language.', 'A1: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks than conventional Recurrent neural networks', 'A2: LSTM Networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76, and memory networks.', 'A3: LSTM Networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76, and memory networks', 'A4: memory network has yielded excellent', 'A5: memory', 'A6: memory network', 'A7: long short term memory networks']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(9)TAKEN?True RQUGE:4.8669
Q1:How many layers of ReLUs are in a ConvNet architecture?
Q2:How many layers of ReLUs are in a ConvNet architecture?
Best ans: 10 to 20 layers of ReLUs are in a ConvNet architectures.
['A0: The number of ReLU layers in a ConvNet architecture is 10 to 20 layers.', 'A1: 10 to 20 layers of ReLUs are in a ConvNet architectures.', 'A2: Recently, ConvNet architectures have 10 to 20 layers of ReLUs.', 'A3: ConvNet architectures have 10 to 20 layers of ReLUs.', 'A4: 10-to-20', 'A5: Ten', 'A6: recent', 'A7: 10 to 20 layers']
Text: ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(10)TAKEN?True RQUGE:4.8294
Q1:Increasingly, these applications make use what?
Q2:What do machine-learning systems increasingly use?
Best ans: Class of techniques called deep learning.
['A0: deep neural networks', 'A1: Class of techniques called deep learning.', 'A2: Increasingly, these applications make use of a class of technologies called deep learning.', 'A3: A class of techniques called deep learning.', 'A4: classes', 'A5: an', 'A6: Deep Learning', 'A7: deep learning.']
Text: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(11)TAKEN?False RQUGE:4.7925
Q1:When was interest in deep feedforward networks revived?
Q2:When was interest in deep feedforward networks revived?
Best ans: Around 2006 (refs 31–34)
['A0: around 2006.', 'A1: Around 2006', 'A2: 2006 was the year that deep feedforward networks were revived.', 'A3: Around 2006 (refs 31–34)', 'A4: near', 'A5: near 2007', 'A6: 2006.', 'A7: 2006']
Text:Multilayer networks can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(12)TAKEN?True RQUGE:4.73
Q1:Using distributed representations, what are some of the benefits that deep net have over classic learning algorithms?
Q2:What are the word vectors in a language model composed of?
Best ans: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network
['A0: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network71.', 'A1: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network', 'A2: A multilayer neural network learns to represent the network’s inputs in a way that makes it easy to predict the target outputs.', 'A3: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word.', 'A4: speed', 'A5: make it easy to predict', 'A6: powerful composition', 'A7: the power of composition']
Text: Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a recurrent neural network taking, as extra input, the representation extracted by a deep convolution neural network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(13)TAKEN?False RQUGE:4.5783
Q1:Which of these features is used to predict the target outputs of a multilayer neural network?
Q2:What are word vectors composed of in a language model?
Best ans: Word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.
['A0: Learning to represent inputs in this way makes it easy for the network to predict the target output of the network.', 'A1: Hidden layers of a multilayer neural network learn to represent network’s input word vector in a way that makes it easy to predict the target outputs.', 'A2: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vector were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A3: Word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A4: activity', 'A5: active components', 'A6: big activity vectors', 'A7: learned features']
Text: Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a recurrent neural network taking, as extra input, the representation extracted by a deep convolution neural network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(14)TAKEN?True RQUGE:4.51
Q1:In what technology are ConvNets being used?
Q2:What applications will images labelled at the pixel level have?
Best ans: autonomous mobile robots and self-driving cars
['A0: autonomous mobile robots and self-driving cars.', 'A1: autonomous mobile robots and self-driving cars', 'A2: Autonomous mobile robots and self-driving cars are applications of ConvNets. Therefore, the final answer is autonomous mobile robot.', 'A3: Autonomous mobile robots and self-driving cars are applications of ConvNets.', 'A4: vision', 'A5: cameras', 'A6: robots', 'A7: cars']
Text: ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(15)TAKEN?True RQUGE:4.4746
Q1:Who was first introduced to train Recurrent Neural Networks?
Q2:What was the first way to train RNNs?
Best ans: Recurrent neural networks (RNNs) were first introduced to train by Backpropagation.
['A0: A method of training RNNs was first introduced to train recurrent neural networks (RNNs) Backpropagation was first introducer', 'A1: RNNs were first introduced to train by Backpropagation.', 'A2: backPropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78.', 'A3: Recurrent neural networks (RNNs) were first introduced to train by Backpropagation.', 'A4: RNNs', 'A5: Recurrent neural', 'A6: B', 'A7: LSTM networks']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(16)TAKEN?True RQUGE:4.4658
Q1:The ConvNet was used in what field?
Q2:What community has recently adopted the Convolutional neural network?
Best ans: Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community. The final answer: computervision
['A0: Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community. The final answer: computervision', 'A1: Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community. The final answer: computervision.', 'A2: The ConvNet has recently been widely adopted by the computervision community.', 'A3: computervision is the field of computer animation. Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. Therefore, the final answer is computer animation field.', 'A4: computing', 'A5: human', 'A6: machine learning', 'A7: artificial intelligence']
Text:Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community.



(17)TAKEN?True RQUGE:4.4358
Q1:Which of these is not a serious issue in general?
Q2:Which of these is not a serious issue in general?
Best ans: Local minima are not a serious issue in general in general.
['A0: local minimums', 'A1: Many applications of deep learning use feedforward neural network architecture (Fig) 1', 'A2: Local minima are not a serious issue in general in general.', 'A3: Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general.', 'A4: Local minimums', 'A5: locally minimal', 'A6: Local minimum', 'A7: hidden layers']
Text:Multilayer networks can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(18)TAKEN?False RQUGE:4.1924
Q1:Machine-learning systems are used to identify objects in images, what?
Q2:What are some of the applications of machine-learning?
Best ans: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search using a class of techniques called deep learning.
['A0: Transcribe speech into text, match news items, posts or products with users’ interests. Deep learning is making major advances in solving problems that have resisted the best attempts. of the artificial intelligence community for many years.', 'A1: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search using a class of techniques called deep learning.', 'A2: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search', 'A3: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A4: search', 'A5: text', 'A6: speech into text', 'A7: transscribe speech into text']
Text: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(19)TAKEN?True RQUGE:4.1334
Q1:In what domains can deep learning be used?
Q2:What areas have deep learning dramatically improved?
Best ans: speech recognition visual object recognition object detection and many other domains such as drug discovery and genomics
['A0: The state-of-the-art in speech recognition, visual object recognition, object detection, drug discovery and genomics.', 'A1: speech recognition visual object recognition object detection and many other domains such as drug discovery and genomics', 'A2: These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.', 'A3: Drug discovery and genomics are two domains that have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains.', 'A4: drugs', 'A5: vision', 'A6: voice detection', 'A7: visual object recognition']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(20)TAKEN?True RQUGE:4.0218
Q1:To properly adjust the weight vector, what is the learning algorithm computed by?
Q2:What does the learning algorithm compute to properly adjust the weight vector?
Best ans: a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.
['A0: a gradient vector', 'A1: Gradient Vector that, for each weight, indicates by what amount the error would increase or decrease If a weight were increased by a tiny amount.', 'A2: a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A3: To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A4: gradient', 'A5: vector', 'A6: Gradient vector', 'A7: for each weight']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(21)TAKEN?True RQUGE:3.9979
Q1:What is the main idea of this passage?
Q2:What do many applications of deep learning use?
Best ans: Deep learning uses feedforward neural networks to map a fixed-size input (for example, an image) to a fix size output.
['A0: Neural nets and backpropagation were largely forsaken and ignored by the machine-learning community.', 'A1: Deep learning uses feedforward neural networks to map a fixed-size input (for example, an image) to a fix size output.', 'A2: Multilayer networks can be trained by simple stochastic gradient descent', 'A3: Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fix size output.', 'A4: local minimums', 'A5: deep neural networks and backpropagation', 'A6: deep neural networks', 'A7: Deep learning uses feedforward neural networks']
Text:Multilayer networks can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(22)TAKEN?True RQUGE:3.9428
Q1:Why does deep learning have two different exponential advantages over classic learning algorithms that do not use distributed representations21?
Q2:Why does deep learning have two different exponential advantages over classic learning algorithms that do not use distributed representations21?
Best ans: Neural networks have two different exponential advantages over classic learning algorithms that do not use distributed representation21. Both of these advantages arise from power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.
['A0: Using distributed representations allows neural networks to learn to predict the target outputs.', 'A1: Neural networks have two different exponential advantages over classic learning algorithms that do not use distributed representation21. Both of these advantages arise from power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.', 'A2: A multilayer neural network learns to represent the network’s inputs in a way that makes it easy to predict the target outputs.', 'A3: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word.', 'A4: Both', 'A5: both', 'A6: arise from the power of composition', 'A7: power of composition']
Text: Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a recurrent neural network taking, as extra input, the representation extracted by a deep convolution neural network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(23)TAKEN?True RQUGE:3.8154
Q1:When was ConvNet out of favour?
Q2:When was the ConvNet first adopted?
Best ans: until recently
['A0: During the period when Neural networks were out of favour, ConvNet was widely adopted. The computervision community has recently adopted Convnet. Therefore, the final answer is during the period.', 'A1: Neural networks were out of favour during the 1980s. Convolutional neural network was out of favor during the 1990s. So, the final answer is during the early 1990s to the late 2000s.', 'A2: Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. The answer: during the time when neural network were out in favour.', 'A3: The period when ConvNet was out of favour was during the 1980s. The 1980s saw the development of many new neural networks. The 1990s saw many newer neural networks being developed. So, the final answer is the 1990s.', 'A4: before', 'A5: in favour', 'A6: until recently', 'A7: neural networks were out of favour']
Text:Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community.



(24)TAKEN?True RQUGE:3.7981
Q1:How are reluc functions used in neural networks?
Q2:What is computed at each hidden layer?
Best ans: f(z) = max(0,z) is applied to z. At each hidden layer we compute the error derivative with respect to the output of each unit.
['A0: f(z) = max(0,z) is applied to z.', 'A1: f(z) = max(0,z) is applied to z. At each hidden layer we compute the error derivative with respect to the output of each unit.', 'A2: To get the weight wjk on the connection from unit j in the layer below is just yj E/zk.', 'A3: The rectified linear unit (ReLU) f(z) = max(0,z) is commonly used in recent years.', 'A4: commonly used in recent years.', 'A5: used in recent years', 'A6: To get the output of each unit', 'A7: commonly used in recent years']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(25)TAKEN?False RQUGE:3.7901
Q1:Which of these is not a type of machine learning?
Q2:Which of these is not a type of machine learning?
Best ans: human engineering
['A0: human engineering.', 'A1: human engineering', 'A2: deep neural networks', 'A3: Human engineers are not able to design features. Deep learning is able.', 'A4: Human engineering', 'A5: humans', 'A6: deep neural networks', 'A7: domain expertise']
Text: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(26)TAKEN?False RQUGE:3.6718
Q1:How many layers are in a typical ConvNet?
Q2:What are the first few stages of a ConvNet composed of?
Best ans: Two types of layers: convolutional layers and pooling layers.
['A0: First few stages are composed of two types of layers: convolutional layers and pooling layers.', 'A1: There are two types of layers in a typical ConvNet: convolutional layers and pooling layers.', 'A2: Two types of layers: convolutional layers and pooling layers.', 'A3: A typical ConvNet is structured as a series of stages composed of two types of layers: convolutional layers and pooling layers.', 'A4: Two kinds', 'A5: Two', 'A6: stages', 'A7: a series of stages']
Text: ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit   Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(27)TAKEN?False RQUGE:3.4667
Q1:What are some applications of ConvNets?
Q2:What are some applications of ConvNets?
Best ans: The detection, segmentation and recognition of objects and regions in images since the early 2000s have been applied with great success to face recognition. Other applications gaining importance involve natural language understanding14 and speech recognition7.
['A0: The detection, segmentation and recognition of objects and regions in images since the early 2000s have been applied with great success to face recognition. Other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A1: Face recognition is a major recent practical success of ConvNets. Other applications gaining importance involve natural language understanding14 and speech recognition7 Convnets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67.', 'A2: Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision- tems for cars Other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A3: Images can be labeled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A4: speech', 'A5: speech recognition', 'A6: facial detection', 'A7: natural language understanding14']
Text: ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(28)TAKEN?False RQUGE:3.4155
Q1:Where is a filter bank located?
Q2:Where are the local patches of a ConvNet located?
Best ans: in feature maps of the previous layer
['A0: feature maps', 'A1: unit is connected to each other through a set weights called filter bank.', 'A2: unit is connected to each other through a set weights', 'A3: The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit', 'A4: the previous layer', 'A5: in feature maps', 'A6: in feature maps of the previous layer', 'A7: feature maps']
Text: ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit   Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(29)TAKEN?False RQUGE:3.1791
Q1:Which of these networks is used for machine translation?
Q2:What are LSTM networks used for
Best ans: LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76.
['A0: Recurrent Neural Networks', 'A1: Long Short-term Memory networks', 'A2: Long Short-term Memory networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17', 'A3: LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76.', 'A4: RNNs', 'A5: encoder', 'A6: Deep ConvNet', 'A7: LSTM networks']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(30)TAKEN?False RQUGE:3.0456
Q1:In what field has convolutional neural network been widely adopted?
Q2:What community has recently adopted ConvNet?
Best ans: Artificial intelligence (AI) is the study of how computers learn. Convolutional neural network (ConvNet) has recently been widely adopted by the Computervision community. The final answer: computervision.
['A0: In the computervision community, convolutional neural network has recently been widely adopted. The computervision is a field that uses computer vision. The final answer: by the computer vision community.', 'A1: Artificial intelligence (AI) is the study of how computers learn. Convolutional neural network (ConvNet) has recently been widely adopted by the Computervision community. The final answer: computervision.', 'A2: Convolutional neural network (ConvNet) has recently been widely adopted by the computervision community. The computervision is a field that uses computer vision to analyze and interpret images. The final answer: computer vision.', 'A3: ', 'A4: video', 'A5: computing', 'A6: artificial intelligence', 'A7: machine learning']
Text:Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community.



(31)TAKEN?False RQUGE:2.9722
Q1:The architecture of a typical ConvNet is structured as what?
Q2:What is the architecture of a typical ConvNet?
Best ans: Series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers.
['A0: stages', 'A1: Series of stages', 'A2: Series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers.', 'A3: a series of stages', 'A4: sequences', 'A5: stage', 'A6: series of stages.', 'A7: stages']
Text: ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit   Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(32)TAKEN?False RQUGE:2.9404
Q1:What is the advantage of using Long Short-term Memory networks?
Q2:What have Long Short-term Memory networks proved to be more effective than conventional RNNs?
Best ans: Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.
['A0: Memory networks have yielded excellent performance on standard question-answering benchmarks.', 'A1: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.', 'A2: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns. This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art.', 'A3: Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.', 'A4: memory networks', 'A5: several layers for each time step', 'A6: have several layers for each time step87', 'A7: when they have several layers for each time step87']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(33)TAKEN?False RQUGE:2.9132
Q1:What is the name of the method that was used to train ConvNet?
Q2:What is the final answer to the question "How do you train a network?"
Best ans: Neural networks are used to train computer vision systems. Convolutional neural networks are the most popular method used to teach computer vision. ConvNet is a method used in computer vision to train a network. Therefore, the final answer is Convolution.
['A0: Computer vision involves using computers and algorithms to process visual data. Convolutional neural networks are used to train computer vision. ConvNet is the name for the method used to teach Convnet. Therefore, the final answer is Convolution.', 'A1: Neural networks are used to train computer vision systems. Convolutional neural networks are the most popular method used to teach computer vision. ConvNet is a method used in computer vision to train a network. Therefore, the final answer is Convolution. Neural Networks.', 'A2: Neural networks are used to train computer vision systems. Convolutional neural networks are the most popular method used to teach computer vision. ConvNet is a method used in computer vision to train a network. Therefore, the final answer is Convolution.', 'A3: Convolutional neural network (ConvNet) is a method of training a neural network. Convolutions are made of a set of data points. The data points are arranged in a way that makes it impossible for the network to learn new ones. Therefore, the final answer is Convolution. ConvNet.', 'A4: generative', 'A5: method', 'A6: CNN', 'A7: deep learning']
Text:Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community.



(34)TAKEN?False RQUGE:2.8028
Q1:The following are examples for the use of ConvNets:
Q2:What are other applications gaining importance?
Best ans: Natural language understanding14 and speech recognition7 are applications gaining importance. ConvNets have been applied with great success to detection, segmentation and classification of objects and regions in images since the early 2000s. Other applications obtaining importance involve natural language understanding and speech.
['A0: Natural language understanding14 and speech recognition7 are applications gaining importance. ConvNets have been applied with great success to detection, segmentation and classification of objects and regions in images since the early 2000s. Other applications obtaining importance involve natural language understanding and speech.', 'A1: Natural language understanding14 and speech recognition7 are applications gaining importance. ConvNets have been applied with great success to detection, segmentation and classification of objects and regions in images since the early 2000s.', 'A2: Mobileye and NVIDIA are using ConvNet-based methods in their autonomous mobile robots and self-driving cars. Other applications gaining importance involve natural language understanding14 and speech recognition7', 'A3: A major recent practical success of face recognition is face recognition. Other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A4: voice', 'A5: speech', 'A6: Face Recognition', 'A7: facial expression recognition']
Text: ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(35)TAKEN?False RQUGE:2.2023
Q1:In which layer do we first compute total input to each unit?
Q2:Where are the units located?
Best ans: Layer Below
['A0: z', 'A1: Layer Below', 'A2: z = max(0,z)', 'A3: layer below the layer of units', 'A4: under', 'A5: hidden', 'A6: above', 'A7: below']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(36)TAKEN?False RQUGE:2.1431
Q1:How can Long Short-term Memory networks be used to train Recurrent Neural Networks 
Q2:What is the difference between a French sentence and an English sentence?
Best ans: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns. Instead, of translating meaning of a French sentence into an English sentence, one can learn ‘translate’ the meaning of an image.
['A0: In order to train RNNs, LSTM networks or related forms of gated units are used for the encoder and decoder networks that perform so well at machine translation17,72,76.', 'A1: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns. Instead, of translating meaning of a French sentence into an English sentence, one can learn ‘translate’ the meaning of an image.', 'A2: LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks', 'A3: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.', 'A4: they can apply backPropagation to train Rnns', 'A5: They have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87,', 'A6: have several layers for each time step87', 'A7: when they have several layers for each time step87']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(37)TAKEN?False RQUGE:1.8188
Q1:Using Rectified Linear Unit  what are some examples that can be used to compute the error derivatives?
Q2:What is the ReLU based on?
Best ans: ReLU f(z) = max(0,z) is a non-linear function used in neural networks. ReLU is based on the Jacobian matrices.
['A0: ReLU f(z) = max(0,z) is a non-linear function used in neural networks. ReLU is based on the Jacobian matrices.', 'A1: ReLU f(z) = max(0,z) is a non-linear function used in neural networks.', 'A2: The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj E/zk. raw pixels could not possibly distinguish the latter two.', 'A3: The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj E/zk.', 'A4: ReLU)', 'A5: ReLU', 'A6: Jacobian matrix', 'A7: f(z) = max(0,z)']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(38)TAKEN?False RQUGE:1.6411
Q1:Convolutional neural network is a type of what?
Q2:What community has recently adopted the Convolutional neural network?
Best ans: computer vision
['A0: Neural networks are computer vision algorithms. Convolutional neural network is a type of computer vision algorithm. Therefore, the final answer is computer vision. Neural network.', 'A1: Computer vision is the study of how computers learn. Neural networks are types of computers that learn. Convolutional neural network is a type of computer vision. Therefore, the final answer is computer vision system.', 'A2: Neural networks are computer vision algorithms. Convolutional neural network is a type of computer vision algorithm. Therefore, the final answer is computer vision.', 'A3: ', 'A4: algorithm', 'A5: computer vision', 'A6: artificial intelligence', 'A7: machine learning']
Text:Convolutional neural network (ConvNet) achieved many successes during the period when neural networks were out of favour. It has recently been widely adopted by the computervision community.



(39)TAKEN?False RQUGE:1.5852
Q1:How do you adjust the weights in a typical deep learning system?
Q2:What is a typical deep learning system?
Best ans: A typical deep learning system uses a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly.
['A0: This is done by showing the input vector for a few examples. To properly adjust a weight, you compute an average gradient for those examples.', 'A1: A typical deep learning system uses a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly.', 'A2: Using a procedure called stochastic gradient descent (SGD), most practitioners show the input vector for a few examples. To properly adjust the weight vector, a learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease with the weight were increased by a tiny amount.', 'A3: To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. To properly adapt the weights, the algorithm computed a negative gradient vector.', 'A4: learning algorithm computes a gradient vector that', 'A5: stochastic gradient descent (SGD)', 'A6: learning algorithm computes a gradient vector', 'A7: computations a gradient vector']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(40)TAKEN?False RQUGE:1.556
Q1:What are the two different exponential advantages of deep nets over classic learning algorithms that do not use distributed representations21?
Q2:What are the two different exponential advantages of deep nets over classic learning algorithms that do not use distributed representations21?
Best ans: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. The hidden layer of recurrent neural network takes, as extra input, the representation extracted by a deep convolution neural network from a test image.
['A0: Using distributed representations allows neural networks to learn to predict the target outputs. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict target output.', 'A1: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by deep nets71, the standard approach to statistical modelling of language did not exploit distributed representations.', 'A2: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. In logical-inspired paradigm, an instance of an instance is something for which the only property is that it is either identical or non-identical to other symbols.', 'A3: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. The hidden layer of recurrent neural network takes, as extra input, the representation extracted by a deep convolution neural network from a test image.', 'A4: both', 'A5: composition', 'A6: from the power of composition', 'A7: the power of composition']
Text: Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a recurrent neural network taking, as extra input, the representation extracted by a deep convolution neural network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(41)TAKEN?False RQUGE:1.5137
Q1:A neural network can be used to learn what kind of information it can learn?
Q2:What is an example of a deep-learning architecture?
Best ans: a neural network can be used to learn to distinguish Samoyeds from white wolves
['A0: Samoyeds from white wolves', 'A1: Differentiating Samoyeds from white wolves is a very difficult task. A neural network can be used to train a machine to do this task.', 'A2: a neural network can be used to learn to distinguish Samoyeds from white wolves.', 'A3: a neural network can be used to learn to distinguish Samoyeds from white wolves', 'A4: distinctions', 'A5: images of dogs', 'A6: images of dogs and cats', 'A7: images']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(42)TAKEN?False RQUGE:1.2731
Q1:The weight of the error derivative is the sum of the weights of the following:
Q2:How are the weights of the layers computed?
Best ans: Each layer has its own weights. The weights are computed in a non-linear fashion.
['A0: Each layer has its own weights. The weights are computed in a non-linear fashion.', 'A1: In a deep learning network, the weights of the error derivative is the sum the weight of the weight vectors.', 'A2: x, y and z can be represented as vectors. The weights of the error derivatives are the sum of the weights for each vector. The error derivative is the sum and weight of the vectors in the weighted sum.', 'A3: The weights of the error derivatives are the sum of the weights for each of the three layers.', 'A4: weights', 'A5: units', 'A6: error derivative', 'A7: layers']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(43)TAKEN?False RQUGE:1.1785
Q1:The author uses Long Short-term Memory networks to train a machine translation system.
Q2:What does this paper present?
Best ans: This paper presents an approach to machine translation that uses Long Short-term Memory networks to train machine translation systems that go all the way from acoustics to the sequence of characters in the transcription.
['A0: In the past, RNNs were used to train a machine translation system. In the present, Rnns are used to learn to translate images.', 'A1: The author uses LSTM networks to train a machine translation system. The author introduce LSTNs to train machine translation systems.', 'A2: Recurrent neural networks (RNNs) are a type of machine translation system. They are used to translate words into a sequence of characters. They have been used for many tasks, such as machine translation.', 'A3: This paper presents an approach to machine translation that uses Long Short-term Memory networks to train machine translation systems that go all the way from acoustics to the sequence of characters in the transcription.', 'A4: This rather naive way', 'A5: Answer: Long Short-term Memory networks', 'A6: the state of the art', 'A7: Inference rules']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(44)TAKEN?False RQUGE:1.1781
Q1:Which of these is not a vector: x, y, or z?
Q2:Which of these is not a vector: x, y, or z?
Best ans: Raw pixels could not possibly distinguish the latter two. x, y and z are vectors.
['A0: raw pixel could not possibly distinguish the latter two.', 'A1: Z is a non-linear function that is applied to the input of the layer below.', 'A2: z is a weighted sum of the output of units in the layer below.', 'A3: Raw pixels could not possibly distinguish the latter two. x, y and z are vectors.', 'A4: ', 'A5: z', 'A6: ', 'A7: ']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(45)TAKEN?False RQUGE:1.0951
Q1:In which task is the encoder better than the decoder for machine translation?
Q2:What is a deep ConvNet?
Best ans: encoder
['A0: Translating the meaning of an image', 'A1: Speech and language modelling', 'A2: In tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks (RNN)', 'A3: LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76.', 'A4: decoder', 'A5: translate', 'A6: encoder', 'A7: image']
Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks  Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(46)TAKEN?False RQUGE:0.9787
Q1:Who designed ConvNets?
Q2:Who is the author of the paper on ConvNets?
Best ans: Leonardo DiCicco
['A0: Leonardo DiCicco', 'A1: Visual neuroscience43', 'A2: Visual neuroscience', 'A3: a deep network that processes data that come in the form of multiple arrays.', 'A4: Monkeys', 'A5: scientists', 'A6: visual neuroscience43', 'A7: Deep neural networks']
Text: ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit   Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



