(0)TAKEN?True RQUGE:5.0814
Q1:In what year was the deep net architecture revived?
Q2:When was interest in deep feedforward networks revived?
Best ans: 2006
['A0: Around 2006', 'A1: 2006', 'A2: Around 2006 (refs 31–34)', 'A3: 2006 was the year deep feedforward networks were revived.', 'A4: ', 'A5: 2006.', 'A6: 2006', 'A7: ']
Text:Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(1)TAKEN?False RQUGE:5.0381
Q1:Which of these networks is used for machine translation?
Q2:What is used for tasks that involve sequential inputs?
Best ans: Recurrent Neural Networks (RNNs) (rnns)
['A0: Recurrent Neural Networks (RNNs) (rnns)', 'A1: RNNs', 'A2: Recurrent Neural Networks (RNNs) (rnns)', 'A3: Long Short-term Memory (LSTM) networks', 'A4: Encoder', 'A5: Rnns', 'A6: Deep convnet', 'A7: Long Short-term Memory (LSTM) networks']
Text:For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks. Backpropagation was first introduced to train RNNs (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(2)TAKEN?False RQUGE:5.0368
Q1:Long Short-term Memory (LSTM) network and memory network are examples for what type of network?
Q2:What is the name of the network that can choose to read from or write to a tape
Best ans: Neural turing machine
['A0: Neural turing machine', 'A1: Networks that perform so well at machine translation include Recurrent Neural Networks (RNNs).', 'A2: RNNs', 'A3: Neural turing machine (ntm) is a type of network that can read from or write to a tape-like memory.', 'A4: Rnn', 'A5: Multilayer', 'A6: Multilayer network', 'A7: Neural turing machine']
Text:For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks. Backpropagation was first introduced to train RNNs (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(3)TAKEN?True RQUGE:5.0185
Q1:In which task is the encoder better than the decoder for machine translation?
Q2:What can one do instead of translating a French sentence into an English sentence?
Best ans: Translating the meaning of an image
['A0: Translating the meaning of an image', 'A1: Translating the meaning of an image. the encoder is a deep convnet that converts pixels into an activity vector in its last hidden layer. the decoder is an Recurrent Neural Network (RNN) similar to the ones used for machine translation and neural language modelling.', 'A2: Long Short-term Memory (LSTM) networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76.', 'A3: Long Short-term Memory (LSTM) networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76', 'A4: Encoder', 'A5: Translate', 'A6: Images', 'A7: Image']
Text:For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks. Backpropagation was first introduced to train RNNs (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(4)TAKEN?True RQUGE:5.0164
Q1:What is the name of the method that most practitioners use to adjust the weights?
Q2:What procedure do most practitioners use?
Best ans: Stochastic Gradient Descent (SGD))
['A0: Stochastic Gradient Descent (SGD) (sgd) is a procedure that most practitioners use to adjust the weights. to properly adjust theweight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A1: In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) (sgd) this consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. the negative gradient vector indicates the direction of steepest descent in this landscape', 'A2: In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) (sgd) this consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly.', 'A3: Stochastic Gradient Descent (SGD) (sgd) is a procedure that most practitioners use to adjust the weights.', 'A4: Stochastic Gradient Descent (SGD))', 'A5: Deep convolutional nets', 'A6: Sgd', 'A7: Statistical gradient descent']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(5)TAKEN?False RQUGE:5.0135
Q1:How does deep learning differ from conventional machine learning techniques?
Q2:What is the key aspect of deep learning?
Best ans: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.
['A0: This is a list of machine-learning applications. deep learning is based on the idea that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A1: Machine-learning systems can process natural data. conventional machine-learning techniques were limited in their ability to process natural. data in their raw form for decades. deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years.', 'A2: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A3: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A4: They are learned from data using a general-purpose learning procedure.', 'A5: Requires very little engineering by hand', 'A6: Required careful engineering and considerable domain expertise to design a feature extractor', 'A7: They are learned from data using a general-purpose learning procedure']
Text:Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(6)TAKEN?False RQUGE:5.0135
Q1:What is the key aspect of deep learning?
Q2:What is the key aspect of deep learning?
Best ans: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.
['A0: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A1: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A2: It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. the key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A3: Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. the key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A4: No engineering by hand', 'A5: Requires very little engineering by hand', 'A6: They are learned from data using a general-purpose learning procedure', 'A7: These layers of features are not designed by human engineers']
Text:Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(7)TAKEN?True RQUGE:4.9825
Q1:In which layer do we compute the error derivative?
Q2:Where do we compute the error derivative with respect to the output of each unit?
Best ans: Each hidden layer
['A0: Layer below.', 'A1: Layer below', 'A2: F(z) = max(0,z)', 'A3: Each hidden layer', 'A4: Hidden layers', 'A5: Above', 'A6: Hidden', 'A7: Below']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the Rectified Linear Unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(8)TAKEN?True RQUGE:4.8958
Q1:Where is a filter bank located?
Q2:Where are units in a ConvNet organized?
Best ans: In feature maps
['A0: Feature maps', 'A1: Unit is connected to local patches in the feature maps of the previous layer through a set of weights', 'A2: In a convnet, each unit is connected to local patches in feature maps of the previous layer through the following set of weights:', 'A3: The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit (ReLU).', 'A4: The previous layer', 'A5: In feature maps', 'A6: In feature maps of the previous layer', 'A7: Feature maps']
Text:ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit.  Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(9)TAKEN?True RQUGE:4.8925
Q1:How many layers of relus are in a convnet architecture?
Q2:How many layers of relus are in a convnet architecture?
Best ans: 10 to 20 layers of relus are in a convnet architecture.
['A0: The architectures of recent convnets have 10 to 20 layers of relus.', 'A1: Recently, convnet architectures have 10 to 20 layers of relus, hundreds of millions of weights, and billions of connections between units.', 'A2: 10 to 20 layers of relus are in a convnet architecture.', 'A3: Convnet architectures have 10 to 20 layers of relus, hundreds of millions of weights, and billions of connections between units.', 'A4: 10-to-20', 'A5: Ten', 'A6: 10-to-20 layer', 'A7: 10 to 20']
Text:ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(10)TAKEN?True RQUGE:4.8294
Q1:Increasingly, these applications make use what?
Q2:What do machine-learning systems increasingly use?
Best ans: Class of techniques called deep learning.
['A0: Deep neural networks', 'A1: Class of techniques called deep learning.', 'A2: Increasingly, these applications make use of a class of techniques called deep learning.', 'A3: A class of techniques called deep learning.', 'A4: Depth', 'A5: Deep learning.', 'A6: Deep learning', 'A7: ']
Text:Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(11)TAKEN?False RQUGE:4.7925
Q1:When was interest in deep feedforward networks renewed?
Q2:When was interest in deep feedforward networks revived?
Best ans: Around 2006 (refs 31–34)
['A0: 2006', 'A1: Around 2006', 'A2: 2006 was the year that deep feedforward networks were revived.', 'A3: Around 2006 (refs 31–34)', 'A4: Near', 'A5: About 2006', 'A6: 2006.', 'A7: 2006']
Text:Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(12)TAKEN?True RQUGE:4.73
Q1:In what way do neural networks have two different exponential advantages?
Q2:What are the word vectors in a language model composed of?
Best ans: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network
['A0: Neural networks have two different exponential advantages over classic learning algorithms that do not use distributed representations21.', 'A1: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network in a language model.', 'A2: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network', 'A3: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word.', 'A4: Distributed representations', 'A5: Use distributed representations', 'A6: From the power of composition', 'A7: The power of composition']
Text:Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a Recurrent Neural Network taking, as extra input, the representation extracted by a deep Convolution Neural Network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(13)TAKEN?False RQUGE:4.716
Q1:What are the two types of layers in a typical convnet?
Q2:What are the convolutional and pooling layers inspired by
Best ans: Convolutional and pooling layers are inspired by the classic notions of simple cells and complex cells in visual neuroscience43
['A0: Convolutional and pooling layers are inspired by the classic notions of simple cells and complex cells in visual neuroscience43', 'A1: In a typical convnet, the first few stages are composed of two types of layers: convolutional layers and pooling layers.', 'A2: A typical convnet is structured as a series of stages. the first few stages are composed of two types of layers: convolutional layers and pooling layers.', 'A3: Convolutional layers and pooling layers are inspired by the classic notions of simple cells and complex cells in visual neuroscience43', 'A4: Swimming', 'A5: Swimming layers', 'A6: Pools', 'A7: Pools layers']
Text:ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit.  Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(14)TAKEN?False RQUGE:4.6555
Q1:A neural network is used to model what?
Q2:What model does the other layers of the network learn to convert the input word vectors into an
Best ans: Language
['A0: Language', 'A1: Language model', 'A2: Language models are used to model language.', 'A3: A neural network is used to model a language', 'A4: Text', 'A5: Languages', 'A6: Language models', 'A7: Language']
Text:Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a Recurrent Neural Network taking, as extra input, the representation extracted by a deep Convolution Neural Network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(15)TAKEN?False RQUGE:4.5783
Q1:Which of these features is used to predict the target outputs of a multilayer neural network?
Q2:What are word vectors composed of in a language model?
Best ans: Word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.
['A0: Activate components each have a value 1 and the rest are 0 in the network. word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A1: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. these word vector were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A2: These feature vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A3: Word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A4: Weight matrices', 'A5: Active components', 'A6: Big activity vectors', 'A7: Learned features']
Text:Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a Recurrent Neural Network taking, as extra input, the representation extracted by a deep Convolution Neural Network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(16)TAKEN?False RQUGE:4.5163
Q1:At each hidden layer, we compute what?
Q2:What do we compute at each hidden layer?
Best ans: Error derivative with respect to the output of each unit
['A0: Error derivative', 'A1: Error derivative with respect to the output of each unit', 'A2: The error derivative with respect to the output of each unit.', 'A3: The error derivative with respect to the output of each unit', 'A4: The total input', 'A5: Total input z', 'A6: The error derivative', 'A7: Error derivative']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the Rectified Linear Unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(17)TAKEN?True RQUGE:4.5116
Q1:Which of these methods has dramatically improved the state-of-the-art in speech recognition?
Q2:What does deep learning allow?
Best ans: Recurrent nets have shone light on sequential data such as text and speech using deep learning. deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.
['A0: Recurrent nets have shone light on sequential data such as text and speech using deep learning.', 'A1: Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets has shone light on sequential data such as text and speech.', 'A2: The state-of-the-art in speech recognition is greatly improved by deep learning. deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.', 'A3: Recurrent nets have shone light on sequential data such as text and speech using deep learning. deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.', 'A4: Convolutional', 'A5: Deeper', 'A6: Deeper nets', 'A7: Deep learning']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(18)TAKEN?True RQUGE:4.4769
Q1:Which of these is not a serious issue in general in general?
Q2:Which of these is not a serious issue in general in general?
Best ans: Local minima are not a serious issue in general in general.
['A0: Local minimums', 'A1: Local minimums in deep networks', 'A2: Local minima are not a serious issue in general in general.', 'A3: Nearly all of these saddle points have very similar values of the objective function. hence, it does not much matter which of the saddle points the algorithm gets stuck at.', 'A4: Local minimums', 'A5: Local minimum', 'A6: Locally minimal', 'A7: Hidden layers']
Text:Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(19)TAKEN?True RQUGE:4.4664
Q1:When a weight is increased by a tiny amount, what would the error increase or decrease?
Q2:What does the gradient vector do?
Best ans: To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.
['A0: To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. to properly adapt the weights, the algorithm computed a negative gradient vector', 'A1: In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. to properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A2: A typical deep-learning system may have hundreds of millions of these adjustable weights. to properly adjust the weight vector, the learning algorithm computes the gradient vector that, for each weight, indicates by what amount the error would increase or decrease.', 'A3: To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A4: Increases', 'A5: Increase', 'A6: Decrease', 'A7: ']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(20)TAKEN?False RQUGE:4.448
Q1:To properly adjust the weight vector, what is done to the weights in the learning algorithm
Q2:What does the learning algorithm do to properly adjust the weight vector?
Best ans: Computation of gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount
['A0: For each weight, the learning algorithm computes a gradient vector that indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A1: Computation of gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A2: Computation of gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount', 'A3: The deep learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount', 'A4: Calculates', 'A5: Computed', 'A6: Computation', 'A7: A gradient vector']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(21)TAKEN?True RQUGE:4.0432
Q1:In what domains can deep learning be used?
Q2:What has deep learning dramatically improved?
Best ans: In speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.
['A0: In speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.', 'A1: Speech recognition visual object recognition object detection and many other domains such as drug discovery and genomics', 'A2: Drug discovery and genomics are two domains that have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains.', 'A3: These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.', 'A4: Vision', 'A5: Voice recognition', 'A6: Visual object recognition', 'A7: Image and speech recognition']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(22)TAKEN?True RQUGE:3.9979
Q1:What is the main idea of this passage?
Q2:What do many applications of deep learning use?
Best ans: Deep learning uses feedforward neural networks to map a fixed-size input (for example, an image) to a fix size output.
['A0: Neural networks are trained by a process called Stochastic Gradient Descent (SGD).', 'A1: Deep learning uses feedforward neural networks to map a fixed-size input (for example, an image) to a fix size output.', 'A2: Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general.', 'A3: Many applications of deep learning use feedforward neural network architecture (fig. 1), which learn to map a fixed-size input (for example, an image) to a fix size output.', 'A4: Local minimums', 'A5: Deep neural networks and backpropagation', 'A6: Deep neural networks', 'A7: Deep learning uses feedforward neural networks']
Text:Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(23)TAKEN?True RQUGE:3.9468
Q1:Using distributed representations, what are two advantages of deep nets over classic learning algorithms?
Q2:What does deep-learning theory show about deep nets?
Best ans: Using distributed representations deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representation21. both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.
['A0: Using distributed representations deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representation21.', 'A1: These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network71.', 'A2: Using distributed representations deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representation21. both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.', 'A3: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word.', 'A4: Fast ‘intuitive’ inference', 'A5: Powerful composition', 'A6: Easy to predict the target outputs', 'A7: The power of composition']
Text:Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a Recurrent Neural Network taking, as extra input, the representation extracted by a deep Convolution Neural Network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(24)TAKEN?True RQUGE:3.845
Q1:What is the main idea of the passage?
Q2:What do we expect to see in the future?
Best ans: We expect much of the future progress in vision to come from systems that are trained end-to-end and combine convnets with Recurrent Neural Networks (RNNs) that use reinforcement learning to decide where to look.
['A0: Recurrent Neural Networks (RNNs) are trained end-to-end and combine convnets with RNNs that use reinforcement learning to decide where to look.', 'A1: The same parameters (matrices u,v,w) are used at each time step.', 'A2: Deep learning is poised to make a large impact over the next few years.', 'A3: We expect much of the future progress in vision to come from systems that are trained end-to-end and combine convnets with Recurrent Neural Networks (RNNs) that use reinforcement learning to decide where to look.', 'A4: Deep neural networks', 'A5: Learning is largely unsupervised', 'A6: RNNs', 'A7: Artificial intelligence will come about through systems that combine representation learning with complex reasoning']
Text:Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with Recurrent Neural Networks that use reinforcement learning to decide where to look. Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  A Recurrent Neural Network can map an input sequence into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t) The same parameters (matrices U,V,W ) are used at each time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network.



(25)TAKEN?True RQUGE:3.7901
Q1:Which of these is not a type of machine learning?
Q2:Which of these is not a type of machine learning?
Best ans: Human engineering
['A0: Human engineering.', 'A1: Human engineering', 'A2: Deep neural networks', 'A3: Identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search', 'A4: Human engineering', 'A5: Humans', 'A6: Deep neural networks', 'A7: Domain expertise']
Text:Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(26)TAKEN?False RQUGE:3.7291
Q1:How are reluc functions used in neural networks?
Q2:What is computed at each hidden layer?
Best ans: F(z) = max(0,z) is applied to z to get the output. at each hidden layer we compute the error derivative with respect to the output of each unit.
['A0: Reluc functions are used in neural networks to approximate the weights of the inputs of each unit.', 'A1: Non-linear functions used in neural networks include the Rectified Linear Unit (ReLU) (relu) f(z) = max(0,z) commonly used in recent years.', 'A2: F(z) = max(0,z) is applied to z to get the output.', 'A3: F(z) = max(0,z) is applied to z to get the output. at each hidden layer we compute the error derivative with respect to the output of each unit.', 'A4: Used in recent years.', 'A5: Used in recent years', 'A6: Commonly used in recent years.', 'A7: Commonly used in recent years']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the Rectified Linear Unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(27)TAKEN?False RQUGE:3.7211
Q1:When a neural network is used to model language, what are some of the features that are learned?
Q2:What is the logic-inspired paradigm?
Best ans: These feature vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network. in the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols.
['A0: These feature vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A1: In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. these word vector were not determined ahead of time by experts, but automatically discovered by the neural network.', 'A2: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. the hidden layer of n-layer neural networks learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word.', 'A3: These feature vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network. in the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols.', 'A4: Weight matrices and scalar non-linearities', 'A5: Scalar non-linearities', 'A6: Active components each of which can be interpreted as a separate feature of the word.', 'A7: Active components each of which can be interpreted as a separate feature of the word']
Text:Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a Recurrent Neural Network taking, as extra input, the representation extracted by a deep Convolution Neural Network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(28)TAKEN?True RQUGE:3.6925
Q1:Machine-learning systems are used to identify objects in images, what?
Q2:What are some of the applications of machine learning?
Best ans: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. increasingly, these applications make use of a class of techniques called deep learning.
['A0: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. increasingly, these applications make use of a class of techniques called deep learning.', 'A1: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search', 'A2: Transcribing speech into text requires careful engineering and considerable domain expertise to design a feature extractor. deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years.', 'A3: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.', 'A4: Search', 'A5: Text', 'A6: Transscribe speech into text', 'A7: Translate speech into text']
Text:Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



(29)TAKEN?True RQUGE:3.6718
Q1:How many layers are in a typical convnet?
Q2:What are the first few stages of a ConvNet composed of?
Best ans: Two types of layers: convolutional layers and pooling layers.
['A0: Two types of layers', 'A1: First few stages are composed of two types of layers: convolutional layers and pooling layers.', 'A2: There are two types of layers in a typical convnet: convolutional layers and pooling layers.', 'A3: Two types of layers: convolutional layers and pooling layers.', 'A4: 2', 'A5: Two kinds', 'A6: Two', 'A7: Stages']
Text:ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit.  Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(30)TAKEN?False RQUGE:3.605
Q1:How are hidden layers different from hidden layers in deep learning?
Q2:What caused the interest in deep feedforward networks to be revived in 2006?
Best ans: Hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (fig. 1), such as hidden layers. interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research (CIFAR).
['A0: Multilayer networks are trained by simple Stochastic Gradient Descent (SGD). hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (fig. 1), such as hidden layers.', 'A1: Many applications of deep learning use feedforward neural network architecture (fig. 1), which learn to map a fixed-size input (for example, an image) to a fix size output.', 'A2: Hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (fig. 1), such as hidden layers. interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research (CIFAR).', 'A3: Hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (fig. 1), such as hidden layers.', 'A4: Classification becomes linearly separable', 'A5: Classification', 'A6: Local minima are not a serious issue in general', 'A7: Local minima are not a serious issue']
Text:Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(31)TAKEN?True RQUGE:3.4794
Q1:The following are examples for the use of convnets:
Q2:What other applications are ConvNets being used for?
Best ans: Mobileye and nvidia are using convnet-based methods in their autonomous mobile robots and self-driving cars. other applications gaining importance involve natural language understanding14 and speech recognition7
['A0: Mobileye and nvidia are using convnet-based methods in their autonomous mobile robots and self-driving cars. other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A1: Mobileye and nvidia are using convnet-based methods in their autonomous mobile robots and self-driving cars. other applications gaining importance involve natural language understanding14 and speech recognition7', 'A2: Face recognition is a major recent practical success of convnets. other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A3: A major recent practical success of face recognition is face recognition. other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A4: Voice', 'A5: Speech recognition', 'A6: Facial expression recognition', 'A7: Face recognition']
Text:ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(32)TAKEN?True RQUGE:3.4075
Q1:Why do we discover the structure of the world by watching it?
Q2:What type of learning is largely unsupervised?
Best ans: Learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. human and animal learning is mainly unsupervised.
['A0: Animals and humans learn by observation. humans and animals are largely unsupervised.', 'A1: The structure of the world is unsupervised. humans and animals learn the structure of their world by observing it.', 'A2: Learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. human and animal learning is mainly unsupervised.', 'A3: Learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object.', 'A4: Unsupervised learning', 'A5: Human learning is largely unsupervised', 'A6: Largely unsupervised', 'A7: Learning is largely unsupervised']
Text:Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with Recurrent Neural Networks that use reinforcement learning to decide where to look. Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  A Recurrent Neural Network can map an input sequence into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t) The same parameters (matrices U,V,W ) are used at each time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network.



(33)TAKEN?False RQUGE:2.9722
Q1:The architecture of a typical convnet is structured as what?
Q2:What is the architecture of a typical ConvNet?
Best ans: Series of stages. the first few stages are composed of two types of layers: convolutional layers and pooling layers.
['A0: Stages', 'A1: Series of stages', 'A2: A series of stages', 'A3: Series of stages. the first few stages are composed of two types of layers: convolutional layers and pooling layers.', 'A4: Sequences', 'A5: Sequence', 'A6: Stage', 'A7: Stages']
Text:ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit.  Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(34)TAKEN?True RQUGE:2.9596
Q1:Who first introduced backpropagation?
Q2:What was backpropagation first introduced to?
Best ans: Recurrent Neural Networks (RNNs) were first introduced to train RNNs (rnns)
['A0: Recurrent Neural Networks (RNNs) were first introduced to train Recurrent Neural Network (RNN).', 'A1: Recurrent Neural Networks (RNNs) were first introduced to train RNNs (rnns)', 'A2: To train Recurrent Neural Networks (RNNs) (rnns) backpropagation was first introduced to train rnns.', 'A3: To train Recurrent Neural Networks (RNNs) (rnns) backpropagation was first introduced to train rnns', 'A4: Convnet', 'A5: Researchers', 'A6: To train rnns', 'A7: Authors']
Text:For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks. Backpropagation was first introduced to train RNNs (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(35)TAKEN?False RQUGE:2.9094
Q1:Why do we need to use a non-linear function?
Q2:What is used to get the derivatives?
Best ans: A non-linear function is used to get the output of the units in the layer below to get a weighted sum of the output. the weightes are jacobian matrix.
['A0: In order to get a weighted sum, we need to compute the error derivative. the weightes are jacobian matrices.', 'A1: A non-linear function is used to get the output of the units in the layer below to get a weighted sum of the output. the weightes are jacobian matrix.', 'A2: A non-linear function is used to get the output of the units in the layer below to get a weighted sum of the output.', 'A3: Raw pixels could not possibly distinguish the latter two.', 'A4: With multiple non-linear layers', 'A5: Sensitive to minute details', 'A6: To get the output of each unit', 'A7: Simultaneously sensitive to minute details']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the Rectified Linear Unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(36)TAKEN?True RQUGE:2.8782
Q1:How do you adjust the weights in a typical deep learning system?
Q2:What does the SGD process do?
Best ans: Stochastic Gradient Descent (SGD) (sgd) is the process whereby practitioners show the input vector for a few examples and compute the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly. to properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.
['A0: Using a procedure called Stochastic Gradient Descent (SGD) (sgd), most practitioners show the input vector for a few examples, computing the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly.', 'A1: Stochastic Gradient Descent (SGD) (sgd) is the process whereby practitioners show the input vector for a few examples and compute the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly.', 'A2: Stochastic Gradient Descent (SGD) (sgd) is the process whereby practitioners show the input vector for a few examples and compute the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly. to properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.', 'A3: To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. to properly adapt the weights, the method is called Stochastic Gradient Descent (SGD) (sgd).', 'A4: Computes a gradient', 'A5: Computes a gradient vector', 'A6: Computations a gradient vector', 'A7: Stochastic Gradient Descent (SGD) (sgd)']
Text:Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.



(37)TAKEN?True RQUGE:2.7983
Q1:A neural network can be used to learn to recognize what kind of objects
Q2:How does a deep learning network learn to recognize objects?
Best ans: In this example, we are going to use a simple deep learning architecture to learn to recognize objects. the basic idea is that the network learns to recognize the objects by performing a series of tasks on the input data. each task is performed on the output of the previous task and the output is compared to the input.
['A0: This is done by learning to recognize objects based on their shape and color. the basic idea is that the network learns to recognize what kind of objects are in the image. then it applies a non-linear function to the weighted sum to get a weighte d of each unit.', 'A1: In this example, we are going to use a simple deep learning architecture to learn to recognize objects. the basic idea is that the network learns to recognize the objects by performing a series of tasks on the input data.', 'A2: This is done by learning to recognize objects based on their shape and color. the basic idea is that the network learns to recognize what kind of objects are in the image.', 'A3: In this example, we are going to use a simple deep learning architecture to learn to recognize objects. the basic idea is that the network learns to recognize the objects by performing a series of tasks on the input data. each task is performed on the output of the previous task and the output is compared to the input.', 'A4: Images from white', 'A5: Images', 'A6: Image', 'A7: Wolves']
Text:It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output. Non-linear functions used in neural networks include the Rectified Linear Unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two. Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



(38)TAKEN?True RQUGE:2.6468
Q1:What are some applications of convnets?
Q2:What are two examples of ConvNet-based applications that are gaining in importance?
Best ans: Face recognition and speech recognition
['A0: Face recognition is a major recent practical success of convnets. other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A1: Companies such as mobileye and nvidia are using convnet-based methods in their upcoming vision- tems for cars other applications gaining importance involve natural language understanding14 and speech recognition7. so, the final answer is natural language understand- ing.', 'A2: Images can be labeled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A3: Companies such as mobileye and nvidia are using convnet-based methods in their upcoming vision- tems for cars other applications gaining importance involve natural language understanding14 and speech recognition7.', 'A4: Detection, segmentation', 'A5: Facial recognition', 'A6: Self-driving cars', 'A7: Face recognition and speech recognition']
Text:ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(39)TAKEN?True RQUGE:2.498
Q1:In what way do neural networks have two different exponential advantages? choose from:
Q2:What are the two advantages of deep nets over classic learning algorithms?
Best ans: Power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40
['A0: Deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21', 'A1: Power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.', 'A2: Power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40', 'A3: The power of composition and depend on the underlying data-generating distribution having an appropriate componential structure', 'A4: Both', 'A5: Distributed representations', 'A6: From the power of composition', 'A7: The power of composition']
Text:Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a Recurrent Neural Network taking, as extra input, the representation extracted by a deep Convolution Neural Network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



(40)TAKEN?True RQUGE:2.4222
Q1:How are Recurrent Neural Networks (RNNs) trained?
Q2:What are the parameters of a RNN?
Best ans: Recurrent Neural Networks (RNNs) are trained by using reinforcement learning. reinforcement learning is used to train RNNs. the same parameters (matrices u,v,w) are used at each time step.
['A0: Recurrent Neural Networks (RNNs) are trained by using reinforcement learning. reinforcement learning is used to train RNNs.', 'A1: In reinforcement learning, the same parameters (matrices u,v,w) are used at every time step of a network. the same parameters are used for each time step.', 'A2: Recurrent Neural Networks (RNNs) are trained by using reinforcement learning. reinforcement learning is used to train RNNs. the same parameters (matrices u,v,w) are used at each time step.', 'A3: An Recurrent Neural Network (RNN) is trained to map an input sequence into an output sequence with elements ot, with each element depending on all the previous xt (for t  t) the same parameters (matrices u,v,w ) are used at each time step.', 'A4: Unsupervised', 'A5: Backpropagation', 'A6: Backpropagation algorithm', 'A7: Using reinforcement learning']
Text:Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with Recurrent Neural Networks that use reinforcement learning to decide where to look. Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  A Recurrent Neural Network can map an input sequence into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t) The same parameters (matrices U,V,W ) are used at each time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network.



(41)TAKEN?True RQUGE:2.3943
Q1:What is the advantage of using Long Short-term Memory (LSTM) networks?
Q2:What have long short-term memory networks proven to be more effective than conventional RNNs?
Best ans: They have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription. memory networks have yielded excellent performance on standard question-answering benchmarks.
['A0: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backpropagation to train Recurrent Neural Networks (RNNs).', 'A1: They have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription. memory networks have yielded excellent performance on standard question-answering benchmarks.', 'A2: They have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.', 'A3: They have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription', 'A4: Having several layers', 'A5: Multiple layers', 'A6: Several layers for each time step87', 'A7: Several layers for each time step']
Text:For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks. Backpropagation was first introduced to train RNNs (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(42)TAKEN?False RQUGE:1.6415
Q1:How can Long Short-term Memory (LSTM) networks be used to train Recurrent Neural Networks (RNNs)?
Q2:What have LSTM networks proven to be more effective than conventional RNNs?
Best ans: Long Short-term Memory (LSTM) networks have subsequently proved to be more effective than conventional Recurrent Neural Networks (RNNs), especially when they have several layers for each time step87, and enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription. memory networks have yielded excellent performance on standard question-answering benchmarks.
['A0: Long Short-term Memory (LSTM) networks have subsequently proved to be more effective than conventional Recurrent Neural Networks (RNNs), especially when they have several layers for each time step87, and enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.', 'A1: Long Short-term Memory (LSTM) networks have subsequently proved to be more effective than conventional Recurrent Neural Networks (RNNs), especially when they have several layers for each time step87, and enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription. memory networks have yielded excellent performance on standard question-answering benchmarks.', 'A2: When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backpropagation to train Recurrent Neural Networks (RNNs). this rather naive way of performing machine translation has quickly become competitive with the state-of-the-art.', 'A3: Long Short-term Memory (LSTM) networks have subsequently proved to be more effective than conventional Recurrent Neural Networks (RNNs), especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.', 'A4: Several layers for each time step', 'A5: Have several layers for each time step', 'A6: They have several layers for each time step87', 'A7: When they have several layers for each time step']
Text:For tasks that involve sequential inputs, such as speech and language, it is often better to use Recurrent Neural Networks. Backpropagation was first introduced to train RNNs (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an Recurrent Neural Network similar to the ones used for machine translation and neural language modelling. Long Short-term Memory networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



(43)TAKEN?False RQUGE:1.4675
Q1:Who designed convnets?
Q2:Convolutional and pooling layers in ConvNets are inspired by the classic notions
Best ans: Visual neuroscience43
['A0: Leonardo dicaprio', 'A1: Visual neuroscience43', 'A2: Visual neuroscience', 'A3: Visual neuroscience43, and the overall architecture is reminiscent of the lgn–v1–v2–v4–it hierarchy in the visual cortex ventral pathway44', 'A4: Monkeys', 'A5: Scientists', 'A6: Visual neuroscience43', 'A7: Deep neural networks']
Text:ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a Rectified Linear Unit.  Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



(44)TAKEN?False RQUGE:1.3144
Q1:How are Recurrent Neural Networks (RNNs) trained? no answer>
Q2:What is the name of the training method used to train rnns?
Best ans: Recurrent Neural Networks (RNNs) (rnns) are trained using reinforcement learning.
['A0: Recurrent Neural Networks (RNNs) (rnns) are trained using reinforcement learning', 'A1: Recurrent Neural Networks (RNNs) (rnns) are trained using reinforcement learning.', 'A2: Backpropagation algorithm (fig. 1) can be directly applied to the computational graph of the unfolded network.', 'A3: Backpropagation algorithm (fig. 1) can be directly applied to the computational graph of the unfolded network', 'A4: End-to-end', 'A5: Unsupervised', 'A6: Reinforcement learning', 'A7: Backpropagation algorithm']
Text:Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with Recurrent Neural Networks that use reinforcement learning to decide where to look. Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  A Recurrent Neural Network can map an input sequence into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t) The same parameters (matrices U,V,W ) are used at each time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network.



(45)TAKEN?False RQUGE:1.1914
Q1:In what technology are convnets being used?
Q2:What are two applications of ConvNets?
Best ans: Autonomous mobile robots and self-driving cars
['A0: Autonomous mobile robots and self-driving cars.', 'A1: Autonomous mobile robots and self-driving cars', 'A2: Autonomous mobile robots and self-driving cars are applications of convnets. therefore, the final answer is autonomous mobile robot.', 'A3: Autonomous mobile robots and self-driving cars. companies such as mobileye and nvidia are using convnet-based methods in their upcoming vision- tems for cars', 'A4: Vision', 'A5: Robots', 'A6: Cameras', 'A7: Cars']
Text:ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



(46)TAKEN?True RQUGE:1.1136
Q1:The idea of deep learning was discovered by several different groups during which decade?
Q2:During what decade was the idea of backpropagation discovered?
Best ans: 1980s
['A0: 1980s.', "A1: 1990's", 'A2: 1979', 'A3: 1980s', 'A4: 1980', 'A5: 80', 'A6: 1970', 'A7: 1990s']
Text:Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.



(47)TAKEN?True RQUGE:1.0272
Q1:Which of these is not a type of neural network?
Q2:Which of these is not a type of neural network?
Best ans: An unfolded network.
['A0: A convnets', 'A1: An unfolded network.', 'A2: An unfolded network', 'A3: Convnets', 'A4: Human', 'A5: Animal', 'A6: Convnets', 'A7: Animal learning']
Text:Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with Recurrent Neural Networks that use reinforcement learning to decide where to look. Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  A Recurrent Neural Network can map an input sequence into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t) The same parameters (matrices U,V,W ) are used at each time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network.



