(0)TAKEN?True RQUGE:4.8036
Q1:What is the purpose of this paper?
Q2:What is the purpose of this paper?
Best ans: To evaluate text summarization systems using COMET model.
['A0: To evaluate text summarization systems using COMET model.', 'A1: Using COMET to evaluate text summarization systems', 'A2: An evaluation model for text summarization systems that uses multilingual outputs for pre-training.', 'A3: This paper introduces a variant of the COMET model – trained on the annotated summarization outputs that uses MT data as a pre-training.', 'A4: to assess machine translation quality', 'A5: Use COMET for text summarization evaluation', 'A6: evaluate text summarization systems', 'A7: To evaluate the quality of text summarization systems']
Text:COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.



(1)TAKEN?True RQUGE:4.5834
Q1:The COMET score is calculated by what method?
Q2:What does COMET use?
Best ans: pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value
['A0: Based on semantic similarities between the translated and reference texts', 'A1: semantic similarities between the translated and reference texts', 'A2: The COMET score is calculated by a stack of feed-forward layers that outputs a single numerical value.', 'A3: pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value', 'A4: stack of feed-forward layers', 'A5: human perception of translation quality', 'A6: pre-trained multilingual language model', 'A7: semantic similarities between the translated and reference texts']
Text: COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.



(2)TAKEN?True RQUGE:4.3702
Q1:Annotation-based generative models: a new approach to quality assessment
Q2:How is the model evaluated?
Best ans: In this paper we propose an annotated version of the COMET2 model that can be used to evaluate the quality of summary output of generative models. We evaluate the model on datasets with various annotation styles.
['A0: Using annotated datasets, we propose an automatic model for quality assessment of generative models. We evaluate our approach on datasets with various annotation styles.', 'A1: In this paper we propose an annotated version of the COMET2 model that can be used to evaluate the quality of summary output of generative models.', 'A2: In this paper we propose an annotated version of the COMET2 model that can be used to evaluate the quality of summary output of generative models. We evaluate the model on datasets with various annotation styles.', 'A3: COMES2 is a variant of the Metric Shared Task model developed by Rei et al. (2020) that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality and regressing a quality score to the data.', 'A4: an annotation-based generative model', 'A5: Using annotated dataset to evaluate quality of generative models', 'A6: Computational Evaluation of Summary Quality of Annotated Generative Models', 'A7: Annotation-based generative models: a new approach to quality assessment.']
Text: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.



(3)TAKEN?True RQUGE:4.2408
Q1:Predicting summary evaluation metrics using annotated MT data.
Q2:How is the COMES2 model evaluated?
Best ans: In this paper we propose an automatic method for predicting summary evaluation metrics using annotated MT data. We evaluate our method on datasets with various annotation styles.
['A0: In this paper we propose an automatic method for predicting summary evaluation metrics using annotated MT data.', 'A1: In this paper we propose an automatic method for predicting summary evaluation metrics using annotated MT data. We evaluate our method on datasets with various annotation styles.', 'A2: Using annotated MT data, we propose a variant of the model called COMES2 that uses the annotted MT dataset for pre-training and is capable of predicting several aspects of summary quality.', 'A3: We propose a variant of the model – COMES2 – that uses the annotation data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4).', 'A4: Computational Modeling for Summary Evaluation', 'A5: Predicting summary evaluation metrics using annotated MT datasets.', 'A6: Using annotated MT data to predict summary evaluation metrics', 'A7: Predicting summary evaluation metrics using annotated MT datasets']
Text: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.



(11)TAKEN?True RQUGE:3.2242
Q1:Predicting summary evaluation metrics using annotated MT data. A variant of COMES2
Q2:What model do we propose?
Best ans: Predicting summary evaluation metrics using annotated MT data. A variant of COMES2
['A0: The COMES2 model is a variant of the COMET model that uses annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.', 'A1: Predicting summary evaluation metrics using annotated MT data. A variant of COMES2', 'A2: COMES2 is a variant of the COMET model that uses annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.', 'A3: A variant of the COMES2 model is proposed that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.', 'A4: Annotation-based summary evaluation metrics for generative models', 'A5: Computational Modeling for Summary Evaluation', 'A6: Using annotated MT data to predict summary evaluation metrics for generative models', 'A7: COMES2: Predicting summary evaluation metrics using annotated WMT data']
Text: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.



(12)TAKEN?True RQUGE:2.8966
Q1:What is the purpose of the COMET2 model?
Q2:What is the purpose of metrics?
Best ans: To evaluate the quality of summary output of a generative model.
['A0: To evaluate the quality of summary output of a generative model.', 'A1: Using annotated MT data for pre-training and predicting several aspects of summary quality.', 'A2: Using annotated MT data for pre-training and predicting several aspects of summary quality', 'A3: A variant of the WMT model – COMES2 that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality.', 'A4: To evaluate summary quality', 'A5: evaluate summary quality of generative models', 'A6: evaluate summary quality', 'A7: summary evaluation metrics']
Text: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.



(14)TAKEN?True RQUGE:2.8203
Q1:Predicting summary evaluation metrics using annotated MT data
Q2:What do we propose?
Best ans: A variant of the Metrics Shared Task model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects o summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.
['A0: This paper presents an approach to automatically predicting summary evaluation metrics using annotated textual annotations.', 'A1: Using annotated MT data, we propose an automatic metric for summarizing summary quality. We evaluate our approach on selected datasets with various annotation styles.', 'A2: A variant of the Metrics Shared Task model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects o summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.', 'A3: We propose a variant of the Metrics Shared Task model – COMES2 – that uses the annotated MT data for pre-training and is capable to predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.', 'A4: Computational Modeling for Summary Evaluation', 'A5: Using annotated MT data to predict summary evaluation metrics', 'A6: Predicting summary evaluation metrics using annotated MT data.', 'A7: COMES2: Predicting summary evaluation metrics using annotated MT data']
Text: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.



(15)TAKEN?True RQUGE:2.5031
Q1:What is the purpose of the COMET2 model? How is it trained?
Q2:What datasets are used to evaluate the COMET2 model?
Best ans: This paper presents a variant of the COMET model that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.
['A0: Predicting several aspects a quality score for summary output from annotated datasets.', 'A1: To evaluate the quality of summary output from WMT models, we propose a variant of a model called COMET2 that uses the annotated data for pre-training and is capable of pre-learning a number of aspects of summary quality.', 'A2: This paper presents a variant of the COMET model that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality.', 'A3: This paper presents a variant of the COMET model that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotation styles.', 'A4: summary evaluation metrics', 'A5: Predicting several aspects of summary quality', 'A6: to measure progress during training and compare output from independent systems', 'A7: predicting several aspects of summary quality']
Text: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.



(20)TAKEN?True RQUGE:1.1028
Q1:what are some examples that use COMET?
Q2:What is an example of a COMET score?
Best ans: An example of this is the translation of a Wikipedia article into English.
['A0: CoMet scores were created by the University of California, Berkeley, and Google.', 'A1: Translation quality metrics are used to evaluate the quality of translations.', 'A2: An example of this is the translation of a Wikipedia article into English.', 'A3: An example of this is the translation of a Wikipedia article into English. The final answer: Wikipedia.', 'A4: Translate', 'A5: Translation Quality', 'A6: Google Translate', 'A7: Machine translation']
Text: COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.



