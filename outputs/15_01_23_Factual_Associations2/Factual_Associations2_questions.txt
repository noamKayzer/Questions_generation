Q:What does ROME achieve?
A:Compared to previous fine-tuning (zhu and colleagues, 2020), interpretability-based methods, Rank-one Model Editing (ROME) achieves good generalization and specificity simultaneously simultaneously.
--------------------------------------------------
Q:Using an autoregressive transformer model, which of these is not true?
A:Mlp contributions dominate the early site.
--------------------------------------------------
Q:How do we select p[o]?
A:We select  to be 3 times larger than the empirical standard deviation of embeddings. we select p[o] to be three times larger then the empirical normal deviation of embeddedings for the grid of states.
Q:What are the steps in the causal mediation analysis?
A:Using a gpt model, we perform a perturbation of the model. we run a set of gpt models on the corrupted baseline. we select  to be 3 times larger than the empirical standard deviation of embeddings. we use the  method to determine the causal importance of a state variable.
--------------------------------------------------
Q:At what layer did the AIE=8.7% occur?
A:Layer 15
--------------------------------------------------
Q:Where does the information accumulate?
A:Middle layer of the mlp
--------------------------------------------------
Q:What type of memory can MLPs be modeled as?
A:Associative
--------------------------------------------------
Q:How does the optimization affect the model weights?
A:Optimization does not directly alter model weight; it identifies a vector representation v that when output at the targeted mlp module represents the new property for the subject s.
Q:Why can't a convolutional network solve this problem?
A:It can only solve the least-squares problem.
Q:What are the steps in selecting the subject?
A:Choose inputs that represent the subject at its last token as a key. choose some vector value v√¢ that encodes a new relation (r, o) as property of subject. sample inputs by using g and g(m(l) i:= z)
Q:How does Bau solve the constrained least-squares problem?
A:We solve the constrained least-squares problem using a fully connected layer. we use the moore-penrose pseudoinverse to solve the problem.
--------------------------------------------------
