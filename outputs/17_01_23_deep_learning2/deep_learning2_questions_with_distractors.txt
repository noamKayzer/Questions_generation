--------------------------------------------------
TEXT: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.
section 1 - (1) What do machine-learning systems increasingly use? --- Class of techniques called deep learning.
 
section 1 - (2) Which of these is not a type of machine learning? --- Human engineering
 Pattern recognition
section 1 - (3) What are some of the applications of machine learning? --- Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. increasingly, these applications make use of a class of techniques called deep learning.
 Deep neural networks
--------------------------------------------------
TEXT: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called Stochastic Gradient Descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average. Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other. The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule.
section 2 - (1) What procedure do most practitioners use? --- Stochastic Gradient Descent (SGD))
 
section 2 - (2) What does deep learning allow? --- Recurrent nets have shone light on sequential data such as text and speech using deep learning. deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.
 
section 2 - (3) What does the gradient vector do? --- To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.
 
section 2 - (4) What has deep learning dramatically improved? --- In speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.
 
section 2 - (5) What does the SGD process do? --- Stochastic Gradient Descent (SGD) (sgd) is the process whereby practitioners show the input vector for a few examples and compute the outputs and errors, and computing the average gradient for those examples, and adjusting the weights accordingly. to properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.
 
section 2 - (6) Where do we compute the error derivative with respect to the output of each unit? --- Each hidden layer
 Several layers
section 2 - (7) How does a deep learning network learn to recognize objects? --- In this example, we are going to use a simple deep learning architecture to learn to recognize objects. the basic idea is that the network learns to recognize the objects by performing a series of tasks on the input data. each task is performed on the output of the previous task and the output is compared to the input.
 | multilayer neural networks
--------------------------------------------------
TEXT: Multilayer networks can be trained by simple Stochastic Gradient Descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s. Many applications of deep learning use feedforward neural network architecture (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed size output. In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general in general. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1), such as hidden layers. Interest in deep feedforward networks was revived around 2006 (refs 31–34) by a group of researchers brought together by the Canadian Institute for Advanced Research The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers. Almost all of these saddle points have very similar values of the objective function. Hence, it does not much matter which of the saddle points the algorithm gets stuck at. Information flows bottom up with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. In 2009, the approach was used to map short temporal windows of coefficients extracted from a sound wave to a set of probabilities for the various fragments of speech that might be represented by the frame in the centre of the window. It achieved record-breaking results on a standard speech recognition benchmark that used a small vocabulary. By 2012, versions of the deep net from 2009 were being developed by many of the major speech groups and were already being deployed in Android phones. For smaller data sets, unsupervised pre-training helps to prevent overfitting40, leading to significantly better generalization.
section 3 - (1) When was interest in deep feedforward networks revived? --- 2006
 
section 3 - (2) Which of these is not a serious issue in general in general? --- Local minima are not a serious issue in general in general.
 Hidden layers can be seen as distorting the input
section 3 - (3) What do many applications of deep learning use? --- Deep learning uses feedforward neural networks to map a fixed-size input (for example, an image) to a fix size output.
 
section 3 - (4) During what decade was the idea of backpropagation discovered? --- 1980s
 2009
