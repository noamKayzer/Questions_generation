RQUGE:4.7668
 Q1:What are ConvNets used for??
Q2:What have ConvNets been applied to with great success?
Best ans: Detection, segmentation and recognition of objects and regions in images. Therefore, the final answer is the detection, Segmentation  and recognition. of objects.
['A0: The relevant information to answer the above question is: ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images', 'A1: Detection, segmentation and recognition of objects and regions in images. Therefore, the final answer is the detection, Segmentation  and recognition. of objects. and recognition', 'A2: Detection, segmentation and recognition of objects and regions in images. Therefore, the final answer is the detection, Segmentation  and recognition. of objects.', 'A3: Detection, segmentation and recognition of objects and regions in images. Therefore, the final answer is the detection, Segmentation  and recognition.', 'A4: real-time vision applications', 'A5: face recognition', 'A6: recognition of objects and regions in images since the early 2000s', 'A7: recognition of objects and regions in images']Text: ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images since the early 2000s. A major recent practical success of face recognition is face recognition. Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and self-driving cars. Companies such as Mobileye and NVIDIA are using ConvNet-based methods in their upcoming vision-   tems for cars. Other applications gaining importance involve natural language understanding14 and speech recognition7.  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. ConvNets are easily amenable to efficient hardware implementations in chips or field-programmable gate arrays66,67. A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.



RQUGE:4.7132
 Q1:What is the difference between deep learning and regular learning??
Q2:What allows computational models to learn representations of data with multiple levels of abstraction?
Best ans: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. Therefore, the final answer is Deep learning....
['A0: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. Therefore, the final answer is Deep learning.....', 'A1: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. Therefore, the final answer is Deep learning....', 'A2: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. Therefore, the final answer is Deep learning...', 'A3: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. Therefore, the final answer is Deep learning..', 'A4: deep-learning architecture is a multilayer stack of simple modules, all (or most)', 'A5: multiple processing layers to learn representations of data with multiple levels of abstraction.', 'A6: multiple processing layers to learn representations of data with multiple levels of abstraction', 'A7: computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction']Text: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.  Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.  Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other.  The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule. It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output.  Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two.  Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module (module) in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



RQUGE:4.5734
 Q1:What is the key aspect of deep learning??
Q2:What is the key aspect of deep learning?
Best ans: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Therefore, the final answer is these layers of layers are notdesigned by human.
['A0: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Therefore, the final answer is these layers of layers are notdesigned by human. engineers : they were learned from Data using', 'A1: These layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Therefore, the final answer is these layers of layers are notdesigned by human.', 'A2: The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Therefore, the final answer is these layers are notdesigned by human.', 'A3: Increasingly, these applications make use of a class of techniques called deep learning. Therefore, the final answer is these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure..', 'A4: These layers of features are not designed by human engineers. They', 'A5: These layers of features are not designed by human engineers.', 'A6: These layers of features are not designed by human', 'A7: These layers of features are not designed by human engineers']Text: Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition. system required careful engineering and considerable domain expertise to design a feature extractor.  Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data.  New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.



RQUGE:4.3746
 Q1:what type of neural network is used for speech recognition?
Q2:What is better for tasks that involve sequential inputs?
Best ans:  recurrent neural networks (RNNs)
['A0: This is relevant information: For tasks that involve sequential inputs, such as speech and language, it is often better to use recurrent neural networks (RNNs) Backpropagation was first introduced to train RNNs But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. Therefore, the final answer is recursive neural networks.', 'A1: In the context of speech recognition, the relevant information to answer the above question is: For tasks that involve sequential inputs, such as speech and language, it is often better to use recurrent neural networks (RNNs) Backpropagation was first introduced to train RNNs But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. Therefore, the final answer is recursive neural network ( recurrent neural network (RNN) ).', 'A2: To answer the above question, the relevant sentence is: For tasks that involve sequential inputs, such as speech and language, it is often better to use recurrent neural networks (RNNs) Backpropagation was first introduced to train RNNs But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. Therefore, the final answer is recursive neural networks..', 'A3: The relevant information to answer the above question is: For tasks that involve sequential inputs, such as speech and language, it is often better to use recurrent neural networks (RNNs) Backpropagation was first introduced to train RNNs But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. Therefore, the final answer is recursive neural networks.', 'A4:  recurrent neural networks (RNNs)', 'A5: memory', 'A6: deep ConvNet', 'A7: memory networks']Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use recurrent neural networks (RNNs)  Backpropagation was first introduced to train RNNs But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an recurrent neural network (RNN) similar to the ones used for machine translation and neural language modelling. long short-term memory (LSTM) networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



RQUGE:4.3003
 Q1:In deep learning, what is the process for adjusting the weights?
Q2:What procedure do most practitioners use?
Best ans: In deep learning, the weights are adjusted by the learning algorithm. To properly adjust weights, the learning algorithms computes a gradient vector that indicates by what amount an error would increase or decrease if the weight were increased by a tiny amount. Therefore, the final answer is stochastic gradient descent ( stochastic gradient descent (SGD) ) This..
['A0: Using a procedure called stochastic gradient descent (SGD)  most practitioners use the input vector for few examples, computing outputs and the errors for those examples, and computing the average gradient for those example, and adjusting the weights accordingly to minimize the output error. Therefore, the final answer is stochistic gradient descent.', 'A1: In deep learning, the weights are adjusted by the learning algorithm. To properly adjust weights, the learning algorithms computes a gradient vector that indicates by what amount an error would increase or decrease if the weight were increased by a tiny amount. Therefore, the final answer is stochastic gradient descent ( stochastic gradient descent (SGD) ) This..', 'A2: In deep learning, the weights are adjusted by the learning algorithm. To properly adjust weights, the learning algorithms computes a gradient vector that indicates by what amount an error would increase or decrease if the weight were increased by a tiny amount. Therefore, the final answer is stochastic gradient descent ( stochastic gradient descent (SGD) ) This.', 'A3: Stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. Therefore, the final answer is stochastic gradient descend ( stochastic gradient descent (SGD) ) This..', 'A4: the learning algorithm computes a gradient vector', 'A5: the learning algorithm computes', 'A6: a procedure called stochastic gradient descent', 'A7: SGD']Text: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.  Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.  Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other.  The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule. It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output.  Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two.  Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module (module) in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



RQUGE:3.9473
 Q1:How do humans and animals learn?
Q2:What is the structure of the world?
Best ans: Human and animal learning is largely unsupervised. We discover the structure of the world by observing it, not by being told the name of every object. Therefore, the final answer is by observating it.
['A0: We discover the structure of the world by observing it, not by being told the name of every object. Therefore, the final answer is by observating it. Observing.', 'A1: We discover the structure of the world by observing it, not by being told the name of every object. Therefore, the final answer is by observating it.', 'A2: Human and animal learning is largely unsupervised. We discover the structure of the world by observing it, not by being told the name of every object. Therefore, the final answer is by observating it..', 'A3: Human and animal learning is largely unsupervised. We discover the structure of the world by observing it, not by being told the name of every object. Therefore, the final answer is by observating it.', 'A4: discover the structure of the world by observing it', 'A5: discover the structure of the world', 'A6: largely unsupervised', 'A7: by observing it']Text: Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object. We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with recurrent neural networks (RNNs) that use reinforcement learning to decide where to look. Natural language understanding is another area in which deep learning is poised to make a large impact over the next few years. Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  A recurrent neural network can maps (map) an input sequence into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t) The same parameters (matrices U,V,W ) are used at each time step. The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network.



RQUGE:3.2576
 Q1:What are the two types of layers in a ConvNet?
Q2:What are the first few stages of a ConvNet composed of?
Best ans: Convolutional layers and pooling layers. Therefore, the final answer is convolutional and pooled layers..
['A0: Convolutional layers and pooling layers. Therefore, the final answer is convolutional and pooled layers...', 'A1: Convolutional layers and pooling layers. Therefore, the final answer is convolutional and pooled layers..', 'A2: In the first few stages, convolutional layers and pooling are the two types of layers. Therefore, the final answer is convolutionary.', 'A3: Convolutional layers and pooling layers. Therefore, the final answer is convolutional and pooled layers.', 'A4: Convolutional', 'A5: pooling layers.', 'A6: pools', 'A7: pooling layers']Text: ConvNets are designed to process data that come in the form of multiple arrays. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a ConvNet are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a rectified linear unit (ReLU)   Deep neural networks exploit the property that many natural signals are compositional hierarchies. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Convolutional and pooling layers in ConvNets are inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway44. ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex.



RQUGE:2.0766
 Q1:How can we train recurrent neural networks??
Q2:What is the solution to the problem of backpropagation?
Best ans: To answer the question, consider the following: Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train RNns.
['A0: Context: Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train RNns. This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic', 'A1: To answer the question, consider the following: Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train RNns.', 'A2: The relevant information to answer the above question is: Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train RNns.', 'A3: To answer the question, consider the following: Backpropagation was first introduced to train recurrent neural networks (RNNs) But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train RNns. This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence', 'A4: backPropagation', 'A5: Apply backPropagation to train RNN', 'A6: Apply backPropagation', 'A7: when they have several layers for each time step']Text: For tasks that involve sequential inputs, such as speech and language, it is often better to use recurrent neural networks (RNNs)  Backpropagation was first introduced to train RNNs But training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78. When we consider the output of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network, it becomes clear how we can apply backPropagation to train Rnns.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art. This raises serious doubts about whether understanding a sentence requires anything like the internal symbolic expressions that are manipulated by using inference rules. It is more compatible with the view that everyday reasoning involves many simultaneous analogies of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103. Instead of translating meaning of a French sentence into an English sentence, one can learn to ‘translate’ the meaning of an image.  The encoder is a deep ConvNet that converts pixels into an activity vector in its last hidden layer. The decoder is an recurrent neural network (RNN) similar to the ones used for machine translation and neural language modelling. long short-term memory (LSTM) networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76. Proposals include the Neural Turing Machine in which the network is augmented by a ‘tape-like’ memory that the RNN can choose to read from or write to88, and memory networks. Memory networks have yielded excellent performance on standard question-answering benchmarks.



RQUGE:1.5338
 Q1:Why are neural networks better than classic learning algorithms? Why?
Q2:What do the hidden layers of a multilayer neural network learn to do?
Best ans: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Therefore, the final answer is learn to present the network's input in  away that makes It easy to anticipate the target.
["A0: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Therefore, the final answer is learn to present the network's input in  away that makes It easy to anticipate the target..", "A1: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Therefore, the final answer is learn to present the network's input in  away that makes It easy to anticipate the target.", "A2: The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Therefore, the final answer is learn to present the network's input in", 'A3: Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. Therefore, the final answer is the hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs..', 'A4: power', 'A5: power of composition', 'A6: distributed representations.', 'A7: distribution of representations']Text: Deep-learning theory shows that deep nets have two different exponential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40. The hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs. Figure 3 | From image to text. Captions generated by a recurrent neural network taking, as extra input, the representation extracted by a deep convolution neural network from a test image.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0. The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word. In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbols. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast ‘intuitive’ inference that underpins effortless commonsense reasoning. Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distributed representations.



RQUGE:1.4517
 Q1:Why are non-linear functions used in neural networks?
Q2:What does image and speech recognition require?
Best ans: Input–output function to be insensitive to irrelevant variations of the input. Therefore, the final answer is to be sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive To large irrelevant variations.
['A0: Input–output function to be insensitive to irrelevant variations of the input. Therefore, the final answer is to be sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive To large irrelevant variations...', 'A1: Input–output function to be insensitive to irrelevant variations of the input. Therefore, the final answer is to be sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive To large irrelevant variations..', 'A2: Input–output function to be insensitive to irrelevant variations of the input. Therefore, the final answer is to be sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive To large irrelevant variations.', 'A3: Input–output function to be insensitive to irrelevant variations of the input. Therefore, the final answer is to be sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive To large irrelevant variations', 'A4: insensitivity', 'A5: insensitivity to large irrelevant variations.', 'A6: insensitivity to large irrelevant variations', 'A7: To properly adjust the weight vector']Text: Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights. To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.  Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. In practice, most practitioners use a procedure called stochastic gradient descent (SGD) This consists of showing the input vector for a few examples, computing the outputs and the errors, and computing the average gradient for those examples, and adjusting the weights accordingly. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.  Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces separated by a hyperplane19. Problems such as image and speech recognition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech. At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be similar to each other.  The chain rule of derivatives tells us how two small effects (that of a small change of x on y on y, and that of y on z) are composed. Substituting one equation into the other gives the chain rule. It also works when x, y and z are vectors (and the derivatives are Jacobian matrices) For simplicity, we have omitted bias terms. At each layer, we first compute the total input z to each unit, which is a weighted sum of the output of the units in the layer below. Then a non-linear function f(.) is applied to z to get the output.  Non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years. At each hidden layer we compute the error derivative with respect to the output of each unit. The error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk. raw pixels could not possibly distinguish the latter two.  Deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning. Each module (module) in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations.



