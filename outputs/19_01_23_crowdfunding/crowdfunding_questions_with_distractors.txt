--------------------------------------------------
TEXT: Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.

section 1 - (1) What do we present? --- A novel model for predicting the success of crowdfunding projects in meeting their funding goals.
 ['A novel data representation schema as framework', 'A novel model with a focus on brand, user profile and product aspects for supporting users to select successful funding channels from an available space via', 'A novel model for predicting product sales based on big social media text analytics', 'A novel, quantitative assessment', 'A novel perspective on social science and finance scholarship']
section 1 - (2) What do entrepreneurs use crowdfunding platforms for? --- To raise funds
 ['To whom and how did it occur', "To raise awareness about this issue, one first looks up information on their online forum called 'crowd21@company3", 'To help them choose appropriate ones and how are they able', 'To what extent does these processes contribute to corporate competitiveness, financial results', 'To raise money and know your future is just about doing business online 124 # You might try to work directly on an']
section 1 - (3) What dataset do we analyze? --- In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics. We analyze large dataset of crowdfunding project data, larger than reported in the art.
 ['In this question you have your choices for each possible answer with corresponding rationale as suggested below to get better understanding about these kind research questions', 'In this paper, we present a critical assessment on how best to use and select data from an ever expanding pool for project-level characterization studies by analyzing 2', 'In this paper, we present a new large scale study which tries out many approaches related', 'In this paper, we try to evaluate how using a rich and specific knowledge representation formalism  A Unified Framework for Knowledge Management System Research—Chall', 'In this paper, we present two complementary contributions for future work related with fundraiser analytics']
--------------------------------------------------
TEXT: In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.

section 2 - (1) What do we show about buzzwords and LIWC? --- Buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the highly correlates features with project’s success in fund raising.
 ['Buzzwords and LiweCl', 'Buzzwords and their relation to science as measured by semantic word vectors', "Buzzwords and Linguistic Inquiry and Word Count (LIWC) may serve as markers when assessing an organization's intentionality", 'Buzzwords and Linguistic Inquiry & Word Count can explain, among others things... what are relevant topics to invest money for; while not really making predictions', 'Buzzwords and Linguistic Inquiry and Word Count (LIWC) can provide helpful clues! Keywords2016 4, FebruaryDutch Wikipedia; Data-driven studiesThe']
section 2 - (2) What does this paper do? --- We use a semantic model to predict the Funding Success (FS) of a crowdfunding project.
 ['We use "what works for other companies look like,', 'We use machine learning models that are supervised, and can distinguish if someone is able or not to succeed given a description based on their previous', 'We use three case studies and four metrics with multiple hypotheses to understand what contributes most successfully for project-level financial performance by investors --', 'We use a semantic mapping technique based on graph-isomorphism tests and support vector machines regression trained via active machine learning to predict, for two datasets from', 'We use a semantic model to quantify and interpret crowd contributions for each possible event with respect 1245']
section 2 - (3) What do we use unique features to find? --- In this paper, we use unique features (for example, the use of buzzwords) to find characteristics that affect the success of a project in meeting its funding goal via crowdfunding. We use a combined model to predict the success.
 ['In this paper, two research approaches aim at investigating these complex questions from a qualitative perspective by using case-oriented interviews (N=', 'In this paper, we look into what kinds or dimensions help us select people for these online social capital-generating activities as measured from an information retrieval', 'In this question-andanswer forum We asked what you used from your experience while running and growing a fundraising campaign --', 'In this paper, we present two complementary research studies with a view on identifying and categorizing user-generated metadata within an established open repository as used for', "In this paper, we use unique feature approach proposed by Dang et al.. (2018) To show our method's performance quantitatively more"]
--------------------------------------------------
TEXT: In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.

section 3 - (1) What type of textual data is required for semantic analysis? --- Projects
 ['only technological projects', 'their project information', 'Studies', 'project’s semantic properties']
--------------------------------------------------
TEXT: The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.

section 4 - (1) What categories of buzzwords were used in this study? --- Using the buzzword data set, this study found buzzwords in the following categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.
 ['Using the data from two German studies, which explored people attitudes towards financial support', 'Using the same example given before, what are some interesting quotes on it that you know better-they would have written well if instead', 'Using the example “Cleaner Robot” to exemplify', 'Using the keyword “crowdsourced project"', 'Using the keywords as criteria it identified a total 248,510 entries and found all but ten which are not related']
--------------------------------------------------
TEXT: We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.

section 5 - (1) What did we use to extract features from the text? --- Language Inquiry and Word Count software tool
 ['word usage analysis', 'textual data', 'the topic–keyword distribution', 'dictionary words']
section 5 - (2) What did previous studies on crowdfunding use LIWC for? --- previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects.
 ['previous studies showed different outcomes regarding these measures and thus led us to make further observations as presented by Danesinamoğlu et al', 'previous studies 135 show which social phenomena occur more frequently around successful than non-successful # Table IVa Semantic features', 'previous studies failed to control what features should be used when performing machine-learne...the question that you will probably struggle finding --', 'previous studies, such as this study # Do we recommend a standard corpus to create ground truth annotation dataset(in NLP/data-', 'previous studies used Linguistic Inquiry and Word Count (LIWC) Software (LISCOR, available at https www liwockdownsample/) to code Project management']
--------------------------------------------------
TEXT: Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.

section 6 - (1) How many topics were in the All_D dataset? --- The All_D dataset had 30 topics. The Tech_D had 15 topics. and the Market_D was 15 topics for the All_d dataset. The total number of topics was 30. The final answer: 30.
 ['The All_D and Civic datasets provide us access to different aspects than those obtained using standard approaches, for both textual description-description similarity analysis', 'The All_D dataset had an intersection (of words) count ranging from one to a number that increased linearly for both, all-female datasets and', 'The All_D dataset had more different domains than a similar sample from 24 million open sourced software contributions, so we might ask is this simply caused', 'The All_D dataset had 30 topics. Which are some more relevant, which irrelevant ones for crowd funding analysis', 'The All_D dataset had five thousand categories but each category contains between one and fifty 1k20 images, for this task it could be more']
section 6 - (2) What did previous studies on crowdfunding use LDA for? --- Performing a topic analysis on the text of project updates.
 ['Performing sentiment analysis is necessary to answer these questions but there seems also a need from machine-learning (ML) approaches with which', 'Performing Principal Component Analysis (PCA) using text data would enable an easy comparison and identification with results presented herein by visualizing different variables, such as “Sports', 'Performing Latent Dirichlet Allocation (LDA) 264', 'Performing a systematic quantitative review and analysis is very complex as different ways to conduct their study lead many authors, reviewers et moderators with', 'Performing a topic analysis on 135 open funding campaigns led us into an investigation that could shed more light and offer practical conclusions than so far found']
section 6 - (3) How did we choose the topic model to use? --- Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose. the one that derived the highest coherency value and the smallest number of topics.
 ['Model perplexity 1354 for our own project set was lower', 'Model perplexity matters here as well!', 'Model perplexity, mutual information gain score', 'Model perplexity can lead researchers towards a particular type semantic space representation that could fit their problem better, so they might prefer...', 'Model perplexity and topic coherence provide valuable hints on what features should be selected when performing machine-learnt data filtering techniques such as Topic modelling, using 3']
section 6 - (4) Which datasets had the highest coherence values? --- All_D dataset All_d dataset Tech_d and Market_d
 ['All_D dataset "The entire corpus and all unique text samples for these labels that were collected from public funding databases on Facebook between March-', 'All_D dataset with only one concept category was related to better ranking than all categories together Keywordsinequality/uncertainty quantification-fairness and', 'All_D dataset All_d dataset Merged120K all 648,579 entries merged by Keras+Jieba into', 'All_D dataset, or at least all # of D (except NLI) for example-only model is preferred here... but', 'All_D dataset All_d dataset A05218A, ZPPR 3(F) BESIS-I E&O']
--------------------------------------------------
TEXT: We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.

section 7 - (1) How did we get the data we used? --- Data was extracted from projects’ posts via Python web scraping.
 ['Data was extracted retrospectively using two independent research studies which explored different aspects within financial literacy', 'Data Science Methods Applied to a Cross Sectorial Corporate Social Account Project', 'Data was extracted from different publications, but most studies just provide information that could not be retrieved by using our methods to analyze / extract specific', 'Data was extracted from projects’ "funds_perKwacha 154, which describes project funding per user-hour based on', 'Data collection, cleaning and transformation with open source tool']
section 7 - (2) Which of these features is not used to determine Funding Success (FS)? --- Previous created projects by.
 ['Previous evidence about this issue was collected and an integrative metaanalysis performed, considering all available data coming directly or indirectly associated with semantic factors', 'Previous research only investigates certain elements and, therefore does fail some aspects for which further explanation may improve our understanding on whether a specific factor', 'Previous literature shows that certain variables have shown significant results during various periods concerning different types or domains such as education institutions, technology startups', 'Previous knowledge about Funding Success (FS) and other social media campaigns could offer insights how successful crowdsourced efforts might fare for them based on prior research', 'Previous created projects data sets dont meet with newcomers since they cans be found only publicly and therefore lack a unique Funding Success (FS) value for']
section 7 - (3) How many previously created projects by one person was the answer to one of our analysis questions? --- One of our analysis questions was: How many projects were previously created?. The answer: 1. The number of previously created projects by one person was 1.
 ['One of our analysis results is that outof-batteries and failed requests account', 'One of our analysis question asks about an extreme value phenomenon involving what might intuitively be considered as a high frequency count data statistic where, for each contributor', 'One of our analysis "what kinds or themes do we come across for people using social capital on their project funding activities from an ethical perspective-', 'One of three answers may be used # {"project name1235 project count value(value0/v6', 'One of these two categories (semantic factors) have been analyzed at least as early is around five or a bit later already and']
--------------------------------------------------
TEXT: The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.

section 8 - (1) What are words that describe a feeling? --- Words that describe feeling include: awed. Awed is a feeling.
 ['Words or numbers about it—both possible mechanisms underpinning successful initiatives when people engage actively and creatively as part * 3', 'Words that describe "feelings and expectations from customers, investors) for which there aren’t clear goals nor desired outcomes after having', 'Words that describe an activity performed by another person', 'Words linked with project features and their relationship', 'Words that describe feeling include: funny, cool and nice']
section 8 - (2) What are some feelings words? --- Anxious, affectionate, anxious, awed, amazed, aggravated
 ['Anxious, affectionate, and afraid could play! Keywords:: In order to successfully fundraise successful project Crowdfund-ment. How can', 'Anxious, affectionate, anxious, awed, hopeful... but there has no specific meaning that could apply to many other emotions besides those included above Keywordsreform', 'Anxious, affectionate, proud or depressed – four types /15s that define what an emotional response might be for those who run successful', 'Anxious, affectionate, or depressed—a matter of meaning we can’t find!") was an article published to mark a milestone', 'Anxious, affectionate, guilty about me.... or love']
section 8 - (3) What are the words for awed and amazed? --- Addition to: feeling
 ['Addition to: Knowledge Discovery Based on Fuzzy Neural Inference - A Review', 'Addition or subtraction from an already huge project’s financial goals', 'Additionnal file 23/17954', 'Addition "amused with all\' being more related', "Addition to: Seminatie / Sociëtie d'information en scandinavian langsie, n o m é"]
section 8 - (4) What was the person explaining to someone because they were confused? --- The person was explaining something to someone because they were confused.
 ['The person was explaining one to someone because they were confused.', 'The person was explaining what to someone because they were confused.', 'The person was explaining a word to someone because they were confused.', 'The person was explaining posts to someone because they were confused.']
--------------------------------------------------
TEXT: We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.

section 9 - (1) What does the f-score measure? --- Model’s precision and recall are the same. F-score is the weighted harmonic mean of a model’s accuracy and recall. The final answer: accuracy.
 ['Model’s precision and recall reveal different types categories for funding proposals', 'Model’s precision and recall are the best indicator values that explain what type or scale for semantic evaluation is needed', 'Model’s precision and its performance comparison are important aspects we explore for this paper which makes use data sets from Reddit forum posts between August to', 'Model’s precision and recall can lead researchers towards a particular typeof false negatives that could occur during experiments with machine learning methods to detect risky behaviors when using', 'Model’s precision and accuracy']
--------------------------------------------------
TEXT: We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.

section 10 - (1) What did we develop? --- A combined Model for Funding Success (FS) in Kickstarter and Indiegogo projects.
 ['A combined semantic web, social science analysis', 'A combined corpus study', 'A combined Model for understanding successful social entrepreneurship', 'A combined Model for Funding Success (FS) Using Machine Learning (MaLiS_f) approach', 'A combined experimental and longitudinal research approach☆']
section 10 - (2) What was the test that was used to evaluate the prediction performance? --- Datasets were obtained from Kaggle.com. We used Beautifulsup to extract additional metadata features. The data was re-scraped using Beautifulsdown. We reconstructed the data into three datasets: All_D (all Kickstarter projects), Tech_D(Tech_D projects) and Market_D. The model was trained on the datasets. The models were tested on a 10-fold cross validation test.
 ['Datasets were obtained from an independent third-person, which can bias some analyses but is not reported by previous researchers who use these resources when performing analysis', 'Datasets were obtained from Kaggle.com. We downloaded 15 data frames, and one label for each row which indicated if this column is present or absent         X', 'Datasets were too different from our approach # 2486 dataset includes only five target keywords among other possible combinations for each query', 'Datasets were obtained from three well know datasets for a variety and number types [ How Good Are Popularity Metrics Ensued When it comes', 'Datasets were obtained from Kaggle.com. We use different models including support, rule extraction algorithms for predicting project results based on data sets collected across a time-interval']
section 10 - (3) What are the three datasets used to study the relationship between features and project category? --- All_D, Market_D and Tech_D datasets.
 ['All_D, only those which did not contain an explicit <http> attribute; Pix36k-Pilatescreen', 'All_D, Market_D and Eskimo (a dataset made for my doctoral students research) were provided anonymously while our own data had no', 'All_D, a corpus where 16209 text instances for every class were provided with its own train validation test split... Yes', 'All_D, Market_D and D2T1 all consist with \\(674k+530^{rd}-century', 'All_D, YG-Bridge or Banco de Santander "Espadaña']
section 10 - (4) What did we develop to validate the results of our study? --- To study Funding Success (FS), we used the following features: Buzzwords, Linguistic Inquiry, Word count and feeling words. We used the CFS (correlation-based feature selection) algorithm to select the most Significant Set of Features (MSSF). We performed a 10-fold cross-validation test to evaluate the model’s performance. We developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.
 ['To study their relation, you would', 'To study these two open scientific questions with new data, which method can make up an approach that takes them correctly and provide reliable measurements for', 'To study why different semantic categories matter # 2) In which ways might it become easier for people interested or concerned with a particular', 'To study "what works for other companies doing similar research...in any area, including science and technology; why not just start with', 'To study Funding Success (FS), we used data from two independent studies collected online through SurveyMonkey® which gathered information on more than one million US users between August and']
section 10 - (5) What datasets were used to study the relationship between features and project category? --- In this paper we combine semantic and meta-data features to predict Funding Success (FS) using a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc, for each record in the dataset. For each record, the values of five metadata features and about 120 semantic features, including buzzwords. We removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing topic sets. To study the relationship between features and project category, we built three datasets: All_D, Tech_D and Market
 ['In this question authors have found that different types, formats or ways 12450 can be transformed into numerical value had an', 'In this section it would be convenient if a classification was done instead; but no such paper has been found yet 7/12', 'In this review only 4% had enough detail data related for researchers using that dataset which may impact analysis on why an article found significance', 'In this paper we present two complementary research studies with new data on over 7,062 high‐interest or large financial goal fundings', 'In this paper we combine semantic network theory, graph analysis with traditional supervised algorithms for machine learning models evaluation based on different textual feature sets extracted via Text Analytics software']
--------------------------------------------------
--------------------------------------------------
TEXT: In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.

section 12 - (1) Which datasets has a unique parameter WC? --- R_Tech _D datasets
 ['R_Tech Science Economics 23/109 available that could influence it positively by reducing their initial investment before they reach some point', 'R_Tech _D, F\\(>12*{logP[d]}/5-03769...$', 'R_Tech _D17520 PERSECENCE AND CITYSCOPY TRENDS IN CHALLENGE-ORIENT', 'R_Tech "The research is carried out within Russian State Research Programme for Innovative Projects on Information Security Protection; funding received to support this', 'R_Tech _Datasets can provide us with all those parameters']
section 12 - (2) What does the CFS algorithm evaluate? --- CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.
 ['CFS algorithm 1 is related to social trust and project risk; while', 'CFS algorithm "featurizes and ranks user data, creating profiles for all users to share among one another while ranking candidates based on', 'CFS algorithm evaluates subsets by measuring semantic similarity for entities and predicates 26%, how closely connected is their underlying relationship network to another related social', 'CFS algorithm can achieve up 243 points per single campaign', 'CFS algorithm evaluates subsets of features; however, it may not consider all available metadata to be valuable as only few data instances are labelled with certain categories (']
section 12 - (3) What features are most influential for Funding Success? --- This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. We find that the following feature are most influential for Funding Success (FS): buzzwords Linguistic Inquiry Word Count Word %
 ['This paper looks deep into these topics, which can only improve our understanding how such a successful project occurs to better serve other similar cases by', 'This paper describes the data setup, measures used and research model applied as well to explore some aspects regarding which semantic fields that project funding is successful or unsuccessful based on', 'This paper describes the data setup, pre-selection process and analysis that resulted on identifying some predictive factors to achieve an accurate forecasting tool which can provide decision makers with', 'This paper focuses on explaining how characteristics and events within a user can be measured using supervised statistical machine-learning model, by analyzing 2', 'This paper describes the results from two empirical studies, which explored different aspects that play a potential positive or negative influence on Fund-mentSuccess by']
section 12 - (4) What does this paper describe? --- This paper describes the feature selection process for Funding Success (FS).
 ['This answer is too simple indeed for several different perspectives from those described', 'This paper describes the feature 13 on page five to our project description for a successful start-up firm and discusses what could possibly influence us positively', 'This paper describes an exploratory study that analysed, compared and contextualized data obtained from online platforms like GoFundMe using NLP to answer a specific', 'This paper describes the feature importance approach and uses three types Of features (code length, number lines per code) as dependent on which metrics to predict', 'This page will only look at project description']
--------------------------------------------------
TEXT: For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.

section 13 - (1) On what dataset was the proposed model evaluated? --- The proposed models were evaluated on the following datasets: All_D.
 ['The same as Figure 3 # In which environment can an author provide their code and other necessary data or information about it for', 'The data set provided by Kaggle and an alternative proposal is detailed, along with a discussion how those two datasets fit as semantic web', 'The proposed models 1 are highly data-centered and they focus on how successful crowdsourced efforts might fare for specific products from scratch which', 'The proposed models were evaluated based on two external datasets collected online through different sources than those from where Crowfunders are recruited, for which no official data', 'The proposed models were evaluated using four different corpora and with two types (unintended positive results, unintentional losses) on each one obtained from']
section 13 - (2) What machine learning algorithms were used? --- To develop the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D.
 ['To develop, implement and apply a model allowing identifying which types of problems crowdfund project owners may have with respect to their choice for', "To develop new techniques for assessing whether users' motivations affect people", 'To develop your first prediction model how would you know if I am right and what impact it might have upon my career development...or is', 'To develop their predictive ability to classify', 'To develop the Semantic-Model, and its performance comparison with others Keywords:: Artificial neural networks ANNs Crowfunders Deep Neural Networks LSTM Network Architecture Models Comparison']
section 13 - (3) What were semantic features used for? --- To develop the Semantic-Model, semantic features were used as input.
 ['To whom did it concern you', 'To develop the proposed theory, five researchers with at least 10% and no less than half academic credit independently analyzed text from nine real', 'To evaluate them what are other factors affecting its effect over', 'To develop the appropriate approach to support a fair comparison between different crowdbased initiatives across multiple data streams and time series platforms, * How can', 'To develop the Semantic-Model, semantic knowledge  Semantics based web intelligence and information system building methodology–The approach to Web Intelligence with Sentient Model/S']
section 13 - (4) On what datasets did we utilize machine learning algorithms? --- All_D datasets.
 ['a much smaller and very specific dataset.', 'R_All_D.', 'its general data structure.', 'the all records.']
section 13 - (5) What datasets were utilized in this study? --- In this study, we utilized three datasets: All_D. The Semantic-Model was developed on the datasets All_d.
 ['In this study, we present two novel contributions for future work related to fundraiser research', 'In this study, we utilized three publicly available social science dataset with different types for measuring people’s attitude and sentiment related to 86 nonfinance companies', 'In this question, two research approaches can be recognized (i) to analyze a particular phenomenon through literature-driven methodologies or;', 'In this study, we utilized three types or corpus (12K-word dataset; NLI annotated project descriptions as a text classifier challenge benchmark set for', 'In this study, we utilized three popular scientific data sources with different types / scales that would facilitate semantic analyses across various platforms at an extensive scale-up using']
section 13 - (6) What was the result of the LightGBM algorithm? --- A model was developed that utilizes a combination of several machine learning algorithms. The model was able to achieve a high accuracy level for all three datasets.
 ['A model was developed for a successful crowd financing strategy based, first step at 1240586-m7h on an', 'A model evaluation approach from an empirical perspective', 'A model for measuring semantic similarity among entities and an assessment by crowd sourers', 'A model was developed with lightgbmL2 R package', 'A model with accuracy and recall values between 50–92% using cross-project feature combinations as well multitask optimization technique']
--------------------------------------------------
TEXT: The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.

section 14 - (1) How was the training performed? --- Using a set of features we use for prediction and the dataset on which learning was applied, we trained a model on All_D with 10-fold cross_validation.
 ['Using a survey with two independent studies, which showed different attitudes on this issue', 'Using a data mining process as framework with textual analysis tool', 'Using a novel approach on an emerging funding mechanism - An interview', 'Using a set of crowdsourcing and humanities experts, their views are examined concerning project metadata', 'Using a set of features we were able to determine some common factors influencing performance during pre-training, while others depended more importantly on contextual circumstances such as investor']
section 14 - (2) What does the figure show? --- Model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the Latent Dirichlet Allocation (LDA)-Model and the Metadata-Model.
 ['Model we present as an illustration for such cases -to our hospitals it was more than anything to go green on Christmas', 'Model we developed has "what looks good and bad from start to finish gets worse over time... that\'s because people think all they would', 'Model we developed has helped to determine how best # - A model on fund raising models for online crowdsourcing based products by Dr Kran', 'Model we developed has been replicated on two additional data and validated with further statistical approach showing how semantic similarity is predictive', 'Model we developed has a good ability to classify both project types, namely with an average accuracy greater than 96% on test cases obtained out']
--------------------------------------------------
TEXT: The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.

section 15 - (1) What feature is highly correlated to FS? --- Buzzwords feature
 ['The semantic features buzzwords feature', 'the topic–keyword distribution feature', 'terms feature', 'semantics feature']
