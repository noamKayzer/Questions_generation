(0)TAKEN?True RQUGE:5.1053
Q1:how were project's posts analyzed in this article
Q2:How did we get the data we used?
Best ans: Data was extracted from projects’ posts via Python web scraping.
['A0: Python web scraping.', 'A1: Python web scraping', 'A2: Projects’ posts were analyzed for metadata features known to affect FS', 'A3: Data was extracted from projects’ posts via Python web scraping.', 'A4: by.', 'A5: by', 'A6: web scraping', 'A7: via Python web scraping']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(1)TAKEN?False RQUGE:5.0934
Q1:how were project's posts analyzed
Q2:How did we get the metadata for the projects?
Best ans: Metadata was extracted from projects’ posts via Python web scraping.
['A0: Python web scraping.', 'A1: Python web scraping', 'A2: Data was extracted from projects’ posts via Python web scraping.', 'A3: Metadata was extracted from projects’ posts via Python web scraping.', 'A4: by.', 'A5: by', 'A6: web scraping', 'A7: via Python web scraping']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(2)TAKEN?False RQUGE:5.083
Q1:Do buzzwords predict Funding Success (FS) for crowdfunded projects?
Q2:is buzzwords related to funding success
Best ans: Yes
['A0: We developed a model to predict Funding Success (FS) for crowdfunding projects. We developed the model based on semantic features only. We also developed an impressive Fscore of 96.2%, which is highly relevant to crowdfunded projects.', 'A1: We developed a model to predict Funding Success (FS) for crowdfunding projects. We developed the model based on semantic features only. We also developed an impressive Fscore of 96.2%, which is highly relevant to crowdfunded projects. Future research could further improve accuracy by considering the characteristics of images and video content.', 'A2: Crowdfunded projects are crowdfunded projects. Buzzwords are used in crowdfunding campaigns to describe projects. The buzzwords feature is among the features that are highly correlated to Funding Success (FS) compared to previous parameters that we examined and by other researchers. We developed a model based on semantic features only and achieved similar accuracy level as previous studies.', 'A3: Funding Success (FS) is a measure of the success of crowdfunded projects. Buzzwords are highly correlated to FS. We developed a model based on semantic features only and achieved similar accuracy level as previous studies.', 'A4: high', 'A5: no', 'A6: highly correlated', 'A7: Yes']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(3)TAKEN?True RQUGE:5.0606
Q1:How to predict crowdFS?
Q2:What do we present?
Best ans: A novel model for predicting the success of crowdfunding projects in meeting their funding goals.
['A0: Model for CrowdFS.', 'A1: Model for CrowdFS', 'A2: The paper presents a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A3: Using large dataset of crowdfunding project data, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%.', 'A4: Modeling CrowdFS', 'A5: model for crowdfunding projects in meeting their funding goals', 'A6: A novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A7: A novel model for predicting the success of crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(4)TAKEN?False RQUGE:5.0507
Q1:Are there any other feelings that you can deduce from the following statement?
Q2:is there a word for a feeling in the english language
Best ans: Yes
['A0: Angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, amazed.', 'A1: Angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, amazed', 'A2: Absurdity is a feeling that is not explained. Awed is derived from the word awe.', 'A3: The sentence is about explaining something. The sentence has no other feelings. The answer: no.', 'A4: alarmed', 'A5: No', 'A6: Yes', 'A7: yes']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(5)TAKEN?True RQUGE:5.0146
Q1:Which tool was used to extract features from the text?
Q2:What did we use to extract features from the text?
Best ans: Language Inquiry and Word Count software tool
['A0: Word Count software', 'A1: Word Count software tool', 'A2: Language Inquiry and Word Count software tool', 'A3: The Linguistic Inquiry and Word Count (LIWC) software tool was used.', 'A4: Language Inquiry', 'A5: Linguistic Inquiry and Word Count (LIWC) analysis', 'A6: LIWC', 'A7: Word Count software']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(6)TAKEN?False RQUGE:5.0146
Q1:Which tool was used to extract features from the text?
Q2:What did we use to extract features from the text?
Best ans: Language Inquiry and Word Count software tool
['A0: Word Count software', 'A1: Word Count software tool', 'A2: Language Inquiry and Word Count software tool', 'A3: The Linguistic Inquiry and Word Count (LIWC) software tool was used.', 'A4: Language Inquiry', 'A5: Linguistic Inquiry and Word Count (LIWC) analysis', 'A6: LIWC', 'A7: Word Count software']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(7)TAKEN?False RQUGE:4.9473
Q1:What is the name of the software tool used to extract features from the text?
Q2:What did we use to extract features from the text?
Best ans: Linguistic Inquiry and Word Count (LIWC) software tool
['A0: LIWC', 'A1: Linguistic Inquiry and Word Count (LIWC) software tool', 'A2: We used the Linguistic Inquiry and Word Count (LIWC) software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The output is VD = SD/NT, which is the value of the feature that corresponds to D. LIWCT', 'A3: Linguistic Inquiry and Word Count (LIWC) software tool is used to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. LIWC analysis measures dictionary words.', 'A4: Linguistic Inquiry and Word Count (LIWC) analysis', 'A5: Language Inquiry', 'A6: Word Count', 'A7: LIWC']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(8)TAKEN?False RQUGE:4.9139
Q1:Who used metric's to measure the performance of their models.
Q2:Who uses metrics to measure the performance of our models?
Best ans: us
['A0: Models are used by people to measure performance. We used metrics to measure the performance.', 'A1: Our used metrics to measure the performance of our models.', 'A2: A model’s performance is measured by F-score (also denoted F1). We used metrics.', "A3: Using metric's is a way to measure the performance. We used metrics to measure performance of our models. The final answer: We.", 'A4: the', 'A5: us', 'A6: company', 'A7: our models']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(9)TAKEN?False RQUGE:4.9
Q1:A novel model for crowdfunding project FS
Q2:What do we present?
Best ans: A novel model for crowdfunding project Funding Success (FS).
['A0: Model for crowdfunding project FS', 'A1: Using semantic features, we analyze both metadata and project semantics. We show that when combining metadata and semantics, we arrive at F1 score accuracy of 96.2%.', 'A2: A novel model for crowdfunding project Funding Success (FS).', 'A3: A novel model for crowdfunding project FS', 'A4: Modeling Crowdfunding Project FS', 'A5: a novel model for crowdfunding project Funding Success (FS).', 'A6: A novel model for crowdfunding project Funding Success (FS).', 'A7: a novel model for crowdfunding project FS']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(10)TAKEN?False RQUGE:4.8389
Q1:Does the Buzz Word dataset contain words from different categories?
Q2:is the funding success study using the buzzword dataset
Best ans: Yes, this study uses the buzzword dataset. The buzzwords are from different categories. The final answer: yes.
['A0: Yes, this study uses the buzzword dataset. The buzzwords are from different categories. The final answer: yes.', 'A1: There are different categories of buzzwords. The buzzword dataset used in this study contains words from different classes. The final answer: yes.', 'A2: Buzzwords are words that are used in conversation. The buzzword dataset used in this study contains words from different categories. The final answer: yes.', 'A3: ', 'A4: yeah', 'A5: no', 'A6: Yes', 'A7: yes']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(11)TAKEN?False RQUGE:4.8356
Q1:What is the ratio of correctly predicted positive records to the all records in actual class?
Q2:What is the ratio of correctly predicted positive records to the all records in actual class?
Best ans: Recall
['A0: A model’s recall is the ratio of correctly predicted positive records to all records in actual class. Recall and F1 are the weighted harmonic mean of the model’ precision and recall. F1 is the weighting harmonic mean.', 'A1: A model’s recall is the ratio of correctly predicted positive records to all records in actual class. Recall and F1 are the weighted harmonic mean of the model’ precision and recall. F1 is the weighting harmonic mean. So, the final answer is F1.', 'A2: In order to measure the performance of our models, we used the metrics to measure performance of the models: recall, precision, and F-score. Recall is the ratio of correctly predicted positive records to all records in actual class. F-Score is the weighted harmonic mean of a model’s precision and recall.', 'A3: Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score is the weighted harmonic mean of a model’s precision and recall. F1 is the Weighted Harmonic Mean of Optimum and Minimum Precision.', 'A4: remember', 'A5: Remember', 'A6: Recall', 'A7: recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(12)TAKEN?False RQUGE:4.8339
Q1:What is the main input to the Latent Dirichlet Allocation (LDA) topic model?
Q2:Along with the dictionary and the number of topics, what is the third input to the LDA
Best ans: the corpus
['A0: dictionaries', 'A1: the corpus', 'A2: Dictionary', 'A3: dictionary', 'A4: corpus', 'A5: vocabulary', 'A6: the dictionary', 'A7: dictionary']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(13)TAKEN?True RQUGE:4.8315
Q1:How many topics did the Latent Dirichlet Allocation (LDA) model have?
Q2:How many topics were in the All_D dataset?
Best ans: The All_D dataset had 30 topics. The Tech_D had 15 topics. and the Market_D was 15 topics for the All_d dataset. The total number of topics was 30. The final answer: 30.
['A0: The All_D dataset had 30 topics. The Tech_D had 15 topics. and the Market_D was 15 topics for the All_d dataset. The total number of topics was 30.', 'A1: The All_D dataset had 30 topics. The Tech_D had 15 topics. and the Market_D was 15 topics for the All_d dataset. The total number of topics was 30. The final answer: 30.', 'A2: Using Gensim Python package Latent Dirichlet Allocation (LDA), we used 30 topics for the All_D dataset, 15 for the Tech_D and 15 for Market_D. The final answer: 15.', 'A3: There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D. The final answer: 30. The', 'A4: thirty', 'A5: fifteen', 'A6: 30 topics', 'A7: 15']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(14)TAKEN?True RQUGE:4.7991
Q1:Which of these features is not used to determine Funding Success (FS)?
Q2:Which of these features is not used to determine Funding Success (FS)?
Best ans: Previous created projects by.
['A0: Previous created projects by.', 'A1: In order to determine Funding Success (FS), we used the following features:', 'A2: Previous created projects by. are not used to determine Funding Success (FS). The final answer: previous created projects b.', 'A3: Previous created projects by. are not used to determine Funding Success (FS).', 'A4: video.', 'A5: video', 'A6: updates', 'A7: videos']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(15)TAKEN?True RQUGE:4.7587
Q1:Why is crowdfunding important?
Q2:What do entrepreneurs use crowdfunding platforms for?
Best ans: To raise funds
['A0: A new model for crowdfunding projects in meeting their funding goals.', 'A1: This paper presents a novel model for predicting the crowdFS of crowdfunding projects in meeting their funding goals.', 'A2: Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of crowdfunding campaigns. We present a novel model for crowdfunding projects in meeting their funding goals', 'A3: We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: To raise funds', 'A5: The question of what influences projects’ fundraising success is very important', 'A6: What influences projects’ fundraising success is very important.', 'A7: What influences projects’ fundraising success is very important']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(16)TAKEN?False RQUGE:4.7579
Q1:Did Funding Success (FS) have any relationship with Buzz Words?
Q2:did they look at the buzzwords in the funding success dataset
Best ans: Buzz words are words that are used in the description of the project. The buzzword dataset used in this study contains words from different categories: general conversation. The Buzzword dataset was not examined. The final answer: no.
['A0: There are different buzzwords in the buzz word dataset. The buzzword dataset uses words from different categories. The Buzz Words dataset is not examined in this study. So, the final answer is no.', 'A1: Buzz words are words that are used in the description of the project. The buzzword dataset used in this study contains words from different categories: general conversation. The Buzzword dataset was not examined. The final answer: no.', 'A2: Funding Success (FS) is a project. Buzz Words are words used in the description of the project. The buzzword dataset used in this study contains words from different categories. So, the final answer is no.', 'A3: ', 'A4: not', 'A5: Yes', 'A6: yes', 'A7: No']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(17)TAKEN?True RQUGE:4.6141
Q1:Which features are associated with crowdfunding project success?
Q2:What do we show about buzzwords and LIWC?
Best ans: Buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the highly correlates features with project’s success in fund raising.
['A0: This work uses the following features: Buzzwords Linguistic Inquiry Word Count Buzzword use', 'A1: This work uses the following features: Buzzwords Linguistic Inquiry Word Count', 'A2: Buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the highly correlates features with project’s success in fund raising.', 'A3: We use a more comprehensive method for predicting crowdfunding project success. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. We show that buzzword and Linguistic Inquiry and Word Count (LIWC) are among the highly correlate features with the project’s success in fund raising.', 'A4: Buzzwords', 'A5: language and buzzwords', 'A6: linguistic inquiry Word Count', 'A7: word count']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(18)TAKEN?True RQUGE:4.57
Q1:Which feature is highly correlated to Funding Success (FS)?
Q2:What feature is highly correlated to FS?
Best ans: Buzzwords feature
['A0: buzzwords', 'A1: Project Description Semantic features', 'A2: Buzzwords', 'A3: Buzzwords feature', 'A4: hyped', 'A5: hype', 'A6: Buzz words', 'A7: buzzwords feature']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(19)TAKEN?True RQUGE:4.518
Q1:How many projects were created by the same person?
Q2:How many previously created projects by one person was the answer to one of our analysis questions?
Best ans: One of our analysis questions was: How many projects were previously created?. The answer: 1. The number of previously created projects by one person was 1.
['A0: One of our analysis questions was: How many projects were previously created?. The answer: 1.', 'A1: One of our analysis questions was: How many projects were previously created?. The answer: 1. The number of previously created projects by one person was 1.', 'A2: We used the following features: The number of photos, videos, the number. of updates, the. number of previously created projects by. The number. and number of previous created projects.', 'A3: We used the following features: The number of photos, videos, the number. of updates, the. number of previously created projects by. The number. and number of previous created projects. were extracted from projects’ posts.', 'A4: 0', 'A5: one', 'A6: once', 'A7: multiple']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(20)TAKEN?True RQUGE:4.5164
Q1:In which datasets were the features evaluated?
Q2:Which datasets has a unique parameter WC?
Best ans: R_Tech _D datasets
['A0: R_Tech _D datasets', 'A1: R_Tech _D', 'A2: R_Tech _D datasets were used.', 'A3: R_Tech _D dataset', 'A4: All', 'A5: datasets', 'A6: CFS', 'A7: CFS algorithm']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(21)TAKEN?True RQUGE:4.4352
Q1:Why did they use the Linguistic Inquiry and Word Count (LIWC) analysis?
Q2:What did previous studies on crowdfunding use LIWC for?
Best ans: previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects.
['A0: previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects.', 'A1: previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects', 'A2: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text', 'A3: We used Linguistic Inquiry and Word Count (LIWC) to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text.', 'A4: measure the appearance of dictionary words', 'A5: to analyze the textual description part of projects.', 'A6: To extract features from the text', 'A7: extract features from the text']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(22)TAKEN?False RQUGE:4.3647
Q1:Is buzzwords an important feature of Funding Success (FS)?
Q2:What is the relevance of the results of this study?
Best ans: Funding Success (FS) refer to the success of crowdfunding campaigns. The buzzwords feature is among the features that are highly correlated to FS. The results are high relevant to fundraisers using crowdfunding web platforms and could be further improved by considering the characteristics.
['A0: Buzzwords are among the features that are highly correlated to Funding Success (FS). The buzzwords feature is among the most important features in FS. The buzz words feature is highly influenced by the crowdfunding platform.', 'A1: Funding Success (FS) refer to the success of crowdfunding campaigns. The buzzwords feature is among the features that are highly correlated to FS. The results are high relevant to fundraisers using crowdfunding web platforms and could be further improved by considering the characteristics.', 'A2: The buzzwords feature is among the features that are highly correlated to Funding Success (FS). The buzz words feature is a feature that is highly reflected in the FS model. The buzz word feature is highly relevant to fundraisers using crowdfunding web platforms.', 'A3: Funding Success (FS) refer to the success of crowdfunding campaigns. The buzzwords feature is among the features that are highly correlated to FS. The results are high relevant to fundraisers using crowdfunding web platforms and could be further improved by considering the characteristics. of images, video content and the semantics of video scripts.', 'A4: FS', 'A5: high', 'A6: Yes', 'A7: yes']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(23)TAKEN?True RQUGE:4.3467
Q1:In which datasets were the proposed models evaluated?
Q2:On what dataset was the proposed model evaluated?
Best ans: The proposed models were evaluated on the following datasets: All_D.
['A0: In this paper, we evaluated proposed models on All_D dataset.', 'A1: The proposed models were evaluated on the following datasets: All_D. The Semantic-Model was evaluated on datasets with semantic features:', 'A2: We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D.', 'A3: The proposed models were evaluated on the following datasets: All_D.', 'A4: All_D.', 'A5: datasets', 'A6: Combined model', 'A7: all three datasets']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(24)TAKEN?False RQUGE:4.3233
Q1:How was Linguistic Inquiry and Word Count (LIWC) used to analyze textual description part of projects?
Q2:What was LIWC used for in previous studies?
Best ans: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. The output of LIWC is VD = SD/NT. LIWC was used to analyze textual description part of projects in previous studies.
['A0: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. The output of LIWC is VD = SD/NT.', 'A1: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. The output of LIWC is VD = SD/NT. LIWC was used to analyze textual description part of projects in previous studies.', 'A2: The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy. Linguistic Inquiry and Word Count (LIWC) analyzes the appearance of dictionary words in a specific text.', 'A3: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. LIWC analyzes the degree to which the text being analyzed is related to the theme of the dictionary. LIWCT analysis measures a numeric value in the range [0.1]. LIWC is used to analyze textual description part of projects.', 'A4: extracted features', 'A5: extraction', 'A6: software tool', 'A7: to extract features from the text']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(25)TAKEN?True RQUGE:4.3084
Q1:The authors used Latent Dirichlet Allocation (LDA) for what purpose?
Q2:What did previous studies on crowdfunding use LDA for?
Best ans: Performing a topic analysis on the text of project updates.
['A0: topic analysis', 'A1: to analyze text of project updates', 'A2: Performing a topic analysis on the text of project updates.', 'A3: Previous studies on crowdfunding used Latent Dirichlet Allocation (LDA) to perform topic analysis on the text of project updates.', 'A4: performance', 'A5: conduct research', 'A6: research', 'A7: research on crowdfunding']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(26)TAKEN?True RQUGE:4.2281
Q1:Where did this study find buzzwords?
Q2:What categories of buzzwords were used in this study?
Best ans: Using the buzzword data set, this study found buzzwords in the following categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.
['A0: This study uses a buzzword dataset. The buzzword data is from different categories: general conversation. The data is sorted by category. The final answer: general discussion.', 'A1: Using the buzzword data set, this study found buzzwords in the following categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.', 'A2: In this study, buzzwords were found in the following different categories: general conversation. The buzzword dataset used in this study contains words from different categories. The Buzzword dataset is a collection of words from various categories.', 'A3: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. The buzz word dataset used is a list of words from various categories.', 'A4: business', 'A5: education', 'A6: the buzzword dataset', 'A7: science and technology']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(27)TAKEN?True RQUGE:4.1971
Q1:How was the model trained?
Q2:How was the training performed?
Best ans: Using a set of features we use for prediction and the dataset on which learning was applied, we trained a model on All_D with 10-fold cross_validation.
['A0: In this study, the model was trained on All_D with 10-fold cross_validation', 'A1: Using a set of features we use for prediction and the dataset on which learning was applied, we trained a model on All_D with 10-fold cross_validation.', 'A2: Models were developed with the algorithms that were used in the studies above, and with additional algorithms. This training was performed on All_D with 10-fold cross_validation.', 'A3: The model was trained on All_D with 10-fold cross-validation. The model we developed has the highest accuracy.', 'A4: 10-fold cross-validation)', 'A5: 10-fold cross-validation', 'A6: performed on All_D', 'A7: trained on All_D with 10-fold cross-validation']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(28)TAKEN?False RQUGE:4.1671
Q1:What is the purpose of this paper?
Q2:What is the main focus of this paper?
Best ans: Research on the Funding Success (FS) of Crowdfunding Projects.
['A0: Identifying Semantic Features that Influence the Success of Crowdfunding Projects', 'A1: Research on the Funding Success (FS) of Crowdfunding Projects.', 'A2: Research on the Funding Success (FS) of Crowdfunding Projects', 'A3: A new approach to studying and predicting Funding Success (FS) of crowdfunding projects.', 'A4: Modeling Funding Success (FS) of Crowdfunding Projects.', 'A5: to predict Funding Success (FS) of crowdfunded projects', 'A6: Modeling Funding Success (FS) of Crowdfunding Projects', 'A7: Research on the relationship between crowdfunding projects and their FS']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(29)TAKEN?False RQUGE:4.1595
Q1:Which features are most correlated with Funding Success (FS)?
Q2:What is the novelty of the combined model?
Best ans: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). The model was trained on the following datasets: All_D (Kickstarter) and Market_D. The model performed well on the FS task. The combined model was developed to combine the semantic and meta-data features.
['A0: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). The model was trained on the following datasets: All_D (Kickstarter) and Market_D. The model performed well on the FS task.', 'A1: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). The model was trained on the following datasets:', 'A2: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). The model was trained on the following datasets: All_D (Kickstarter) and Market_D. The model performed well on the FS task. The combined model was developed to combine the semantic and meta-data features.', 'A3: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). The model was trained on the following datasets: All_D (Kickstarter) and Market_D. The model performed well on the FS task. The combined model was developed to combine the semantic and meta-data features. The models were tested on a 10-fold cross-validation test.', 'A4: buzzword', 'A5: CFS', 'A6: The combined-model', 'A7: Buzzwords']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(30)TAKEN?False RQUGE:4.1434
Q1:What influences crowdfunding projects’ fundraising success?
Q2:What do we analyze
Best ans: Our model for predicting the crowdFS of crowdfunding projects focuses on semantic features only. In an additional model we developed, we examine both project metadata and project semantics, providing a comprehensive study of factors. We analyze a large dataset of crowdfunding project data, larger than reported in the art, and demonstrate higher accuracy of our model.
['A0: A novel model for crowdfunding projects’ fundraising success.', 'A1: Using semantic features, we propose an additional model that incorporates both project metadata and project semantics. We analyze a large dataset of crowdfunding project data, larger than reported in the art.', 'A2: Our model for predicting the crowdFS of crowdfunding projects focuses on semantic features only. In an additional model we developed, we examine both project metadata and project semantics, providing a comprehensive study of factors. We analyze a large dataset of crowdfunding project data, larger than reported in the art, and demonstrate higher accuracy of our model.', 'A3: Model for crowdfunding projects’ fundraising success. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals based on semantic features. We analyze a large dataset of crowdfunding project data, larger than reported in the art.', 'A4: semantic features and metadata.', 'A5: semantic features', 'A6: semantic features and metadata', 'A7: model for crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(31)TAKEN?False RQUGE:4.1116
Q1:What is the relationship between Funding Success (FS) and buzzwords?
Q2:What is the relevance of this study?
Best ans: This study investigates the relationship between Funding Success (FS) and buzzwords. The buzzword feature is among the features that are highly correlated to FS. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.
['A0: This study investigates the relationship between Funding Success (FS) and buzzwords. The buzzword feature is among the features that are highly correlated to FS.', 'A1: This study investigates the relationship between Funding Success (FS) and buzzwords. The buzzword feature is among the features that are highly correlated to FS. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.', 'A2: Buzzwords feature is among the features that are highly correlated to Funding Success (FS) compared to other parameters that we examined and by other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies.', 'A3: We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. The buzzwords feature is among the features that are highly correlated to Funding Success (FS) compared to other parameters that we examined and by other researchers examined.', 'A4: highly', 'A5: a novel model', 'A6: high correlation', 'A7: are highly correlated']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(32)TAKEN?True RQUGE:4.0534
Q1:A combined model for Funding Success (FS) in Kickstarter and Indiegogo projects
Q2:What did we develop?
Best ans: A combined Model for Funding Success (FS) in Kickstarter and Indiegogo projects.
['A0: The proposed model is based on the combination of semantic and meta-data features to predict Funding Success (FS) in Kickstarter and Indiegogo projects. The proposed models are based upon the following:', 'A1: To study correlation between features and project category, we built three data sets: All_D, Tech_D and Market_D. The original feature space was reduced into three Principal Components (PCs). The model’s design, derivation and operational tests were conducted using the Python packages: scikit-learn, scikit_learnd, scikis-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the performance.', 'A2: Our dataset is a combination of Kickstarter and Indiegogo projects. We obtained a dataset of 5,000,000 projects. The dataset was divided into three datasets: All_D, Tech_D and Market_D. The original features space was reduced into three Principal Components (PCs). The model’s design and derivation were conducted using the Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the performance.', 'A3: A combined Model for Funding Success (FS) in Kickstarter and Indiegogo projects.', 'A4: Modeling Funding Success (FS) in Kickstarter and Indiegogo Projects', 'A5: a combined model for Funding Success (FS) in Kickstarter and Indiegogo projects.', 'A6: a combined model for Funding Success (FS) in Kickstarter and Indiegogo projects', 'A7: A combined model for Funding Success (FS) in Kickstarter and Indiegogo projects.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(33)TAKEN?False RQUGE:4.0534
Q1:A combined model for Funding Success (FS) in Kickstarter and Indiegogo projects.
Q2:What did we develop?
Best ans: A combined Model for Funding Success (FS) in Kickstarter and Indiegogo projects.
['A0: The proposed model is based on the combination of semantic and meta-data features to predict Funding Success (FS) in Kickstarter and Indiegogo projects. The proposed models are based upon the following:', 'A1: In this paper, we combine the following features: buzzwords, sentimental words, feeling words, explanation words, and Latent Dirichlet Allocation (LDA) topics. The model is based on the LDA topic coherence score. The models were trained on a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects.', 'A2: This paper presents a combined model for Funding Success (FS) in Kickstarter and Indiegogo projects using a dataset of 50,000 Kickstarter projects and 50,00 IndiegoGo projects from the Kaggle website for 2018. We obtained these data from the following sources:', 'A3: A combined Model for Funding Success (FS) in Kickstarter and Indiegogo projects.', 'A4: A combined model for Funding Success (FS) in Kickstarter and Indiegogo projects.  2017 Elsevier B.V.', 'A5: In this paper, we propose a combined Model for Funding Success (FS) in Kickstarter and Indiegogo projects.', 'A6: We developed a combined model for Funding Success (FS) in Kickstarter and Indiegogo projects.', 'A7: A combined model for Funding Success (FS) in Kickstarter and Indiegogo projects.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(34)TAKEN?True RQUGE:4.0013
Q1:Which model has the highest accuracy?
Q2:What does the figure show?
Best ans: Model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the Latent Dirichlet Allocation (LDA)-Model and the Metadata-Model.
['A0: Model we developed has the highest accuracy.', 'A1: Semantic-Model is similar to that of the Latent Dirichlet Allocation (LDA)-Model and the Metadata-Model.', 'A2: The model we developed has the highest accuracy.', 'A3: Model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the Latent Dirichlet Allocation (LDA)-Model and the Metadata-Model.', 'A4: Metadata', 'A5: model developed', 'A6: Latent Dirichlet Allocation (LDA)-Model', 'A7: Model we developed']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(35)TAKEN?False RQUGE:4.0004
Q1:A model for predicting the success of crowdfunding projects
Q2:What is the main purpose of this paper?
Best ans: a model for predicting the success of crowdfunding projects.
['A0: This article introduces a model for predicting the success of crowdfunding projects.', 'A1: Using the largest dataset available, we develop a model for predicting the success of crowdfunding projects.', 'A2: A model for studying and predicting the success of crowdfunding projects.', 'A3: We propose a model for predicting the success of crowdfunding projects.', 'A4: Modeling the Success of Crowdfunding Projects', 'A5: a model for predicting the success of crowdfunding projects.', 'A6: a model for predicting the success of crowdfunding projects', 'A7: A model for predicting the success of crowdfunding projects.']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(36)TAKEN?True RQUGE:3.9909
Q1:In which metrics would you find F-score?
Q2:What does the f-score measure?
Best ans: Model’s precision and recall are the same. F-score is the weighted harmonic mean of a model’s accuracy and recall. The final answer: accuracy.
['A0: Model’s precision and recall are the same. F-score is the weighted harmonic mean of a model’s accuracy and recall. The final answer: accuracy. and recall.', 'A1: F-score is the weighted harmonic mean of a model’s precision and recall.', 'A2: Model’s precision and recall are the same. F-score is the weighted harmonic mean of a model’s accuracy and recall. The final answer: accuracy.', 'A3: Precision is the ratio of correctly predicted positive records to the all records in actual class. F-score is the weighted harmonic mean of a model’s precision and recall.', 'A4: Precision', 'A5: reliability', 'A6: performance', 'A7: accuracy']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(37)TAKEN?False RQUGE:3.9233
Q1:We propose a model for predicting the Funding Success (FS) of crowdfunding projects.
Q2:What is the main focus of this paper?
Best ans: A model for predicting the Funding Success (FS) of crowdfunding projects.
['A0: Research on crowdfunding projects. We use a semantic model of crowdfunding projects to predict the Funding Success (FS) of projects.', 'A1: Using the largest dataset available, we propose an approach to predicting the Funding Success (FS) of crowdfunding projects.', 'A2: A model for predicting the Funding Success (FS) of crowdfunding projects.', 'A3: The paper proposes a model for predicting the Funding Success (FS) of crowdfunding projects.', 'A4: A model for crowdfunding project Funding Success (FS).', 'A5: An approach to predict the Funding Success (FS) of crowdfunding projects', 'A6: A model for crowdfunding project FS', 'A7: The paper presents a model for predicting the Funding Success (FS) of crowdfunding projects.']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(38)TAKEN?True RQUGE:3.9205
Q1:how were semantic features used?
Q2:What machine learning algorithms were used?
Best ans: To develop the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D.
['A0: Several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D', 'A1: The Semantic-Model was developed using several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D.', 'A2: To develop the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D.', 'A3: Semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D.', 'A4: As input', 'A5: used as input', 'A6: as input', 'A7: several machine learning algorithms']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(39)TAKEN?True RQUGE:3.903
Q1:What is a feature of the CFS algorithm?
Q2:What does the CFS algorithm evaluate?
Best ans: CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.
['A0: Among the top 10 features, about 70% are the same across datasets.', 'A1: A feature selection process that evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.', 'A2: Among the top 10 features, about 70% are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive', 'A3: CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.', 'A4: feature selection process.', 'A5: feature selection process', 'A6: features correlated to Funding Success (FS) are dependent on the project category', 'A7: individual predictive ability of each feature along with the degree of redundancy between them']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(40)TAKEN?True RQUGE:3.8711
Q1:Which features are most influential for Funding Success (FS)?
Q2:What features are most influential for Funding Success?
Best ans: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. We find that the following feature are most influential for Funding Success (FS): buzzwords Linguistic Inquiry Word Count Word %
['A0: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. We find that the following feature are most influential for Funding Success (FS): buzzwords Linguistic Inquiry Word Count Word % WC', 'A1: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. We find that the following feature are most influential for Funding Success (FS): buzzwords Linguistic Inquiry Word Count Word %', 'A2: Using a multi-stage approach, we used the following data to: a) analyze the data; and b) find the features that are most influential for Funding Success (FS) based on the individual predictive ability of each feature along with the degree of redundancy between them.', 'A3: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. We find that the following feature are most influential for Funding Success (FS): buzzwords Linguistic Inquiry Word Count', 'A4: the following feature selection process:', 'A5: Feature Selection', 'A6: the following feature selection process', 'A7: Feature Selection Process']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(41)TAKEN?False RQUGE:3.8037
Q1:Why do they use textual data for semantic analysis?
Q2:What is required for semantic analysis?
Best ans: Textual data is required for semantic analysis. For this reason, the projects’ texts are required.
['A0: They used textual data from projects to model the semantics of their work.', 'A1: Textual data is required for semantic analysis. For this reason, the projects’ texts are required.', 'A2: A project’s texts are required for semantic analysis. For this purpose, the projects’ textual data is required.', 'A3: This is because the projects’ textual data is required for semantic analysis.', 'A4: to understand how the model works.', 'A5: to understand how the model works', 'A6: To measure the accuracy of the models.', 'A7: To measure the accuracy of the models']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(42)TAKEN?False RQUGE:3.7209
Q1:Modeling the Success of Crowdfunding Projects
Q2:What dataset did we analyze
Best ans: Our model for crowdfunding projects focuses on semantic features only, which is comparable to that of previous models, and we analyze a large dataset of crowdfunding project data, larger than reported in the art.
['A0: Using large datasets, we present an innovative model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A1: Modeling the Success of Crowdfunding Projects', 'A2: Our model for crowdfunding projects focuses on semantic features only, which is comparable to that of previous models, and we analyze a large dataset of crowdfunding project data, larger than reported in the art.', 'A3: A novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: An Experimental Study of Crowdfunding Projects', 'A5: model for crowdfunding project success in meeting their funding goals', 'A6: model for crowdfunding project success', 'A7: A Model for CrowdFS']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(43)TAKEN?False RQUGE:3.7159
Q1:Why did they develop this model?
Q2:What is the buzzwords feature?
Best ans: Funding Success (FS) and buzzwords feature is among the features that are highly correlated to FS
['A0: Funding Success (FS) is highly correlated to buzzwords', 'A1: This study investigates the relationship between Funding Success (FS) and buzzwords', 'A2: Funding Success (FS) and buzzwords feature is among the features that are highly correlated to FS', 'A3: The results of our study are highly relevant to fundraisers using crowdfunding web platforms.', 'A4: high relevance', 'A5: to fundraise using crowdfunding web platforms.', 'A6: to fundraise using crowdfunding web platforms', 'A7: highly relevant to fundraisers using crowdfunding web platforms.']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(44)TAKEN?True RQUGE:3.711
Q1:The paper presents a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects for 2018.
Q2:What was the test that was used to evaluate the prediction performance?
Best ans: Datasets were obtained from Kaggle.com. We used Beautifulsup to extract additional metadata features. The data was re-scraped using Beautifulsdown. We reconstructed the data into three datasets: All_D (all Kickstarter projects), Tech_D(Tech_D projects) and Market_D. The model was trained on the datasets. The models were tested on a 10-fold cross validation test.
['A0: Datasets were obtained from Kaggle.com. We used Beautifulsup to extract additional metadata features. The data was re-scraped using Beautifulsdown. We reconstructed the data into three datasets: All_D (all Kickstarter projects), Tech_D(Tech_D projects) and Market_D. The model was trained on the datasets.', 'A1: Datasets were obtained from Kaggle.com. We used Beautifulsup to extract additional metadata features. The data was re-scraped using Beautifulsdown. We reconstructed the data into three datasets: All_D (all Kickstarter projects), Tech_D(Tech_D projects) and Market_D. The model was trained on the datasets. The models were tested on a 10-fold cross validation test.', 'A2: In this paper we use a dataset from Kickstarter and Indiegogo. We obtained the data from Kaggle. We used Beautifulsup to extract additional metadata features. We built three datasets: All_D, Tech_D and Market_D. We performed a 10-fold cross-validation test to evaluate the model’s performance. We developed the combined-model whose novelty lies in the combination of semantic and meta-data features, whose innovation lies in combining the two. We applied a 10 fold cross- Validation test.', 'A3: In this paper we use a dataset from Kickstarter and Indiegogo. We obtained the data from Kaggle. We used Beautifulsup to extract additional metadata features. We built three datasets: All_D, Tech_D and Market_D. We performed a 10-fold cross-validation test to evaluate the model’s performance.', 'A4: A dataset of Kickstarter and Indiegogo projects for 2018.', 'A5: The paper presents a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects for 2018. We obtained these data from the Kaggle website for 2018.', 'A6: We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects for 2018. We obtained these data from the Kaggle website for 2018.', 'A7: a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects for 2018.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(45)TAKEN?True RQUGE:3.6574
Q1:Why did they use several machine learning algorithms?
Q2:What were semantic features used for?
Best ans: To develop the Semantic-Model, semantic features were used as input.
['A0: Semantic features were used as input. So they used several machine learning algorithms.', 'A1: Semantic features were used as input.', 'A2: The Semantic-Model, semantic features were used as input.', 'A3: To develop the Semantic-Model, semantic features were used as input.', 'A4: to develop the Semantic-Model', 'A5: a high accuracy level', 'A6: semantic features were used as input.', 'A7: Semantic features were used as input']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(46)TAKEN?True RQUGE:3.5692
Q1:Which factors influence the Funding Success (FS) of crowdfunding projects?
Q2:What dataset do we analyze?
Best ans: In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics. We analyze large dataset of crowdfunding project data, larger than reported in the art.
['A0: Model for crowdfunding projects in meeting their funding goals. Model for predicting the success of crowdfunding projects by combining semantic features and metadata.', 'A1: In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics.', 'A2: In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics. We analyze large dataset of crowdfunding project data, larger than reported in the art.', 'A3: Using the dataset of Crowdfunding platforms, we present a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: Metadata', 'A5: semantic features only', 'A6: metadata', 'A7: semantic features']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(47)TAKEN?False RQUGE:3.5691
Q1:how many dictionary words were used in the text?
Q2:What did LIWC analysis do
Best ans: Linguistic Inquiry and Word Count (LIWC) Analysis Measures the appearance of dictionary words in a specific text The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.
['A0: Linguistic Inquiry and Word Count (LIWC) Analysis Measures the appearance of dictionary words in a specific text The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.', 'A1: A Linguistic Inquiry and Word Count (LIWC) analyzes how many words appear or are used in a specific text. LIWC analysis measures the appearance of dictionary words in s a text. The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well.', 'A2: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. The number and diversity of features is much larger than previously published and our dataset is significantly greater as well as arriving at much higher model accuracy.', 'A3: The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy. Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text.', 'A4: 0.1', 'A5: 1', 'A6: Word Count', 'A7: in the range [0.1]']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(48)TAKEN?True RQUGE:3.5624
Q1:how do we measure topic coherence in Latent Dirichlet Allocation (LDA) models?
Q2:How did we choose the topic model to use?
Best ans: Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose. the one that derived the highest coherency value and the smallest number of topics.
['A0: Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose. the one that derived the highest coherency value and the smallest number of topics. There were 30 topics that.', 'A1: Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose. the one that derived the highest coherency value and the smallest number of topics.', 'A2: To avoid overfitting, we chose the one that. derived the highest coherence value and the smallest number of topics.', 'A3: In Latent Dirichlet Allocation (LDA), each document is a collection of topics, where each word in the document belongs to one or some of these topic[s]. To avoid overfitting, we chose one that derived the highest coherence value and the smallest number of topics.', 'A4: model coherence value', 'A5: model coherence', 'A6: by choosing the one that derived.', 'A7: to judge how good a given topic model is']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(49)TAKEN?True RQUGE:3.5534
Q1:Feature Selection Process for FS
Q2:What does this paper describe?
Best ans: This paper describes the feature selection process for Funding Success (FS).
['A0: Feature Selection Process for FS', 'A1: This paper describes the feature selection process for Funding Success (FS).', 'A2: A feature selection process is used to select a set of features that are most influential for Funding Success (FS). The following features are evaluated: The following are the features that have a higher correlation in all datasets: buzzwords Linguistic Inquiry Word Count The following is an example of a feature selection method:', 'A3: To find features that are most influential, we used the Latent Dirichlet Allocation (LDA) Algorithm. We used the following features: The following features are evaluated: The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them:', 'A4: Description of Feature Selection Process:', 'A5: Description of Feature Selection Process', 'A6: We describe the feature selection process for Funding Success (FS).', 'A7: This paper describes the feature selection process for Funding Success (FS).']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(50)TAKEN?False RQUGE:3.5178
Q1:Which metric is used to measure the accuracy of the models?
Q2:What is required for semantic analysis?
Best ans: We present the metric that is used to measure the accuracy. For semantic analysis, we use the projects’ textual data.
['A0: For the semantics analysis, we use textual data from projects.', 'A1: We present the metric that is used to measure the accuracy. For semantic analysis, we use the projects’ textual data.', 'A2: We present the metric that is used to measure the accuracy. For semantic analysis, we use the projects’ textual data. So, the final answer is semantic analysis.', 'A3: The metric used to measure the accuracy of the models is the following: Semantic analysis, the projects’ textual data.', 'A4: metric', 'A5: SEM', 'A6: accuracy', 'A7: Semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(51)TAKEN?True RQUGE:3.4001
Q1:Using Semantic Models to Predict CrowdFS
Q2:What does this paper do?
Best ans: We use a semantic model to predict the Funding Success (FS) of a crowdfunding project.
["A0: An approach to crowdfunding that uses the semantics of the project's text to provide high-quality Funding Success (FS) prediction.", 'A1: Using Semantic Models To Predict CrowdFS (CFS) of Crowdfunding Projects.', 'A2: We use a semantic model to predict the Funding Success (FS) of a crowdfunding project.', 'A3: In this paper, we use semantic models and a large dataset to study and predict crowdFS.', 'A4: Research on the Semantic Models for CrowdFS', 'A5: A Semantic Model for CrowdFS', 'A6: We use semantic models to predict crowdFS.', 'A7: Using Semantic Models to Predict CrowdFS: A Case Study']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(52)TAKEN?False RQUGE:3.3544
Q1:How are features used in the model?
Q2:What is required for semantic analysis?
Best ans: To model the semantics, we use features from the projects’ textual data.
['A0: To model the semantics, we use features from the projects’ textual data.', "A1: To model the semantics, we use features from the projects’ textual data. For semantic analysis, the projects' texts have to be analyzed.", 'A2: For semantic analysis the projects’ textual data is required.', 'A3: The following features are used in the model: The following are the features used in our models: The metric used to measure the accuracy of the model is the metric for the model.', 'A4: Semantic analysis', 'A5: the projects’ textual data', 'A6: semantic analysis, the projects’ textual data', 'A7: semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(53)TAKEN?True RQUGE:3.3022
Q1:What datasets were used to study the relationship between features and the project category?
Q2:What are the three datasets used to study the relationship between features and project category?
Best ans: All_D, Market_D and Tech_D datasets.
['A0: Kickstarter and Indiegogo datasets.', 'A1: Kickstarter and Indiegogo datasets', 'A2: 50,000 Kickstarter and 50,00 Indiegogo projects', 'A3: All_D, Market_D and Tech_D datasets.', 'A4: Market_D', 'A5: All', 'A6: market', 'A7: The combined-model']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(54)TAKEN?False RQUGE:3.2955
Q1:In which datasets were the models trained?
Q2:On what dataset was training performed?
Best ans: All_D with 10-fold cross-validation
['A0: A: All_D', 'A1: All_D with 10-fold cross-validation.', 'A2: All_D', 'A3: All_D with 10-fold cross-validation', 'A4: alld', 'A5: Latent Dirichlet Allocation (LDA)-Model', 'A6: , All_D', 'A7: SVM, J48']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(55)TAKEN?False RQUGE:3.2746
Q1:Compared to previous studies, which model has better accuracy?
Q2:What does the figure show?
Best ans: Semantic-Model has the highest accuracy. The accuracy of Latent Dirichlet Allocation (LDA)-Model is similar to that of the Metadata-Model.
['A0: Semantic-Model has the highest accuracy.', 'A1: A model that is trained on All_D with 10-fold cross-validation has the highest accuracy.', 'A2: Model we developed has the highest accuracy.', 'A3: Semantic-Model has the highest accuracy. The accuracy of Latent Dirichlet Allocation (LDA)-Model is similar to that of the Metadata-Model.', 'A4: Latent Dirichlet Allocation (LDA)-Model', 'A5: Model', 'A6: we developed', 'A7: model we developed']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(56)TAKEN?True RQUGE:3.2095
Q1:Whose textual data is required for semantic analysis?
Q2:What type of textual data is required for semantic analysis?
Best ans: Projects
['A0: To analyze the textual data, the following entities are needed: Projects.', 'A1: Projects are the textual data of the projects. For semantic analysis, the projects’ texts are required.', 'A2: We use text from projects to model our model. For semantic analysis, the projects’ text is required.', 'A3: For semantic analysis, the projects’ textual data is required.', 'A4: authors', 'A5: Projects', 'A6: project', 'A7: projects’']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(57)TAKEN?False RQUGE:3.1929
Q1:A model for predicting the success of crowdfunding projects.
Q2:What is the title of this paper?
Best ans: Modeling Funding Success (FS) of Crowdfunding Projects: A Semantic Approach.
['A0: Modeling Funding Success (FS) of Crowdfunding Projects', 'A1: Modeling Funding Success (FS) of Crowdfunding Projects: A Semantic Approach.', 'A2: Using the largest dataset available, we develop a model that predicts the success of crowdfunding projects.', 'A3: A model for studying and predicting the success of crowdfunding projects.', 'A4: Using Semantic Models to Predict FS', 'A5: A model for predicting the success of crowdfunding projects. -', 'A6: Modeling the Success of Crowdfunding Projects', 'A7: An approach to studying and predicting the success of crowdfunding projects.']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(58)TAKEN?True RQUGE:3.1859
Q1:Where was a high accuracy model for prediction model?
Q2:On what datasets did we utilize machine learning algorithms?
Best ans: All_D datasets.
['A0: All_D datasets.', 'A1: All_D datasets. The LightGBM algorithm exhibited a high accuracy level for all three datasets in this study.', 'A2: LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular the accuracy of the prediction model is greater than 94% for all datasets', 'A3: In particular, the accuracy of the prediction model is greater than 94% for all datasets.', 'A4: LightGBM', 'A5: datasets', 'A6: on All_D', 'A7: All_D']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(59)TAKEN?False RQUGE:3.1484
Q1:How are the features selected?
Q2:What does the CFS algorithm evaluate?
Best ans: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.
['A0: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process.', 'A1: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets.', 'A2: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.', 'A3: Using a Latent Dirichlet Allocation (LDA) algorithm, we evaluate subsets of features based on individual predictive ability of each feature along with the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets.', 'A4: feature selection process', 'A5: we found the features that are most influential for FS', 'A6: based on the individual predictive ability of each feature along with the degree of redundancy between them', 'A7: CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(60)TAKEN?False RQUGE:3.1064
Q1:Feature Selection Process
Q2:What is the conclusion of the study
Best ans: Feature Selection Process: The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to Funding Success (FS) are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets
['A0: To find features that are most influential, we used the following features: The following features are evaluated: The CFS algorithms evaluate subsets based on individual predictive ability of each feature along with the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that features correlated to CFS are dependent on project category. Among the top 10 features, about 70% of features are the same across datasets', 'A1: We used the Latent Dirichlet Allocation (LDA) algorithm to evaluate subsets of features based on the individual predictive ability and the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features are dependent on the project category. Among the top 10 features, about 70% of the features were the same across datasets', 'A2: This section describes the feature selection process for Funding Success (FS). The CFS algorithm evaluates subsets of features based on individual predictive ability of each feature along with the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets', 'A3: Feature Selection Process: The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them. The semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to Funding Success (FS) are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets', 'A4: Description of datasets', 'A5: Description of datasets and methodology', 'A6: Feature Selection Process (CFS)', 'A7: description']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(61)TAKEN?True RQUGE:3.0915
Q1:Which features are most correlated with Funding Success (FS)? How are they selected and evaluated?
Q2:What did we develop to validate the results of our study?
Best ans: To study Funding Success (FS), we used the following features: Buzzwords, Linguistic Inquiry, Word count and feeling words. We used the CFS (correlation-based feature selection) algorithm to select the most Significant Set of Features (MSSF). We performed a 10-fold cross-validation test to evaluate the model’s performance. We developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.
['A0: To study Funding Success (FS), we used the following features: Buzzwords, Linguistic Inquiry, Word count and feeling words. We used the CFS (correlation-based feature selection) algorithm to select the most Significant Set of Features (MSSF). We performed a 10-fold cross-validation test to evaluate the model’s performance.', 'A1: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). We used a combination of semantic and meta-data features to select and evaluate the most Significant Set of Features (MSSF).', 'A2: To study Funding Success (FS), we used the following features: Buzzwords, Linguistic Inquiry, Word count and feeling words. We used the CFS (correlation-based feature selection) algorithm to select the most Significant Set of Features (MSSF). We performed a 10-fold cross-validation test to evaluate the model’s performance. We developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.', 'A3: A large dataset of Kickstarter and Indiegogo projects was used to train the model to predict Funding Success (FS). We used a combination of semantic and meta-data features to select and evaluate the most Significant Set of Features (MSSF). The model was tested on a 10-fold cross-validation test.', 'A4: CFS (correlation-based feature selection) algorithm. GraphNet datasets.', 'A5: CFS (correlation-based feature selection) algorithm.', 'A6: The combined-model is a model that combines semantic and meta-data features.', 'A7: CFS (correlation-based feature selection) algorithm']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(62)TAKEN?True RQUGE:3.0437
Q1:How do we combine semantic and meta-data features to predict Funding Success (FS)?
Q2:What datasets were used to study the relationship between features and project category?
Best ans: In this paper we combine semantic and meta-data features to predict Funding Success (FS) using a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc, for each record in the dataset. For each record, the values of five metadata features and about 120 semantic features, including buzzwords. We removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing topic sets. To study the relationship between features and project category, we built three datasets: All_D, Tech_D and Market
['A0: In this paper we combine semantic and meta-data features to predict Funding Success (FS) using a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects.', 'A1: In this paper we combine semantic and meta-data features to predict Funding Success (FS) using a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc, for each record in the dataset. For each record, the values of five metadata features and about 120 semantic features, including buzzwords. We removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing topic sets. To study the relationship between features and project category, we built three datasets: All_D, Tech_D and Market', 'A2: For each record, we obtained the values of five metadata features and about 120 semantic features. We removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing the topic sets. To study the relationship between features and the project category we built three datasets: All_D, Tech_D and Market_D. The original features space was reduced into three Principal Components (PCs). To identify the most Significant Set of Features (MSSF) that has the highest impact on Funding Success (FS), we use the CFS (correlation-based feature selection) algorithm. The models’ design, derivation and operational tests were conducted using the Python packages: scikit-learn, scikits-feature, LightGBM. We employed a', 'A3: The dataset used in this paper is a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count (LIWC) outputs, feelings words, explanation words, and Latent Dirichlet Allocation (LDA) outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets. To study the relationship between features and the project category,', 'A4: Models were developed using the following methods:', 'A5: In this paper, we combine semantic and meta-data features to predict Funding Success (FS) by combining the following:', 'A6: The combined-model is a model that combines semantic and meta-data features to predict FS', 'A7: In this paper, we combine semantic and meta-data features to predict FS']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(63)TAKEN?False RQUGE:3.0087
Q1:What are the buzzwords used in the buzz word dataset?
Q2:What categories of buzzwords were used in the study?
Best ans: Buzzwords in the buzz word dataset include: general conversation, education, business, sales and marketing, science, and technology. The buzzword dataset used in this study contains words from different categories: general discussion, education.
['A0: Buzzwords in the buzz word dataset include: general conversation, education, business, sales and marketing, science, and technology. The buzzword dataset used in this study contains words from different categories: general discussion, education.', 'A1: In this study the buzzwords are: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. The buzzword dataset used in this study contains words from different categories: general discussion, education.', 'A2: General conversation, education, business, sales and marketing, science and technology, politics and current affairs. The buzzword dataset used in this study contains words from different categories: general conversation, educational, business. The Buzzword dataset contains words in different categories.', 'A3: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. The buzz word dataset used is a list of words from various categories.', 'A4: marketing', 'A5: politics', 'A6: political', 'A7: business']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(64)TAKEN?True RQUGE:3.0034
Q1:Addition to: feeling
Q2:What are words that describe a feeling?
Best ans: Words that describe feeling include: awed. Awed is a feeling.
['A0: Addition to feeling:', 'A1: Words that describe feeling include: awed.', 'A2: Words that describe feeling include: awed. Awed is a feeling.', 'A3: The following are some of the emotions you might have if you were a child:', 'A4: added, explain', 'A5: Addition to and', 'A6: description of emotion', 'A7: words and emotions']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(65)TAKEN?False RQUGE:2.9965
Q1:A novel model for crowdfunding project Funding Success (FS).
Q2:What is the model based on?
Best ans: The paper presents a novel model for crowdfunding project Funding Success (FS) that combines both semantic features and project metadata. The model is based on a large dataset of crowdfunding project data.
['A0: This work proposes a novel model for crowdfunding project Funding Success (FS) that combines both semantic features and project metadata.', 'A1: The paper presents a novel model for crowdfunding project Funding Success (FS) that combines both semantic features and project metadata.', 'A2: Using large dataset of crowdfunding project data, we show that when combining metadata and semantics, we arrive at F1 score accuracy of 96.2%.', 'A3: The paper presents a novel model for crowdfunding project Funding Success (FS) that combines both semantic features and project metadata. The model is based on a large dataset of crowdfunding project data.', 'A4: Modeling Crowdfunding Projects', "A5: Modeling Crowdfunding Projects' FS", 'A6: a novel model for crowdfunding project Funding Success (FS) :', 'A7: a novel model for crowdfunding project FS']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(66)TAKEN?False RQUGE:2.8025
Q1:The authors used what to measure their models?
Q2:What is required for semantic analysis?
Best ans: metric is used to measure the accuracy of the model. For semantic analysis, the projects’ textual data is required.
['A0: Authors use a semantic analysis metric to measure their models.', 'A1: metric is used to measure the accuracy of the model. For semantic analysis, the projects’ textual data is required.', 'A2: The authors used the following metric to measure their models: The authors use the following features to measure the accuracy of their models.', 'A3: metric is used to measure the accuracy of the model. For semantic analysis, the projects’ textual data is required. The final answer: metric.', 'A4: metric', 'A5: textual data', 'A6: accuracy of the models', 'A7: semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(67)TAKEN?False RQUGE:2.6551
Q1:Which metrics are used to measure the performance of our models?
Q2:What metrics do we use to measure the performance of our models?
Best ans: Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall. The weightes harmonic mean is F1.
['A0: F-score is the weighted harmonic mean of a model’s precision and recall. F-Score is used to measure the performance of our models. The weightes harmonic mean is F1.', 'A1: F-score is the weighted harmonic mean of a model’s precision and recall. F-Score is used to measure the performance of our models.', 'A2: We used Recall and F-score to measure performance of our models. Recall is the ratio of correctly predicted positive records to all records in actual class. F-Score is the weighted harmonic mean of a model’s precision and recall.', 'A3: Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall. The weightes harmonic mean is F1.', 'A4: reliability', 'A5: R', 'A6: precision', 'A7: recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(68)TAKEN?False RQUGE:2.6418
Q1:How are F-score and recall measured?
Q2:What is the F-score?
Best ans: To measure the performance of a model, F-score is the weighted harmonic mean (F1) of predicting correctly positive records to all records in actual class.
['A0: To measure the performance of a model, F-score is the weighted harmonic mean (F1) of predicting correctly positive records to all records in actual class.', 'A1: Inference models are used to predict positive records. F-score is the weighted harmonic mean of a model’s precision and recall. F1 is the Weighted Harmonic Mean of Optimum and Minimum.', "A2: A model's F-score is the weighted harmonic mean of a model''s precision and recall. F-Score is also denoted F1. F-recall is the ratio of correctly predicted positive records to the all records in actual class.", 'A3: Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall. F1 is the Weighted Harmonic Mean of  F1.', 'A4: by F1', 'A5: precision', 'A6: accuracy', 'A7: accuracy and recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(69)TAKEN?True RQUGE:2.613
Q1:In which datasets did we find the highest coherence value?
Q2:Which datasets had the highest coherence values?
Best ans: All_D dataset All_d dataset Tech_d and Market_d
['A0: All_D dataset All_d dataset Tech_d and Market_d datasets', 'A1: All_D dataset All_d dataset Tech_d and Market_d', 'A2: All_D dataset All_d dataset Tech_d and Market_d datasets were used to evaluate the model.', 'A3: In both datasets, we found 30 topics that derived the highest coherence value.', 'A4: All', 'A5: Tech', 'A6: all', 'A7: market']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(70)TAKEN?False RQUGE:2.613
Q1:Which datasets used Latent Dirichlet Allocation (LDA)?
Q2:Which datasets had the highest coherence values?
Best ans: All_D dataset All_d dataset Tech_d and Market_d.
['A0: Markets and Crowdfunding', 'A1: Markets and Crowdfunding. All_D. Tech_D and Market_D datasets.', 'A2: All_D dataset All_d dataset Tech_d and Market_d.', 'A3: All_D dataset All_d dataset Tech_d and Market_d', 'A4: marketd', 'A5: Tech', 'A6: all', 'A7: market']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(71)TAKEN?True RQUGE:2.5076
Q1:What are the three datasets used?
Q2:What datasets were utilized in this study?
Best ans: In this study, we utilized three datasets: All_D. The Semantic-Model was developed on the datasets All_d.
['A0: In this study, we utilized three datasets: All_D.', 'A1: In this study, we utilized three datasets: All_D. The Semantic-Model was developed on the datasets All_d.', 'A2: All_D is a dataset of semantic features. All_d is based on the dataset of the same name.', 'A3: The Semantic-Model was developed on the datasets All_D, a dataset of semantic features, and the dataset of the state of the art (metadata) models.', 'A4: All_D.', 'A5: datasets', 'A6: Combined model', 'A7: all three datasets']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(72)TAKEN?True RQUGE:2.4689
Q1:What are the emotions that are deduced from the following statements?
Q2:What are some feelings words?
Best ans: Anxious, affectionate, anxious, awed, amazed, aggravated
['A0: Adequate, affectionate, anxious, awed, aggravated, annoyed', 'A1: Anxious, affectionate, anxious, awed, amazed, aggravated.', 'A2: Anxious, affectionate, anxious, awed, amazed, aggravated', 'A3: Adequate, affectionate, anxious, awed, aggravated', 'A4: alarmed', 'A5: angry', 'A6: embarrassed', 'A7: awkward']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(73)TAKEN?False RQUGE:2.4643
Q1:Additional topics for the following sentence:
Q2:What are some feelings words?
Best ans: Alarmed, awed, aggravated
['A0: awed, awkward', 'A1: Alarmed, awed, aggravated', 'A2: Answer: Awed', 'A3: Alarmed, awed', 'A4: Anxious', 'A5: explain, alarmed', 'A6: word: awed', 'A7: describe']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(74)TAKEN?True RQUGE:2.3882
Q1:Addition to:
Q2:What are the words for awed and amazed?
Best ans: Addition to: feeling
['A0: awed, astonished, aggravated', 'A1: awed, astonished, aggravated and annoyed are the most common feelings.', 'A2: Addition to add to add.', 'A3: In this article, we will discuss the following emotions: Awed.', 'A4: Anxious', 'A5: Answer: awed', 'A6: Addition to: feeling', 'A7: Addition to:']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(75)TAKEN?True RQUGE:2.3587
Q1:How do we predict the success of crowdfunding projects?
Q2:What do we use unique features to find?
Best ans: In this paper, we use unique features (for example, the use of buzzwords) to find characteristics that affect the success of a project in meeting its funding goal via crowdfunding. We use a combined model to predict the success.
['A0: Using the largest dataset available, we use a combination model to predict the success of crowdfunding projects.', 'A1: In this paper, we use unique features (for example, the use of buzzwords) to find characteristics that affect the success of a project in meeting its funding goal via crowdfunding. We use a combined model to predict the success.', 'A2: In this paper, we use unique features (for example, the use of buzzwords) to find characteristics that affect the success of a project in meeting its funding goal via crowdfunding.', 'A3: Research on crowdfunding projects. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantic characteristics of the projects’ text. We show that buzzword and Linguistic Inquiry and Word Count (LIWC) among the highly correlate features with the project’s success in fund raising. We provide a set of recommendations that may increase project Funding Success (FS) chances.', 'A4: use of buzzwords and linguistic inquiry', 'A5: using a semantic model of crowdfunding projects’ text', 'A6: using a semantic model of crowdfunding projects', 'A7: using a semantic model']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(76)TAKEN?False RQUGE:2.3349
Q1:To measure how well a model perform, what metrics are used?
Q2:What metrics are used to measure a model's performance?
Best ans: F-score is the weighted harmonic mean of a model’s precision and recall. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-Score is used to measure how well a Model perform.
['A0: To measure how well a model perform, F-score is used. F-Score is the weighted harmonic mean of a models precision and recall.', 'A1: F-score is the weighted harmonic mean of a model’s precision and recall. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-Score is used to measure how well a Model perform.', 'A2: Using the ratio of correctly predicted positive records to the all records in actual class, F-score is the weighted harmonic mean of a model’s precision and recall.', 'A3: The F-score is the weighted harmonic mean of a model’s precision and recall. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-Score is also denoted F1.', 'A4: reliability', 'A5: Precision', 'A6: accuracy', 'A7: precision and recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(77)TAKEN?True RQUGE:2.305
Q1:Why did he explain something?
Q2:What was the person explaining to someone because they were confused?
Best ans: The person was explaining something to someone because they were confused.
['A0: Because is to explain. Awed is to amaze someone.', 'A1: The person was explaining something to someone because they were confused.', 'A2: He explained something to explain it.', 'A3: For instance, he explained that he meant to explain something in other words. Awed, awed.', 'A4: Awed', 'A5: annoyed', 'A6: awed', 'A7: reason']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(78)TAKEN?True RQUGE:2.3013
Q1:Combined Model
Q2:What was the result of the LightGBM algorithm?
Best ans: A model was developed that utilizes a combination of several machine learning algorithms. The model was able to achieve a high accuracy level for all three datasets.
['A0: A model was developed that utilizes a combination of several machine learning algorithms.', 'A1: Combined Model: The combined model is a model that incorporates the features of several machine learning algorithms.', 'A2: In this paper, we combine the Semantic Model with the state of the art (metadata) model.', 'A3: A model was developed that utilizes a combination of several machine learning algorithms. The model was able to achieve a high accuracy level for all three datasets.', 'A4: Models', 'A5: model', 'A6: Model', 'A7: Combined Model']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(79)TAKEN?False RQUGE:2.2543
Q1:How many buzzwords were included?
Q2:Did the study examine the relationship between Funding Success and buzzwords used in the description of
Best ans: A dataset of buzzwords was used in this study. The buzzword dataset used in the study contains words from different categories: general conversation. The Buzzword dataset was a collection of words from various categories. The number of buzz words in the dataset is not examined. The final answer: not examined
['A0: There were a total of 0 buzzwords in the dataset.', 'A1: In the buzzword dataset used in this study there were words from different categories: general conversation. The buzzwords in the dataset are listed below:', 'A2: A dataset of buzzwords was used in this study. The buzzword dataset used in the study contains words from different categories: general conversation. The Buzzword dataset was a collection of words from various categories. The number of buzz words in the dataset is not examined. The final answer: not examined', 'A3: A dataset of buzzwords was used in this study. The buzzword dataset used in the study contains words from different categories: general conversation. The Buzzword dataset was a collection of words from various categories. The number of buzz words in the dataset is not examined. The final answer: not examined ( ).', 'A4: 10', 'A5: four', 'A6: seven', 'A7: six']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(80)TAKEN?False RQUGE:2.2543
Q1:How many buzzwords were included?
Q2:Did the study examine the relationship between Funding Success and buzzwords used in the description of
Best ans: A dataset of buzzwords was used in this study. The buzzword dataset used in the study contains words from different categories: general conversation. The Buzzword dataset was a collection of words from various categories. The number of buzz words in the dataset is not examined. The final answer: not examined
['A0: There were a total of 0 buzzwords in the dataset.', 'A1: In the buzzword dataset used in this study there were words from different categories: general conversation. The buzzwords in the dataset are listed below:', 'A2: A dataset of buzzwords was used in this study. The buzzword dataset used in the study contains words from different categories: general conversation. The Buzzword dataset was a collection of words from various categories. The number of buzz words in the dataset is not examined. The final answer: not examined', 'A3: A dataset of buzzwords was used in this study. The buzzword dataset used in the study contains words from different categories: general conversation. The Buzzword dataset was a collection of words from various categories. The number of buzz words in the dataset is not examined. The final answer: not examined ( ).', 'A4: 10', 'A5: four', 'A6: seven', 'A7: six']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(81)TAKEN?False RQUGE:2.1516
Q1:Who was Funding Success (FS)?
Q2:What is the buzzword dataset used in a study?
Best ans: Buzzwords used in this study are words from different categories. The buzzword dataset used in the study contains words from the following categories: general conversation. The Buzzword dataset is used in a study. The final answer: Funding Success (FS).
['A0: In this study, buzzwords were used in the description of the project to describe the buzzword dataset. The buzzword data was used in this study to describe buzzword in the buzz word dataset. So the final answer is no.', 'A1: Buzzwords used in this study are words from different categories. The buzzword dataset used in the study contains words from the following categories: general conversation. The Buzzword dataset is used in a study. The final answer: Funding Success (FS).', 'A2: A BuzzWord dataset was used in this study. BuzzWores were words from different categories. The buzzword dataset used in the study contains words from general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. So, the final answer is no.', 'A3: ', 'A4: founder', 'A5: project', 'A6: Project', 'A7: nonprofit']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(82)TAKEN?False RQUGE:1.8618
Q1:In which category of buzzwords was Funding Success (FS) not examined for?
Q2:In which category of buzzwords was Funding Success (FS) not examined for?
Best ans: Funding Success (FS) was not examined for buzzwords in the category of science and technology. The buzzword dataset used in this study contains words from different categories: science and science. The Buzzword dataset contains buzzword words from the following categories: business. The answer: business.
['A0: Funding Success (FS) was not examined for buzzwords in the category of science and technology. The buzzword dataset used in this study contains words from different categories: science and science. The Buzzword dataset contains buzzword words from the following categories: business. The answer: business.', 'A1: Business refers to words that are used in business. The buzzword dataset used in this study contains words from different categories: business, sales and marketing. The Buzzword dataset contains words that were not used in the description of the project. The final answer: Business.', 'A2: Education is the category of buzzwords used in this study. The buzzword dataset used in the study contains words from different categories: education. The Buzzword dataset contains words in different categories including: education, business, sales and marketing, science and technology. The final answer: education.', 'A3: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. The buzz word dataset used for this study includes words from the following categories:', 'A4: politics', 'A5: political', 'A6: business', 'A7: general conversation']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(83)TAKEN?False RQUGE:1.7539
Q1:What are some of the algorithms used in this study?
Q2:What was the training performed on?
Best ans: SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.
['A0: LSVM, J48, Random forest, LightGBM, SDG, and DNN.', 'A1: LSVM, J48, Random forest, LightGBM, SDG, and DNN', 'A2: SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.', 'A3: SVM, J48, Random forest, LightGBM, SDG, and DNN.', 'A4: J48', 'A5: Semantic-Model', 'A6: DNN', 'A7: Latent Dirichlet Allocation (LDA)-Model']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(84)TAKEN?False RQUGE:1.7363
Q1:Predicting CrowdFS
Q2:What is the novel model for predicting the success of crowdfunding projects?
Best ans: Model for CrowdFS.
['A0: Model for CrowdFS.', 'A1: Model for CrowdFS', 'A2: Using semantic features and metadata, we propose a model for crowdfunding projects to predict their Funding Success (FS).', 'A3: The paper presents a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: Predicting crowdFS', 'A5: Modeling CrowdFS', 'A6: model for crowdfunding projects in meeting their funding goals.', 'A7: model for crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(85)TAKEN?False RQUGE:1.6931
Q1:How many percent of the data was used for the experiments?
Q2:What is the final answer to the question, "What is the accuracy level of the semantic-
Best ans: The experiments used 91.2% of the data. The Semantic-Model was developed on a dataset of. The data was used as input for the experiments. The final answer: 91.
['A0: The experiments used 91.2% of the data. The Semantic-Model was developed on a dataset of.', 'A1: The experiments used 91.2% of the data. The Semantic-Model was developed on a dataset of. The data was used as input for the experiments. The final answer: 91.', 'A2: The experiments used 91.2% of the data.', 'A3: 91.2% of the data was used for experiments. The Semantic-Model was developed on the data set All_D.', 'A4: 90', 'A5: 90%', 'A6: 95', 'A7: 100']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(86)TAKEN?False RQUGE:1.602
Q1:how many features are the same across dataset
Q2:What is the conclusion of the study?
Best ans: Among top 10 features, about 70% of the features are the same across datasets.
['A0: We used the following dataset:', 'A1: Among top 10 features, about 70% of the features are the same across datasets.', 'A2: To find out which feature is most influential for Funding Success (FS), we used a combination of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process.', 'A3: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process.', 'A4: 71%', 'A5: 70 percent.', 'A6: 70 percent', 'A7: 70']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(87)TAKEN?False RQUGE:1.5939
Q1:How do we predict Funding Success (FS)?
Q2:What two models were used in this study?
Best ans: Model based on semantic features only. Model focusing on both project-specific aspects and semantics of project descriptions.
['A0: Model based on semantic features only.', 'A1: Our study investigate the relationship between Funding Success (FS) and buzzwords and develop a model that focuses on both project-specific aspects and semantics of project descriptions.', 'A2: Model based on semantic features only. Model focusing on both project-specific aspects and semantics of project descriptions.', 'A3: This paper presents a novel model based on semantic features only. The model achieves similar accuracy level as previous studies. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.', 'A4: Modeling buzzwords', 'A5: new models', 'A6: A novel model based on semantic features only', 'A7: novel model based on semantic features only']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(88)TAKEN?False RQUGE:1.5819
Q1:how do we measure topic coherence
Q2:What are the three main inputs to the LDA topic model?
Best ans: Topic coherence is a measure of how well the model is performing on the dataset. The three main inputs to the Latent Dirichlet Allocation (LDA) topic model are: dictionary, corpus, and a number of topics. Model perplexity measures how well a model is able to model the content of a document and how many topics it has.
['A0: Topic coherence is a measure of how well the model is performing on the dataset.', 'A1: Topic coherence is a measure of how well the model is performing on the dataset. The three main inputs to the Latent Dirichlet Allocation (LDA) topic model are: dictionary, corpus, and a number of topics. Model perplexity measures how well a model is able to model the content of a document and how many topics it has.', 'A2: Using Latent Dirichlet Allocation (LDA), we used Gensim Python package LDA implementation to execute LDA to perform topic analysis on the text of project updates.', 'A3: To avoid overfitting, we chose the one that derived the highest coherence value.', 'A4: topic coherence provide a convenient', 'A5: topic coherence provide', 'A6: topic coherence', 'A7: model coherence']
Text:Latent Dirichlet Allocation is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(89)TAKEN?False RQUGE:1.4873
Q1:Combined Model: 91.2%
Q2:What is the accuracy level of the combined model?
Best ans: A combined model was developed for the datasets. The combined model is comparable in accuracy with the state of the art (metadata) models.
['A0: Combined Model: 91.2%', 'A1: Combined Model: 91.2% The Semantic-Model was developed on the All_D dataset and the state of the art (metadata) models on the dataset.', 'A2: For the development of the Semantic-Model we utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D dataset.', 'A3: A combined model was developed for the datasets. The combined model is comparable in accuracy with the state of the art (metadata) models.', 'A4: Accuracy', 'A5: accuracy', 'A6: Results', 'A7: results']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(90)TAKEN?False RQUGE:1.4457
Q1:Does Linguistic Inquiry and Word Count (LIWC) analysis measure dictionary words in a specific word?
Q2:What does LIWC analysis measure?
Best ans: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. LIWC analysis is used to analyze the textual description part of projects. LIWD analysis measures dictionary words. LIDC analysis measures a numeric value in the range [0.1]. LIWC is a tool used to extract features from the text.
['A0: No. Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. LIWC analysis is used to analyze the textual description part of projects. LIWD is used for textual descriptions. LIDC is used as a textual analysis tool for crowdfunding.', 'A1: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. LIWC analysis is used to analyze the textual description part of projects. LIWD analysis measures dictionary words.', 'A2: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. LIWC analysis is used to analyze the textual description part of projects. LIWD analysis measures dictionary words. LIDC analysis measures a numeric value in the range [0.1].', 'A3: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. LIWC analysis is used to analyze the textual description part of projects. LIWD analysis measures dictionary words. LIDC analysis measures a numeric value in the range [0.1]. LIWC is a tool used to extract features from the text.', 'A4: No', 'A5: no', 'A6: Yes', 'A7: yes']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(91)TAKEN?False RQUGE:1.3393
Q1:Who created this article?
Q2:Who performed the analysis?
Best ans: A team of researchers.
['A0: A team of researchers.', 'A1: Data was collected from projects’ posts.', 'A2: The article was written by a team of researchers.', 'A3: Projects’ posts were extracted from projects’ posts via Python web scraping.', 'A4: Projects', 'A5: Team', 'A6: Data Science', 'A7: The authors']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(92)TAKEN?False RQUGE:1.2997
Q1:What are the most common features of projects?
Q2:What was the most common feature of projects?
Best ans: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects. The final answer: the number.
['A0: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects. The final answer: the number. number of. photos.', 'A1: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by.', 'A2: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects. The final answer: the number.', 'A3: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects.', 'A4: uploaded', 'A5: posts', 'A6: video', 'A7: videos']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(93)TAKEN?False RQUGE:1.151
Q1:Which of these is not a feeling?
Q2:Which of these is not a feeling?
Best ans: Absurdity is a feeling. Absurdies are not feelings.
['A0: Absurdity is a feeling. Absurdies are not feelings.', 'A1: awed means that you are amazed. Awed is a feeling. The final answer: awked.', 'A2: Awed is a feeling. Awe is derived from the Greek word aesop.', 'A3: Alarmed is a feeling. Awed means that you are amazed at something or someone. The final answer: awed.', 'A4: Alarmed', 'A5: Awed', 'A6: awkward', 'A7: loving']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(94)TAKEN?False RQUGE:1.1367
Q1:What is the metric used to measure the accuracy of the models?
Q2:What feature is used to measure accuracy?
Best ans: sentence completion
['A0: This section presents the metric used to measure accuracy. For semantic analysis, the projects’ textual data is required.', 'A1: For semantic analysis, the projects’ textual data is required.', 'A2: We present the metric that is used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.', 'A3: For semantic analysis, the projects’ textual data is required. The metric used to measure the accuracy of the models is:', 'A4: metric', 'A5: sentence completion', 'A6: accuracy', 'A7: Semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(95)TAKEN?False RQUGE:1.1294
Q1:How do you feel about the person who is describing something?
Q2:What are two feelings words that were derived from the topic of the article?
Best ans: You feel awed about the person who is explaining something. You feel affectionate about someone who is describing something.
['A0: Absurdity is a feeling that comes from not understanding something. Awed means that you feel like you are awed.', 'A1: Describe something means to explain something. Awed is a feeling that comes from being awed or astonished at something.', 'A2: Awed is the feeling you get when you are awed. Awe is the emotion you get if you are amazed.', 'A3: You feel awed about the person who is explaining something. You feel affectionate about someone who is describing something.', 'A4: loving', 'A5: loved', 'A6: awkward', 'A7: annoyed']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



