(0)TAKEN?True RQUGE:5.0931
Q1:How many projects were created by the same person?
Q2:How many previously created projects by one person was the answer to our analysis question?
Best ans: One of our analysis questions was: how many projects were previously created?. the answer: 1. the number of previously created projects by one person was 1.
['A0: One of our analysis questions was: how many projects were previously created?. the answer: 1.', 'A1: One of our analysis questions was: how many projects were previously created?. the answer: 1. the number of previously created projects by one person was 1.', 'A2: We used the following features: the number of photos, videos, the number. of updates, the. number of previously created projects by. the number. and number of previous created projects.', 'A3: We used the following features: the number of photos, videos, the number. of updates, the. number of previously created projects by. the number. and number of previous created projects. were extracted from projects’ posts.', 'A4: 0', 'A5: One', 'A6: Once', 'A7: Multiple']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(1)TAKEN?False RQUGE:5.083
Q1:Do buzzwords predict Funding Success (FS) for crowdfunded projects?
Q2:is buzzwords related to funding success
Best ans: Yes
['A0: We developed a model to predict Funding Success (FS) (fs) for crowdfunding projects. we developed the model based on semantic features only. we also developed an impressive fscore of 96.2%, which is highly relevant to crowdfunded projects.', 'A1: We developed a model to predict Funding Success (FS) (fs) for crowdfunding projects. we developed the model based on semantic features only. we also developed an impressive fscore of 96.2%, which is highly relevant to crowdfunded projects. future research could further improve accuracy by considering the characteristics of images and video content.', 'A2: Crowdfunded projects are crowdfunded projects. buzzwords are used in crowdfunding campaigns to describe projects. the buzzwords feature is among the features that are highly correlated to Funding Success (FS) (fs) compared to previous parameters that we examined and by other researchers. we developed a model based on semantic features only and achieved similar accuracy level as previous studies.', 'A3: Funding Success (FS) (fs) is a measure of the success of crowdfunded projects. buzzwords are highly correlated to fs. we developed a model based on semantic features only and achieved similar accuracy level as previous studies.', 'A4: High', 'A5: No', 'A6: Yes', 'A7: Highly correlated']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(2)TAKEN?True RQUGE:5.0606
Q1:How to predict crowdFS?
Q2:What do we present?
Best ans: A novel model for predicting the success of crowdfunding projects in meeting their funding goals.
['A0: Model for crowdFS.', 'A1: Model for crowdFS', 'A2: The paper presents a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A3: Using large dataset of crowdfunding project data, we show that when combining semantics and metadata, we arrive at f1 score accuracy of 96.2%.', 'A4: Modeling crowdFS', 'A5: Model for crowdfunding projects in meeting their funding goals', 'A6: A novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A7: A novel model for predicting the success of crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(3)TAKEN?True RQUGE:5.0146
Q1:Which tool was used to extract features from the text?
Q2:What did we use to extract features from the text?
Best ans: Language inquiry and word count software tool
['A0: Word count software', 'A1: Word count software tool', 'A2: Language inquiry and word count software tool', 'A3: The Linguistic Inquiry and Word Count (LIWC) software tool was used.', 'A4: Language inquiry', 'A5: Linguistic Inquiry and Word Count (LIWC) analysis', 'A6: Liwc', 'A7: Word count software']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(4)TAKEN?False RQUGE:5.0146
Q1:Which tool was used to extract features from the text?
Q2:What did we use to extract features from the text?
Best ans: Language inquiry and word count software tool
['A0: Word count software', 'A1: Word count software tool', 'A2: Language inquiry and word count software tool', 'A3: The Linguistic Inquiry and Word Count (LIWC) software tool was used.', 'A4: Language inquiry', 'A5: Linguistic Inquiry and Word Count (LIWC) analysis', 'A6: Liwc', 'A7: Word count software']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(5)TAKEN?True RQUGE:4.9592
Q1:Why did they use several machine learning algorithms?
Q2:What was used as input for the development of the semantic-model?
Best ans: To develop the semantic-model, semantic features were used as input.
['A0: Semantic features were used as input. so they used several machine learning algorithms.', 'A1: Semantic features were used as input.', 'A2: The semantic-model, semantic features were used as input.', 'A3: To develop the semantic-model, semantic features were used as input.', 'A4: To develop the semantic-model', 'A5: Semantic features were used as input.', 'A6: A high accuracy level', 'A7: Semantic features were used as input']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(6)TAKEN?False RQUGE:4.9
Q1:A novel model for crowdfunding project FS
Q2:What do we present?
Best ans: A novel model for crowdfunding project Funding Success (FS).
['A0: Model for crowdfunding project fs', 'A1: Using semantic features, we analyze both metadata and project semantics. we show that when combining metadata and semantics, we arrive at f1 score accuracy of 96.2%.', 'A2: A novel model for crowdfunding project Funding Success (FS).', 'A3: A novel model for crowdfunding project fs', 'A4: Modeling crowdfunding project FS', 'A5: A novel model for crowdfunding project Funding Success (FS).', 'A6: A novel model for crowdfunding project fs', 'A7: ']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(7)TAKEN?False RQUGE:4.8851
Q1:Modeling the success of crowdfunding projects
Q2:What does the paper present?
Best ans: The paper presents a novel model for predicting the success of crowdfunding projects in meeting their funding goals, which is comparable to that of previous models.
['A0: Modeling the success of crowdfunding projects.', 'A1: The paper presents a novel model for predicting the success of crowdfunding projects in meeting their funding goals, which is comparable to that of previous models.', 'A2: Modeling the success of crowdfunding projects', 'A3: Modeling the success of crowdfunding projects. we present a novel model for predicting the success a crowdfunding project in meeting its funding goals. we analyze a large dataset of crowdfunding project data, larger than reported in the art.', 'A4: Model for crowdfunding projects in meeting their funding goals.', 'A5: Modeling the success of crowdfunding projects.', 'A6: Model for crowdfunding projects in meeting their funding goals', 'A7: A novel model for predicting the success of crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(8)TAKEN?False RQUGE:4.8356
Q1:What is the ratio of correctly predicted positive records to the all records in actual class?
Q2:What is the ratio of correctly predicted positive records to all records in actual class?
Best ans: Recall
['A0: A model’s recall is the ratio of correctly predicted positive records to all records in actual class. recall and f1 are the weighted harmonic mean of the model’ precision and recall. f1 is the weighting harmonic mean.', 'A1: A model’s recall is the ratio of correctly predicted positive records to all records in actual class. recall and f1 are the weighted harmonic mean of the model’ precision and recall. f1 is the weighting harmonic mean. so, the final answer is f1.', 'A2: In order to measure the performance of our models, we used the metrics to measure performance of the models: recall, precision, and f-score. recall is the ratio of correctly predicted positive records to all records in actual class. f-score is the weighted harmonic mean of a model’s precision and recall.', 'A3: Recall is the ratio of correctly predicted positive records to the all records in actual class. f-score is the weighted harmonic mean of a model’s precision and recall. f1 is the weighted harmonic mean of optimum and minimum precision.', 'A4: Remember', 'A5: Recall', 'A6: ', 'A7: ']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(9)TAKEN?True RQUGE:4.7991
Q1:Which of these features is not used to determine Funding Success (FS)?
Q2:Which of these features is not used to determine Funding Success (FS)?
Best ans: Previous created projects by.
['A0: Previous created projects by.', 'A1: In order to determine Funding Success (FS) (fs), we used the following features:', 'A2: Previous created projects by. are not used to determine Funding Success (FS) (fs). the final answer: previous created projects b.', 'A3: Previous created projects by. are not used to determine Funding Success (FS) (fs).', 'A4: Video.', 'A5: Video', 'A6: Updates', 'A7: Videos']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(10)TAKEN?True RQUGE:4.7587
Q1:Why is crowdfunding important?
Q2:What do entrepreneurs use crowdfunding platforms for?
Best ans: To raise funds
['A0: A new model for crowdfunding projects in meeting their funding goals.', 'A1: This paper presents a novel model for predicting the crowdFS of crowdfunding projects in meeting their funding goals.', 'A2: Hence, the question of what influences projects’ fundraising success is very important. previous studies examined various factors such as project goals and project duration that may influence the outcomes of crowdfunding campaigns. we present a novel model for crowdfunding projects in meeting their funding goals', 'A3: We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: To raise funds', 'A5: The question of what influences projects’ fundraising success is very important', 'A6: What influences projects’ fundraising success is very important.', 'A7: What influences projects’ fundraising success is very important']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(11)TAKEN?False RQUGE:4.7143
Q1:Did Funding Success (FS) have any relationship with buzz words?
Q2:did they look at funding success and buzzwords
Best ans: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. the buzz word dataset used is not examined. the final answer: no.
['A0: A buzzword dataset contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics and current affairs. the buzzword dataset was not examined. the final answer: no.', 'A1: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. the buzz word dataset used is not examined. the final answer: no.', 'A2: ', 'A3: ', 'A4: Yes', 'A5: No', 'A6: Not examined', 'A7: ']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(12)TAKEN?True RQUGE:4.6627
Q1:How were project's posts analyzed
Q2:How were the metadata features extracted?
Best ans: Projects’ posts were analyzed for metadata features known to affect Funding Success (FS) by using python web scraping.
['A0: Projects’ posts were analyzed for metadata features known to affect FS', 'A1: A set of features known to affect Funding Success (FS) was used for our analysis.', 'A2: Projects’ posts were analyzed for metadata features known to affect Funding Success (FS) by using python web scraping.', 'A3: Data was extracted from projects’ posts via python web scraping.', 'A4: By.', 'A5: By', 'A6: Python web scraping.', 'A7: Via python web scraping']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(13)TAKEN?False RQUGE:4.6627
Q1:How were project's posts analyzed in this article
Q2:How were the metadata features extracted?
Best ans: Projects’ posts were analyzed for metadata features known to affect Funding Success (FS) by using python web scraping.
['A0: Python web scraping is used for data extraction. the data was extracted from projects’ posts via python web crawling.', 'A1: A set of features known to affect Funding Success (FS) was used for this article to analyze project posts.', 'A2: Projects’ posts were analyzed for metadata features known to affect FS', 'A3: Projects’ posts were analyzed for metadata features known to affect Funding Success (FS) by using python web scraping.', 'A4: Web scraping', 'A5: Python web scraping.', 'A6: Via python web scraping', 'A7: Metadata features known to affect FS']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(14)TAKEN?True RQUGE:4.5806
Q1:Which features are associated with the successful completion of crowdfunding projects?
Q2:What does this study show about buzzwords and LIWC?
Best ans: Buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the highly correlates features with the project’s success in fund raising, which is a promising approach for crowdfunding.
['A0: Buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the highly correlates features with the project’s success in fund raising, which is a promising approach for crowdfunding. the methodological approach is to:', 'A1: Using the largest dataset available, we use a combination model to predict the success of crowdfunding projects. the model is based on the following features: buzzwords linguistic inquiry word count', 'A2: Buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the highly correlates features with the project’s success in fund raising, which is a promising approach for crowdfunding.', 'A3: Using the largest dataset available, we use a combination model to predict the success of crowdfunding projects. the model is based on the following features: buzzwords linguistic inquiry word count word count is a feature of the project that is highly correlate with the project’s success in fund raising.', 'A4: Buzzwords', 'A5: Language and semantics', 'A6: Linguistic inquiry word count', 'A7: Word count']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(15)TAKEN?True RQUGE:4.57
Q1:Which feature is highly correlated to Funding Success (FS)?
Q2:What feature is highly correlated to FS?
Best ans: Buzzwords feature
['A0: Project description semantic features', 'A1: Buzzwords', 'A2: Buzzwords feature', 'A3: Project description semantic features of project descriptions. the buzz word feature is among the features that are highly correlated to Funding Success (FS). the buzzwords feature is one of the features.', 'A4: Hyped', 'A5: Hype', 'A6: Buzz words', 'A7: Buzzwords feature']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(16)TAKEN?False RQUGE:4.5555
Q1:Using semantic models to predict fundraising success
Q2:What is the main focus of this paper?
Best ans: Using semantic models to predict fundraising success.
['A0: This work uses semantic models to predict crowdFS.', 'A1: We use a semantic model to predict crowdFS.', 'A2: Using semantic models to predict fundraising success.', 'A3: Using semantic models to predict fundraising success', 'A4: Methods and results', 'A5: Research on crowdfunding projects using semantic models', 'A6: Using semantic models to predict fundraising success.', 'A7: A new approach to studying and predicting crowdFS']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(17)TAKEN?True RQUGE:4.5164
Q1:In which datasets were the features evaluated?
Q2:Which datasets has a unique parameter WC?
Best ans: R_tech _d datasets
['A0: R_tech _d datasets', 'A1: R_tech _d', 'A2: R_tech _d datasets were used.', 'A3: R_tech _d dataset', 'A4: All', 'A5: Datasets', 'A6: Cfs', 'A7: Cfs algorithm']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(18)TAKEN?False RQUGE:4.5159
Q1:A model for predicting crowdFS using semantic features
Q2:What is the main focus of this paper?
Best ans: A model for predicting crowdfs using semantic features.
['A0: An approach to studying and predicting crowdFS (fs) using semantic features.', 'A1: A model for crowdFS using semantic features.', 'A2: This work proposes a model for predicting crowdFS using semantic features.', 'A3: The paper presents a model for predicting crowdFS (cf) using semantic features. the model is based on the following:', 'A4: Model for predicting crowdfs using semantic features.', 'A5: Model for predicting crowdfs using semantic features', 'A6: A model for predicting crowdfs using semantic features.', 'A7: A model for predicting crowdfs using semantic features']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(19)TAKEN?True RQUGE:4.5117
Q1:In which datasets did we use the Latent Dirichlet Allocation (LDA) method to perform topic analysis?
Q2:What datasets had the highest coherence values?
Best ans: Alld, techd,marketd
['A0: Alld, techd,marketd', 'A1: Markets and crowdfunding', 'A2: All_d dataset all_d dataset tech_d and market_d data.', 'A3: All_d dataset all_d dataset tech_d and market_d data', 'A4: Marketd', 'A5: Tech', 'A6: All', 'A7: Market']
Text:Latent Dirichlet Allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(20)TAKEN?False RQUGE:4.509
Q1:What is the purpose of this paper?
Q2:What does this paper take a more of compared to prior studies?
Best ans: A comprehensive approach to studying and predicting the success of crowdfunding projects.
['A0: Predict Funding Success (FS) of crowdfunding projects', 'A1: Research on the Funding Success (FS) of crowdfunding projects', 'A2: We propose a new approach to studying and predicting the success of crowdfunding projects.', 'A3: A comprehensive approach to studying and predicting the success of crowdfunding projects.', 'A4: To predict crowdFS of projects', 'A5: To predict crowdFS', 'A6: Modeling Funding Success (FS) of crowdfunding projects', 'A7: A comprehensive approach to studying and predicting the success of crowdfunding projects']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(21)TAKEN?False RQUGE:4.509
Q1:What is the purpose of this paper?
Q2:What does this paper take a more of compared to prior studies?
Best ans: A comprehensive approach to studying and predicting the success of crowdfunding projects.
['A0: Predict Funding Success (FS) of crowdfunding projects', 'A1: Research on the Funding Success (FS) of crowdfunding projects', 'A2: We propose a new approach to studying and predicting the success of crowdfunding projects.', 'A3: A comprehensive approach to studying and predicting the success of crowdfunding projects.', 'A4: To predict crowdFS of projects', 'A5: To predict crowdFS', 'A6: Modeling Funding Success (FS) of crowdfunding projects', 'A7: A comprehensive approach to studying and predicting the success of crowdfunding projects']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(22)TAKEN?False RQUGE:4.4352
Q1:Why did they use the Linguistic Inquiry and Word Count (LIWC) analysis?
Q2:What did previous studies on crowdfunding use LIWC for?
Best ans: Previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects.
['A0: Previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects.', 'A1: Previous studies used Linguistic Inquiry and Word Count (LIWC) to analyze the textual description part of projects', 'A2: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text', 'A3: We used Linguistic Inquiry and Word Count (LIWC) to extract features from the text. liwc analysis measures the appearance of dictionary words in a specific text.', 'A4: Measure the appearance of dictionary words', 'A5: To analyze the textual description part of projects.', 'A6: To extract features from the text', 'A7: Extract features from the text']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(23)TAKEN?False RQUGE:4.4098
Q1:What is the name of the software tool used to extract features from the text?
Q2:What software tool did we use to extract features from the text?
Best ans: Linguistic Inquiry and Word Count (LIWC) analysis
['A0: LIWC', 'A1: We used the Linguistic Inquiry and Word Count (LIWC) software tool to extract features from the text. liwc analysis measures the appearance of dictionary words in a specific text. the output is vd = sd/nt, which is the value of the feature that corresponds to d. liwct', 'A2: Linguistic Inquiry and Word Count (LIWC) software tool', 'A3: Linguistic Inquiry and Word Count (LIWC) software tool is used to extract features from the text. liwc analysis measures the appearance of dictionary words in a specific text. the output of liwc is vd = sd/nt, which is the value of the feature that corresponds to d. liwc analysis measures dictionary words.', 'A4: Linguistic Inquiry and Word Count (LIWC) analysis', 'A5: Language inquiry', 'A6: Word count', 'A7: Liwc']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(24)TAKEN?True RQUGE:4.3876
Q1:How do we predict Funding Success (FS)?
Q2:What model did we develop?
Best ans: Model based on semantic features only. model focusing on both project-specific aspects and semantics of project descriptions.
['A0: Model based on semantic features only.', 'A1: Our study investigate the relationship between Funding Success (FS) and buzzwords and develop a model that focuses on both project-specific aspects and semantics of project descriptions.', 'A2: Model based on semantic features only. model focusing on both project-specific aspects and semantics of project descriptions.', 'A3: This paper presents a novel model based on semantic features only. the model achieves similar accuracy level as previous studies. the results of our study are highly relevant to fundraisers using crowdfunding web platforms.', 'A4: Modeling buzzwords', 'A5: New models', 'A6: A novel model based on semantic features only', 'A7: Novel model based on semantic features only']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(25)TAKEN?False RQUGE:4.3683
Q1:Compared to previous studies, which model has better accuracy?
Q2:What is the accuracy of the semantic-model?
Best ans: Semantic-model has the highest accuracy. the accuracy of Latent Dirichlet Allocation (LDA)-model is similar to that of the metadata-model.
['A0: Semantic-model has the highest accuracy.', 'A1: A model that is trained on all_d with 10-fold cross-validation has the highest accuracy.', 'A2: Model we developed has the highest accuracy.', 'A3: Semantic-model has the highest accuracy. the accuracy of Latent Dirichlet Allocation (LDA)-model is similar to that of the metadata-model.', 'A4: Latent Dirichlet Allocation (LDA)-model', 'A5: Model', 'A6: We developed', 'A7: Model we developed']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(26)TAKEN?True RQUGE:4.3467
Q1:In which datasets were the proposed models evaluated?
Q2:On what dataset was the proposed model evaluated?
Best ans: The proposed models were evaluated on the following datasets: all_d.
['A0: In this paper, we evaluated proposed models on all_d dataset.', 'A1: The proposed models were evaluated on the following datasets: all_d. the semantic-model was evaluated on datasets with semantic features:', 'A2: We utilized several machine learning algorithms (including svm, j48, random forest, lightgbm, sdg, dnn, and more) on all_d.', 'A3: The proposed models were evaluated on the following datasets: all_d.', 'A4: All_d.', 'A5: Datasets', 'A6: Combined model', 'A7: All three datasets']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(27)TAKEN?True RQUGE:4.3233
Q1:How was Linguistic Inquiry and Word Count (LIWC) used to analyze textual description part of projects?
Q2:What was liwc used for in previous studies?
Best ans: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. the output of liwc is vd = sd/nt. liwc was used to analyze textual description part of projects in previous studies.
['A0: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. the output of liwc is vd = sd/nt.', 'A1: Using Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. the output of liwc is vd = sd/nt. liwc was used to analyze textual description part of projects in previous studies.', 'A2: The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. as a result, we arrive at much higher model accuracy. Linguistic Inquiry and Word Count (LIWC) analyzes the appearance of dictionary words in a specific text.', 'A3: Linguistic Inquiry and Word Count (LIWC) analysis measures the appearance of dictionary words in a specific text. liwc analyzes the degree to which the text being analyzed is related to the theme of the dictionary. liwct analysis measures a numeric value in the range [0.1]. liwc is used to analyze textual description part of projects.', 'A4: Extracted features', 'A5: Extraction', 'A6: Software tool', 'A7: To extract features from the text']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(28)TAKEN?False RQUGE:4.3199
Q1:Predicting crowdFS
Q2:What is the additional model we propose?
Best ans: Using semantic features, we propose an additional model that incorporates both project metadata and project semantics.
['A0: Model for crowdFS.', 'A1: Model for crowdFS', 'A2: Using semantic features, we propose an additional model that incorporates both project metadata and project semantics.', 'A3: A novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: Modeling crowdFS with semantic features', 'A5: Modeling crowdFS', 'A6: Model for crowdfunding projects in meeting their funding goals.', 'A7: Model for crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(29)TAKEN?True RQUGE:4.2281
Q1:Where did this study find buzzwords?
Q2:What categories of buzzwords were used in this study?
Best ans: Using the buzzword data set, this study found buzzwords in the following categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.
['A0: This study uses a buzzword dataset. the buzzword data is from different categories: general conversation. the data is sorted by category. the final answer: general discussion.', 'A1: Using the buzzword data set, this study found buzzwords in the following categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.', 'A2: In this study, buzzwords were found in the following different categories: general conversation. the buzzword dataset used in this study contains words from different categories. the buzzword dataset is a collection of words from various categories.', 'A3: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. the buzz word dataset used is a list of words from various categories.', 'A4: Business', 'A5: Education', 'A6: The buzzword dataset', 'A7: Science and technology']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(30)TAKEN?True RQUGE:4.1971
Q1:How was the model trained?
Q2:How was the training performed?
Best ans: Using a set of features we use for prediction and the dataset on which learning was applied, we trained a model on all_d with 10-fold cross_validation.
['A0: In this study, the model was trained on all_d with 10-fold cross_validation', 'A1: Using a set of features we use for prediction and the dataset on which learning was applied, we trained a model on all_d with 10-fold cross_validation.', 'A2: Models were developed with the algorithms that were used in the studies above, and with additional algorithms. this training was performed on all_d with 10-fold cross_validation.', 'A3: The model was trained on all_d with 10-fold cross-validation. the model we developed has the highest accuracy.', 'A4: 10-fold cross-validation)', 'A5: 10-fold cross-validation', 'A6: Performed on all_d', 'A7: Trained on all_d with 10-fold cross-validation']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(31)TAKEN?False RQUGE:4.1683
Q1:What is the relationship between Funding Success (FS) and buzzwords?
Q2:What is the relevance of this study
Best ans: This study investigates the relationship between Funding Success (FS) (fs) and buzzwords. the buzzword feature is among the features that are highly correlated to fs. the results of our study are highly relevant to fundraisers using crowdfunding web platforms.
['A0: This study investigates the relationship between Funding Success (FS) (fs) and buzzwords. the buzzword feature is among the features that are highly correlated to fs.', 'A1: This study investigates the relationship between Funding Success (FS) (fs) and buzzwords. the buzzword feature is among the features that are highly correlated to fs. the results of our study are highly relevant to fundraisers using crowdfunding web platforms.', 'A2: Buzzwords feature is among the features that are highly correlated to Funding Success (FS) (fs) compared to other parameters that we examined and by other researchers examined. we developed a novel model based on semantic features only and achieved similar accuracy level as previous studies.', 'A3: We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. the buzzwords feature is among the features that are highly correlated to Funding Success (FS) compared to other parameters that we examined and by other researchers examined.', 'A4: Highly', 'A5: A novel model', 'A6: High correlation', 'A7: Are highly correlated']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(32)TAKEN?False RQUGE:4.1434
Q1:What influences crowdfunding projects’ fundraising success?
Q2:What do we analyze
Best ans: Our model for predicting the crowdFS of crowdfunding projects focuses on semantic features only. in an additional model we developed, we examine both project metadata and project semantics, providing a comprehensive study of factors. we analyze a large dataset of crowdfunding project data, larger than reported in the art, and demonstrate higher accuracy of our model.
['A0: A novel model for crowdfunding projects’ fundraising success.', 'A1: Using semantic features, we propose an additional model that incorporates both project metadata and project semantics. we analyze a large dataset of crowdfunding project data, larger than reported in the art.', 'A2: Our model for predicting the crowdFS of crowdfunding projects focuses on semantic features only. in an additional model we developed, we examine both project metadata and project semantics, providing a comprehensive study of factors. we analyze a large dataset of crowdfunding project data, larger than reported in the art, and demonstrate higher accuracy of our model.', 'A3: Model for crowdfunding projects’ fundraising success. we present a novel model for predicting the success of crowdfunding projects in meeting their funding goals based on semantic features. we analyze a large dataset of crowdfunding project data, larger than reported in the art.', 'A4: Semantic features and metadata.', 'A5: Semantic features', 'A6: Semantic features and metadata', 'A7: Model for crowdfunding projects in meeting their funding goals']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(33)TAKEN?False RQUGE:4.1214
Q1:Combined model: 91.2%
Q2:What is the accuracy level of the semantic-model?
Best ans: Combined model: 91.2%
['A0: Combined model: 91.2%', 'A1: Combined model: 91.2% the semantic-model was developed on the dataset all_d. the model was able to predict the occurrence of the following words:', 'A2: Models were trained on the datasets: all_d, svm, j48, random forest, lightgbm and sdg. the model was compared to the state of art (metadata) models, which exhibited an accuracy level of 91.2 %.', 'A3: For the development of the semantic-model we utilized several machine learning algorithms (including svm, j48, random forest, lightgbm, sdg, dnn, and more) on all_d dataset.', 'A4: Accuracy', 'A5: Results', 'A6: Combined model', 'A7: ']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(34)TAKEN?False RQUGE:4.0951
Q1:What is the main input to the Latent Dirichlet Allocation (LDA) topic model?
Q2:What is one of the three main inputs to the LDA topic model?
Best ans: Dictionary
['A0: Dictionary', 'A1: The corpus', 'A2: Dictionary is the collection of words that are used to describe topics. the three main inputs to the Latent Dirichlet Allocation (LDA) topic model are the dictionary, the corpus, and the number of topics.', 'A3: Dictionary the three main inputs to the Latent Dirichlet Allocation (LDA) topic model are the dictionary, the corpus, and the number of topics.', 'A4: Corpus', 'A5: Vocabulary', 'A6: The dictionary', 'A7: Dictionary']
Text:Latent Dirichlet Allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(35)TAKEN?True RQUGE:4.0013
Q1:Which model has the highest accuracy?
Q2:What does the figure show?
Best ans: Model we developed has the highest accuracy. the accuracy of the semantic-model is similar to that of the Latent Dirichlet Allocation (LDA)-model and the metadata-model.
['A0: Model we developed has the highest accuracy.', 'A1: Semantic-model is similar to that of the Latent Dirichlet Allocation (LDA)-model and the metadata-model.', 'A2: The model we developed has the highest accuracy.', 'A3: Model we developed has the highest accuracy. the accuracy of the semantic-model is similar to that of the Latent Dirichlet Allocation (LDA)-model and the metadata-model.', 'A4: Metadata', 'A5: Model developed', 'A6: Latent Dirichlet Allocation (LDA)-model', 'A7: Model we developed']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(36)TAKEN?True RQUGE:3.8889
Q1:A combined model for Funding Success (FS) in kickstarter and indiegogo projects
Q2:What did we develop?
Best ans: A combined model for Funding Success (FS) (fs) in kickstarter and indiegogo projects.
['A0: In this paper, we combine the following features: buzzwords, sentimental words, feeling words, explanation words, and Latent Dirichlet Allocation (LDA) topics. the combined model is based on the combination of semantic and meta-data features.', 'A1: A combined model for Funding Success (FS) (fs) in kickstarter and indiegogo projects.', 'A2: In this paper, we combine the following features: buzzwords, sentimental words, feeling words, explanation words, and Latent Dirichlet Allocation (LDA) topics. the combined model is based on the combination of semantic and meta-data features. the model is trained on a dataset of 50,000 kickstarter and 50,00 indiegogo projects.', 'A3: To study correlation between buzzwords and Funding Success (FS), we built three datasets: all_d (kickstarter) and market_d(indiegogo) to study the relationship between features and the project category. to study the correlation between features with the project categories we built 3 datasets. to identify the most Significant Set of Features (MSSF) (mssf) that has the highest impact on fs, we use the cfs algorithm. to evaluate the performance, we compared our model’s accuracy to previous research models.', 'A4: Combining semantic and meta-data features to predict FS', 'A5: A combined model for Funding Success (FS) in kickstarter and indiegogo projects.', 'A6: A combined model for Funding Success (FS) in kickstarter and indiegogo projects', 'A7: A combined model for Funding Success (FS) (fs) in kickstarter and indiegogo projects.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(37)TAKEN?False RQUGE:3.8528
Q1:Why did they develop this model?
Q2:What is the relationship between buzzwords and fs?
Best ans: Funding Success (FS) and buzzwords feature is among the features that are highly correlated to fs
['A0: Funding Success (FS) is highly correlated to buzzwords', 'A1: This study investigates the relationship between Funding Success (FS) and buzzwords', 'A2: Funding Success (FS) and buzzwords feature is among the features that are highly correlated to fs', 'A3: The results of our study are highly relevant to fundraisers using crowdfunding web platforms.', 'A4: High relevance', 'A5: To fundraise using crowdfunding web platforms.', 'A6: To fundraise using crowdfunding web platforms', 'A7: Highly relevant to fundraisers using crowdfunding web platforms.']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(38)TAKEN?True RQUGE:3.8395
Q1:Which features are most influential for Funding Success (FS)?
Q2:What are the most influential features for Funding Success?
Best ans: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. we find that the following feature are most influential for Funding Success (FS): buzzwords linguistic inquiry word count word %
['A0: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. we find that the following feature are most influential for Funding Success (FS): buzzwords linguistic inquiry word count word % wc', 'A1: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. we find that the following feature are most influential for Funding Success (FS): buzzwords linguistic inquiry word count word %', 'A2: Using a multi-stage approach, we used the following data to: a) analyze the data; and b) find the features that are most influential for Funding Success (FS) (fs) based on the individual predictive ability of each feature along with the degree of redundancy between them.', 'A3: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. we find that the following feature are most influential for Funding Success (FS): buzzwords linguistic inquiry word count', 'A4: The following feature selection process:', 'A5: Feature selection', 'A6: The following feature selection process', 'A7: Feature selection process']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(39)TAKEN?False RQUGE:3.8037
Q1:Why do they use textual data for semantic analysis?
Q2:What is required for semantic analysis?
Best ans: Textual data is required for semantic analysis. for this reason, the projects’ texts are required.
['A0: They used textual data from projects to model the semantics of their work.', 'A1: Textual data is required for semantic analysis. for this reason, the projects’ texts are required.', 'A2: A project’s texts are required for semantic analysis. for this purpose, the projects’ textual data is required.', 'A3: This is because the projects’ textual data is required for semantic analysis.', 'A4: To understand how the model works.', 'A5: To understand how the model works', 'A6: To measure the accuracy of the models.', 'A7: To measure the accuracy of the models']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(40)TAKEN?False RQUGE:3.8006
Q1:Combined model
Q2:On what dataset was the model evaluated?
Best ans: A model was developed that utilizes a combination of several machine learning algorithms. the model was evaluated on the following datasets: all_d.
['A0: Combined model: the combined model is a model that incorporates the features of the three datasets.', 'A1: A model was developed that utilizes a combination of several machine learning algorithms.', 'A2: A model was developed that utilizes a combination of several machine learning algorithms. the model was evaluated on the following datasets: all_d.', 'A3: Combined model: the combined model is a model that incorporates the features of the three datasets. the model is trained on the same set of semantic features as the baseline model.', 'A4: Models', 'A5: The semantic-model', 'A6: Model', 'A7: Semantic-model']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(41)TAKEN?True RQUGE:3.6689
Q1:The authors used Latent Dirichlet Allocation (LDA) for what purpose?
Q2:What did previous studies on crowdfunding use LDA to perform?
Best ans: Topic analysis on the text of project updates
['A0: Topic analysis', 'A1: Topic analysis on the text of project updates', 'A2: Performing a topic analysis on the text of project updates.', 'A3: Topic modeling method to perform topic analysis on the text of project updates.', 'A4: Performance', 'A5: Performing text mining', 'A6: Performance topic modelling', 'A7: Conduct research']
Text:Latent Dirichlet Allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(42)TAKEN?True RQUGE:3.6635
Q1:How were semantic features used?
Q2:What machine learning algorithms were used?
Best ans: Semantic features were used as input. we utilized several machine learning algorithms (including svm, j48, random forest, lightgbm, sdg, dnn, and more) on all_d.
['A0: Input: we utilized several machine learning algorithms (including svm, j48, random forest ad lightgbm ) on all_d. table 2 presents the accuracies metrics of the combined-model:', 'A1: Several machine learning algorithms (including svm, j48, random forest, lightgbm, sdg, dnn and more) on all_d.', 'A2: For the development of the semantic-model we utilized several machine learning algorithms (including svm, j48, random forest, lightgbm, sdg, dnn, and more) on all_d.', 'A3: Semantic features were used as input. we utilized several machine learning algorithms (including svm, j48, random forest, lightgbm, sdg, dnn, and more) on all_d.', 'A4: Machine learning algorithms', 'A5: As input', 'A6: Input', 'A7: Several machine learning algorithms']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(43)TAKEN?True RQUGE:3.5672
Q1:Which factors influence the Funding Success (FS) of crowdfunding projects?
Q2:What dataset did we analyze?
Best ans: In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics. we analyze large dataset of crowdfunding project data, larger than reported in the art.
['A0: Model for crowdfunding projects in meeting their funding goals. model for predicting the success of crowdfunding projects by combining semantic features and metadata.', 'A1: In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics.', 'A2: In this paper, we present a novel model for crowdfunding projects in meeting their funding goals, which examines both project metadata and project semantics. we analyze large dataset of crowdfunding project data, larger than reported in the art.', 'A3: Using the dataset of crowdfunding platforms, we present a novel model for predicting the success of crowdfunding projects in meeting their funding goals.', 'A4: Metadata', 'A5: Semantic features only', 'A6: Semantic features', 'A7: Project metadata']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(44)TAKEN?True RQUGE:3.5374
Q1:How many buzzwords were included?
Q2:The relationship between funding success and buzzwords used in the description of the project was what?
Best ans: A dataset of buzzwords was used in this study. the buzzword dataset used in the study contains words from different categories: general conversation. the buzzword dataset was a collection of words from various categories. the number of buzz words in the dataset is not examined. the final answer: not examined
['A0: There were a total of 0 buzzwords in the dataset.', 'A1: In the buzzword dataset used in this study there were words from different categories: general conversation. the buzzwords in the dataset are listed below:', 'A2: A dataset of buzzwords was used in this study. the buzzword dataset used in the study contains words from different categories: general conversation. the buzzword dataset was a collection of words from various categories. the number of buzz words in the dataset is not examined. the final answer: not examined', 'A3: A dataset of buzzwords was used in this study. the buzzword dataset used in the study contains words from different categories: general conversation. the buzzword dataset was a collection of words from various categories. the number of buzz words in the dataset is not examined. the final answer: not examined ( ).', 'A4: 10', 'A5: Four', 'A6: Seven', 'A7: Six']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(45)TAKEN?False RQUGE:3.5374
Q1:How many buzzwords were included?
Q2:The relationship between funding success and buzzwords used in the description of the project was what?
Best ans: A dataset of buzzwords was used in this study. the buzzword dataset used in the study contains words from different categories: general conversation. the buzzword dataset was a collection of words from various categories. the number of buzz words in the dataset is not examined. the final answer: not examined
['A0: There were a total of 0 buzzwords in the dataset.', 'A1: In the buzzword dataset used in this study there were words from different categories: general conversation. the buzzwords in the dataset are listed below:', 'A2: A dataset of buzzwords was used in this study. the buzzword dataset used in the study contains words from different categories: general conversation. the buzzword dataset was a collection of words from various categories. the number of buzz words in the dataset is not examined. the final answer: not examined', 'A3: A dataset of buzzwords was used in this study. the buzzword dataset used in the study contains words from different categories: general conversation. the buzzword dataset was a collection of words from various categories. the number of buzz words in the dataset is not examined. the final answer: not examined ( ).', 'A4: 10', 'A5: Four', 'A6: Seven', 'A7: Six']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(46)TAKEN?False RQUGE:3.5178
Q1:Which metric is used to measure the accuracy of the models?
Q2:What is required for semantic analysis?
Best ans: We present the metric that is used to measure the accuracy. for semantic analysis, we use the projects’ textual data.
['A0: For the semantics analysis, we use textual data from projects.', 'A1: We present the metric that is used to measure the accuracy. for semantic analysis, we use the projects’ textual data.', 'A2: We present the metric that is used to measure the accuracy. for semantic analysis, we use the projects’ textual data. so, the final answer is semantic analysis.', 'A3: The metric used to measure the accuracy of the models is the following: semantic analysis, the projects’ textual data.', 'A4: Metric', 'A5: Sem', 'A6: Accuracy', 'A7: Semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(47)TAKEN?False RQUGE:3.5064
Q1:In which metrics would you find f-score?
Q2:What is the meaning of F-score?
Best ans: F-score is the weighted harmonic mean of a model’s precision and recall. recall is the ratio of correctly predicted positive records to all records in actual class.
['A0: F-score is the weighted harmonic mean of a model’s precision and recall.', 'A1: To measure performance, you would find f-score in the following metrics: recall, precision, and recall.', 'A2: F-score is the weighted harmonic mean of a model’s precision and recall. recall is the ratio of correctly predicted positive records to all records in actual class.', 'A3: Model’s precision and recall are the same. f-score is the weighted harmonic mean of a model’s accuracy and recall. therefore, the final answer is precision and remember.', 'A4: Repeat', 'A5: Reliability', 'A6: Performance', 'A7: Accuracy']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(48)TAKEN?False RQUGE:3.3544
Q1:How are features used in the model?
Q2:What is required for semantic analysis?
Best ans: To model the semantics, we use features from the projects’ textual data.
['A0: To model the semantics, we use features from the projects’ textual data.', "A1: To model the semantics, we use features from the projects’ textual data. for semantic analysis, the projects' texts have to be analyzed.", 'A2: For semantic analysis the projects’ textual data is required.', 'A3: The following features are used in the model: the following are the features used in our models: the metric used to measure the accuracy of the model is the metric for the model.', 'A4: The projects’ textual data', 'A5: Semantic analysis, the projects’ textual data', 'A6: Semantic analysis', 'A7: Highlighting where we differ from previous studies']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(49)TAKEN?False RQUGE:3.2955
Q1:In which datasets were the models trained?
Q2:On what dataset was training performed?
Best ans: All_d with 10-fold cross-validation
['A0: A: all_d', 'A1: All_d with 10-fold cross-validation.', 'A2: All_d', 'A3: All_d with 10-fold cross-validation', 'A4: Alld', 'A5: Latent Dirichlet Allocation (LDA)-model', 'A6: , all_d', 'A7: Svm, j48']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(50)TAKEN?True RQUGE:3.2886
Q1:What datasets were used to study the relationship between features and the project category?
Q2:What are the three datasets used to study the relationship between features and project category?
Best ans: All_d, market_d and tech_d datasets.
['A0: Kickstarter and indiegogo datasets.', 'A1: Kickstarter and indiegogo datasets', 'A2: 50,000 kickstarter and 50,00 indiegogo projects', 'A3: All_d, market_d and tech_d datasets.', 'A4: Market_d', 'A5: All', 'A6: Market', 'A7: The combined-model']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(51)TAKEN?True RQUGE:3.2275
Q1:Which features are most important for Funding Success (FS)? how are they combined?
Q2:What was the test dataset used to train the model?
Best ans: A large dataset of kickstarter and indiegogo projects was used to train the model, which is a combination of semantic and meta-data features. the model was trained on the dataset of 50,000 kickstarter projects and 50,00 indiegog projects. the models were trained on a 10-fold cross-validation test.
['A0: In this paper, we combine the following features: buzzwords, sentimental words, feeling words and explanation words.', 'A1: A large dataset of kickstarter and indiegogo projects was used to train the model', 'A2: A large dataset of kickstarter and indiegogo projects was used to train the model, which is a combination of semantic and meta-data features. the model was trained on the dataset of 50,000 kickstarter projects and 50,00 indiegog projects.', 'A3: A large dataset of kickstarter and indiegogo projects was used to train the model, which is a combination of semantic and meta-data features. the model was trained on the dataset of 50,000 kickstarter projects and 50,00 indiegog projects. the models were trained on a 10-fold cross-validation test.', 'A4: Buzzwords and meta-data features', 'A5: The combination of semantic and meta-data features.', 'A6: The combination of semantic and meta-data features', 'A7: The combined-model, whose novelty lies in the combination of semantic and meta-data features.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(52)TAKEN?True RQUGE:3.2095
Q1:Whose textual data is required for semantic analysis?
Q2:What type of textual data is required for semantic analysis?
Best ans: Projects
['A0: To analyze the textual data, the following entities are needed: projects.', 'A1: Projects are the textual data of the projects. for semantic analysis, the projects’ texts are required.', 'A2: We use text from projects to model our model. for semantic analysis, the projects’ text is required.', 'A3: For semantic analysis, the projects’ textual data is required.', 'A4: Authors', 'A5: Project', 'A6: Projects’', 'A7: Projects']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(53)TAKEN?True RQUGE:3.2039
Q1:Which features are most important for Funding Success (FS)?
Q2:What was developed to combine the features of the three datasets?
Best ans: A large dataset of kickstarter and indiegogo projects was used to train the model to predict Funding Success (FS) (fs). the model was trained on the following datasets: all_d, market_d and tech_d. the model performed well in predicting fs. the combined model was developed to combine the features of the three dataset.
['A0: To study Funding Success (FS), we used the following features: buzzwords, linguistic inquiry, word count, feelings words, explanation words and LDA', 'A1: A large dataset of kickstarter and indiegogo projects was used to train the model to predict Funding Success (FS) (fs). the model was trained on the following datasets:', 'A2: A large dataset of kickstarter and indiegogo projects was used to train the model to predict Funding Success (FS) (fs). the model was trained on the following datasets: all_d, market_d and tech_d. the model performed well in predicting fs. the combined model was developed to combine the features of the three dataset.', 'A3: A large dataset of kickstarter and indiegogo projects was used to train the model to predict Funding Success (FS) (fs). the model was trained on the following datasets: all_d, market_d and tech_d. the model performed well in predicting fs.', 'A4: Buzzword', 'A5: Cfs', 'A6: Buzzwords', 'A7: The combined-model']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(54)TAKEN?True RQUGE:3.1859
Q1:Where was a high accuracy model for prediction model?
Q2:On what datasets did we utilize machine learning algorithms?
Best ans: All_d datasets.
['A0: All_d datasets.', 'A1: All_d datasets. the lightgbm algorithm exhibited a high accuracy level for all three datasets in this study.', 'A2: Lightgbm algorithm exhibited a high accuracy level for all three datasets. in particular the accuracy of the prediction model is greater than 94% for all datasets', 'A3: In particular, the accuracy of the prediction model is greater than 94% for all datasets.', 'A4: Lightgbm', 'A5: Datasets', 'A6: On all_d', 'A7: All_d']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(55)TAKEN?False RQUGE:3.1551
Q1:How are f-score and recall measured?
Q2:What is the meaning of F-score?
Best ans: F-score is the weighted harmonic mean of a model’s precision and recall. f1 is the weighted harmonic mean of f and r1. recall and f1 are the same.
['A0: To measure the performance of a model, f-score is the weighted harmonic mean (f1) of f-scor and recall.', 'A1: The weighted harmonic mean of a model’s precision and recall is f1. the weight of f-score is the weighteded harmonic sum of m.', 'A2: F-score is the weighted harmonic mean of a model’s precision and recall. f1 is the weighted harmonic mean of f and r1.', 'A3: F-score is the weighted harmonic mean of a model’s precision and recall. f1 is the weighted harmonic mean of f and r1. recall and f1 are the same.', 'A4: Accuracy', 'A5: By a model', 'A6: Accuracy and recall', 'A7: A model’s precision and recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(56)TAKEN?True RQUGE:3.1484
Q1:How are the features selected?
Q2:What does the cfs algorithm evaluate?
Best ans: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. the cfs algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.
['A0: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process.', 'A1: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. the cfs algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them. the semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets.', 'A2: This paper describes the data setup, the usage of Latent Dirichlet Allocation (LDA) algorithm, the feature correlation analysis, and the feature selection process. the cfs algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.', 'A3: Using a Latent Dirichlet Allocation (LDA) algorithm, we evaluate subsets of features based on individual predictive ability of each feature along with the degree of redundancy between them. the semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets.', 'A4: Feature selection process', 'A5: We found the features that are most influential for FS', 'A6: Based on the individual predictive ability of each feature along with the degree of redundancy between them', 'A7: Cfs algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(57)TAKEN?False RQUGE:3.0724
Q1:What are the buzzwords used in the buzz word dataset?
Q2:What are some examples of buzzwords used in the study?
Best ans: Buzzwords in the buzz word dataset include: general conversation, education, business, sales and marketing, science, and technology. the buzzword dataset used in this study contains words from different categories: general discussion, education.
['A0: Buzzwords in the buzz word dataset include: general conversation, education, business, sales and marketing, science, and technology. the buzzword dataset used in this study contains words from different categories: general discussion, education.', 'A1: In this study the buzzwords are: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. the buzzword dataset used in this study contains words from different categories: general discussion, education.', 'A2: General conversation, education, business, sales and marketing, science and technology, politics and current affairs. the buzzword dataset used in this study contains words from different categories: general conversation, educational, business. the buzzword dataset contains words in different categories.', 'A3: The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. the buzz word dataset used is a list of words from various categories.', 'A4: Marketing', 'A5: Politics', 'A6: Political', 'A7: Business']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(58)TAKEN?True RQUGE:3.0034
Q1:Addition to: feeling
Q2:What are words that describe a feeling?
Best ans: Words that describe feeling include: awed. awed is a feeling.
['A0: Addition to feeling:', 'A1: Words that describe feeling include: awed.', 'A2: Words that describe feeling include: awed. awed is a feeling.', 'A3: The following are some of the emotions you might have if you were a child:', 'A4: Added, explain', 'A5: Addition to and', 'A6: Description of emotion', 'A7: Words and emotions']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(59)TAKEN?False RQUGE:2.9522
Q1:Is buzzwords an important feature of Funding Success (FS)?
Q2:What is the relevance of the study
Best ans: Funding Success (FS) refer to the success of crowdfunding campaigns. the buzzwords feature is among the features that are highly correlated to fs. the results are high relevant to fundraisers using crowdfunding web platforms and could be further improved by considering the characteristics.
['A0: Buzzwords are among the features that are highly correlated to Funding Success (FS) (fs). the buzzwords feature is among the most important features in fs. the buzz words feature is highly influenced by the crowdfunding platform.', 'A1: Funding Success (FS) refer to the success of crowdfunding campaigns. the buzzwords feature is among the features that are highly correlated to fs. the results are high relevant to fundraisers using crowdfunding web platforms and could be further improved by considering the characteristics.', 'A2: The buzzwords feature is among the features that are highly correlated to Funding Success (FS) (fs). the buzz words feature is a feature that is highly reflected in the FS model. the buzz word feature is highly relevant to fundraisers using crowdfunding web platforms.', 'A3: Funding Success (FS) refer to the success of crowdfunding campaigns. the buzzwords feature is among the features that are highly correlated to fs. the results are high relevant to fundraisers using crowdfunding web platforms and could be further improved by considering the characteristics. of images, video content and the semantics of video scripts.', 'A4: Fs', 'A5: High', 'A6: Yes', 'A7: Highly correlated']
Text:The study is the first that investigates the relationship between Funding Success and buzzwords. The buzzwords feature is among the features that are highly correlated to FS compared to other parameters that we examined and that other researchers examined. We developed a novel model based on semantic features only and achieved similar accuracy level as previous studies. We also developed a prediction model with an impressive Fscore of 96.2%, focusing on both project-specific aspects and semantics of project descriptions. The results of our study are highly relevant to fundraisers using crowdfunding web platforms.  Future research could further improve accuracy by considering the characteristics of images, video content and the semantics of video scripts. Further improvements in the model’s performance could be achieved via novel feature selection algorithms.



(60)TAKEN?False RQUGE:2.9398
Q1:A novel model for crowdfunding project Funding Success (FS).
Q2:What is the model based on
Best ans: The paper presents a novel model for crowdfunding project Funding Success (FS) (fs) that combines both semantic features and project metadata. the model is based on a large dataset of crowdfunding project data.
['A0: This work proposes a novel model for crowdfunding project Funding Success (FS) (fs) that combines both semantic features and project metadata.', 'A1: The paper presents a novel model for crowdfunding project Funding Success (FS) (fs) that combines both semantic features and project metadata.', 'A2: Using large dataset of crowdfunding project data, we show that when combining metadata and semantics, we arrive at f1 score accuracy of 96.2%.', 'A3: The paper presents a novel model for crowdfunding project Funding Success (FS) (fs) that combines both semantic features and project metadata. the model is based on a large dataset of crowdfunding project data.', 'A4: Modeling crowdfunding projects', "A5: Modeling crowdfunding projects' FS", 'A6: A novel model for crowdfunding project Funding Success (FS) (fs) :', 'A7: A novel model for crowdfunding project Funding Success (FS) (fs)']
Text:Crowdfunding platforms allow entrepreneurs to publish projects and raise funds for realizing them. Hence, the question of what influences projects’ fundraising success is very important. Previous studies examined various factors such as project goals and project duration that may influence the outcomes of fundraising campaigns. We present a novel model for predicting the success of crowdfunding projects in meeting their funding goals. Our model focuses on semantic features only, whose performance is comparable to that of previous models. In an additional model we developed, we examine both project metadata and project semantics, delivering a comprehensive study of factors influencing crowdFS. Further, we analyze a large dataset of crowdfunding project data, larger than reported in the art. Finally, we show that when combining semantics and metadata, we arrive at F1 score accuracy of 96.2%. We compare our model’s accuracy to the accuracy of previous research models by applying their methods on our dataset, and demonstrate higher accuracy of our model. In addition to our scientific contribution, we provide practical recommendations that may increase project Funding Success chances.



(61)TAKEN?False RQUGE:2.881
Q1:To measure how well a model perform, what metrics are used?
Q2:What is the f-score?
Best ans: F-score is the weighted harmonic mean of a model’s precision and recall. recall is the ratio of correctly predicted positive records to the all records in actual class. f-score is used to measure how well a model perform.
['A0: To measure how well a model perform, f-score is used. f-score is the weighted harmonic mean of a models precision and recall.', 'A1: F-score is the weighted harmonic mean of a model’s precision and recall. recall is the ratio of correctly predicted positive records to the all records in actual class. f-score is used to measure how well a model perform.', 'A2: Using the ratio of correctly predicted positive records to the all records in actual class, f-score is the weighted harmonic mean of a model’s precision and recall.', 'A3: The f-score is the weighted harmonic mean of a model’s precision and recall. recall is the ratio of correctly predicted positive records to the all records in actual class. f-score is also denoted f1.', 'A4: Reliability', 'A5: Precision', 'A6: Accuracy', 'A7: Precision and recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(62)TAKEN?False RQUGE:2.813
Q1:Which datasets used Latent Dirichlet Allocation (LDA)?
Q2:What datasets had the highest coherence values?
Best ans: All_d dataset all_d dataset tech_d and market_d data
['A0: Markets and crowdfunding', 'A1: All_d dataset all_d dataset tech_d and market_d data', 'A2: All_d dataset all_d dataset tech_d and market_d', 'A3: We used the all_d dataset, the tech_d and market_d datasets.', 'A4: Marketd', 'A5: Tech', 'A6: All', 'A7: Market']
Text:Latent Dirichlet Allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(63)TAKEN?False RQUGE:2.8025
Q1:The authors used what to measure their models?
Q2:What is required for semantic analysis?
Best ans: Metric is used to measure the accuracy of the model. for semantic analysis, the projects’ textual data is required.
['A0: Authors use a semantic analysis metric to measure their models.', 'A1: Metric is used to measure the accuracy of the model. for semantic analysis, the projects’ textual data is required.', 'A2: The authors used the following metric to measure their models: the authors use the following features to measure the accuracy of their models.', 'A3: Metric is used to measure the accuracy of the model. for semantic analysis, the projects’ textual data is required. the final answer: metric.', 'A4: Metric', 'A5: Textual data', 'A6: Accuracy of the models', 'A7: Semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(64)TAKEN?False RQUGE:2.6751
Q1:How do we predict the success of crowdfunding projects?
Q2:What unique features do we use to predict FS?
Best ans: Using the largest dataset available, we develop models that predict Funding Success (FS) for crowdfunding projects. we use unique features (for example, the use of buzzwords) and perform extensive research on semantic characteristics of the projects’ text.
['A0: Using the largest dataset available, we develop models that predict Funding Success (FS) for crowdfunding projects.', 'A1: Using the largest dataset available, we develop models that predict Funding Success (FS) for crowdfunding projects. we use unique features (for example, the use of buzzwords) and perform extensive research on semantic characteristics of the projects’ text.', 'A2: In this paper, we use unique features (for example, the use of buzzwords) and perform extensive research on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding, to provide high-quality Funding Success (FS) prediction', 'A3: A new approach to studying and predicting the success of crowdfunding projects. we use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. we show that buzzword use, linguistic inquiry, and word count are among the highly correlate features with project’s success in fund raising.', 'A4: Use of buzzwords and linguistic inquiry', 'A5: Use of buzzwords and linguistic inquiry to predict FS', 'A6: Use of buzzwords and linguistic inquiry to predict Funding Success (FS) of crowdfunding projects', 'A7: Use unique features (for example, the use of buzzwords) and perform extensive research on the semantics']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(65)TAKEN?False RQUGE:2.6551
Q1:Which metrics are used to measure the performance of our models?
Q2:What metrics do we use to measure the performance of our models?
Best ans: Recall is the ratio of correctly predicted positive records to the all records in actual class. f-score (also denoted f1) is the weighted harmonic mean of a model’s precision and recall. the weightes harmonic mean is f1.
['A0: F-score is the weighted harmonic mean of a model’s precision and recall. f-score is used to measure the performance of our models. the weightes harmonic mean is f1.', 'A1: F-score is the weighted harmonic mean of a model’s precision and recall. f-score is used to measure the performance of our models.', 'A2: We used recall and f-score to measure performance of our models. recall is the ratio of correctly predicted positive records to all records in actual class. f-score is the weighted harmonic mean of a model’s precision and recall.', 'A3: Recall is the ratio of correctly predicted positive records to the all records in actual class. f-score (also denoted f1) is the weighted harmonic mean of a model’s precision and recall. the weightes harmonic mean is f1.', 'A4: Reliability', 'A5: R', 'A6: Precision', 'A7: Recall']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(66)TAKEN?False RQUGE:2.5727
Q1:Feature selection process
Q2:What are the features that have a higher correlation in all datasets?
Best ans: To find features that are most influential, we used the following method. the following features are evaluated: the following are features that have a higher correlation in all datasets: the semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that has the highest correlation in each dataset: the feature number is unique for the r_tech _d top 10 list.
['A0: To find features that are most influential, we used the following method.', 'A1: This paper describes the feature selection process for Funding Success (FS).', 'A2: To find features that are most influential, we used the following method. the following features are evaluated: the following are features that have a higher correlation in all datasets: the semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that has the highest correlation in each dataset: the feature number is unique for the r_tech _d top 10 list. the feature wc is unique in the r _tech top 10 lists.', 'A3: To find features that are most influential, we used the following method. the following features are evaluated: the following are features that have a higher correlation in all datasets: the semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that has the highest correlation in each dataset: the feature number is unique for the r_tech _d top 10 list.', 'A4: Description of datasets', 'A5: Description of datasets and methodology', 'A6: Feature selection process for FS', 'A7: Description']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(67)TAKEN?False RQUGE:2.546
Q1:What are the three datasets used?
Q2:What datasets were used in this study?
Best ans: In this study, we utilized three datasets: all_d. the semantic-model was developed on the datasets all_d.
['A0: In this study, we utilized three datasets: all_d.', 'A1: In this study, we utilized three datasets: all_d. the semantic-model was developed on the datasets all_d.', 'A2: All_d is a dataset of semantic features. all_d is based on the dataset of the same name.', 'A3: The semantic-model was developed on the datasets all_d, a dataset of semantic features, and the dataset of the state of the art (metadata) models.', 'A4: All_d.', 'A5: Datasets', 'A6: Combined model', 'A7: All three datasets']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(68)TAKEN?False RQUGE:2.4689
Q1:What are the emotions that are deduced from the following statements?
Q2:What are some feelings words?
Best ans: Anxious, affectionate, anxious, awed, amazed, aggravated
['A0: Adequate, affectionate, anxious, awed, aggravated, annoyed', 'A1: Anxious, affectionate, anxious, awed, amazed, aggravated.', 'A2: Anxious, affectionate, anxious, awed, amazed, aggravated', 'A3: Adequate, affectionate, anxious, awed, aggravated', 'A4: Alarmed', 'A5: Angry', 'A6: Embarrassed', 'A7: Awkward']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(69)TAKEN?False RQUGE:2.4643
Q1:Additional topics for the following sentence:
Q2:What are some feelings words?
Best ans: Alarmed, awed, aggravated
['A0: Awed, awkward', 'A1: Alarmed, awed, aggravated', 'A2: Answer: awed', 'A3: Alarmed, awed', 'A4: Anxious', 'A5: Explain, alarmed', 'A6: Word: awed', 'A7: Describe']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(70)TAKEN?False RQUGE:2.3882
Q1:Addition to:
Q2:What are the words for awed and amazed?
Best ans: Addition to: feeling
['A0: Awed, astonished, aggravated', 'A1: Awed, astonished, aggravated and annoyed are the most common feelings.', 'A2: Addition to add to add.', 'A3: In this article, we will discuss the following emotions: awed.', 'A4: Anxious', 'A5: Answer: awed', 'A6: Addition to: feeling', 'A7: Addition to:']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(71)TAKEN?False RQUGE:2.305
Q1:Why did he explain something?
Q2:What was the person explaining to someone because they were confused?
Best ans: The person was explaining something to someone because they were confused.
['A0: Because is to explain. awed is to amaze someone.', 'A1: The person was explaining something to someone because they were confused.', 'A2: He explained something to explain it.', 'A3: For instance, he explained that he meant to explain something in other words. awed, awed.', 'A4: Annoyed', 'A5: Awed', 'A6: Reason', 'A7: For instance']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(72)TAKEN?False RQUGE:2.2804
Q1:A model for predicting crowdFS
Q2:What is proposed in this paper?
Best ans: A model for crowdFS (f) is proposed.
['A0: A model for crowdFS (f)', 'A1: A model for crowdFS (f) is proposed.', 'A2: An approach to studying and predicting crowdFS.', 'A3: Using the largest dataset available, we develop a model for predicting crowdFS.', 'A4: Modeling crowdFS', 'A5: A model for predicting crowdfs:', 'A6: A model for predicting crowdfs', 'A7: ']
Text:In recent years, crowdfunding has emerged as a popular financing mechanism that allows various types of projects to get funded. Three main parties are involved in crowdfunding: entrepreneurs (and their projects), individuals or groups who support the project, and online platforms, i.e., crowdfunding websites. The average success rate of funded crowdfunding projects in Kickstarter is 37.5% [4] In this paper, we refer to a project as a Funding Success (FS) if it achieved its funding goal (i.e.  In contrast to prior studies, we take a more comprehensive approach to studying and predicting FS. We use unique features (for example, the use of buzzwords) and perform extensive research on the semantics of the projects’ text. Our main research objective is to find characteristics, with a focus on semantic characteristics, which affect the success of a project in meeting its funding goal via crowdfunding. This approach leverages known results and improves upon them to provide high-quality FS prediction.  Achieving these research objectives should provide new insights into the relationship between crowdfunding project’s data and their FS. This should provide entrepreneurs with additional and desirable ways to improve their chances of reaching their funding goals. The accuracy of the semantic-model (in terms of F-score and correctly classified instances) is comparable to the accuracy of state-of-the-art models which are based on metadata features such as the number of updates, number of images, etc.  The accuracy of the combined-model is higher than the accuracy derived in previous studies that use other models [11–14] Fourth, we show that buzzwords and Linguistic Inquiry and Word Count are among the highly correlate features with the project’s success in fund raising. Fifth, to the best of our knowledge, our research relies on the largest dataset used to date for developing models that predict FS of crowdfunding projects. In addition to the scientific contributions listed above, we provide a set of recommendations that may increase project FS chances.  The Methodology section focuses on the research methodology applied, including the acquisition of our dataset, preprocessing, feature extraction and selection. The Results section presents the empirical evaluation of our research model and discusses the results. It further compares our models to models in earlier studies.



(73)TAKEN?False RQUGE:2.1917
Q1:The paper presents a dataset of 50,000 kickstarter and 50,00 indiegogo projects for 2018.
Q2:What is the purpose of the division?
Best ans: The dataset used in this paper is a dataset of 50,000 kickstarter and 50,00 indiegogo projects for 2018. we obtained these data from the kaggle website for 2018. for each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count (LIWC) outputs, feelings words, explanation words, and Latent Dirichlet Allocation (LDA) outputs. we removed lda topics that overlapped with liwc topics by comparing the topic sets. to study the relationship between features and the project category, we built three datasets: all_d, tech_d and market_d. the division aims to address the analysis
['A0: Using the kaggle dataset, we obtained a dataset of 50,000 kickstarter and 50,00 indiegogo project. we obtained the values of five metadata features and about 120 semantic features, including buzzwords, linguistic inquiry outputs, feelings words, explanation words and Latent Dirichlet Allocation (LDA) outputs. we removed lda topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing the topic sets. we built three datasets: all_d, tech_d and market_d. the original features space was reduced into three Principal Components (PCs). to identify the most Significant Set of Features (MSSF) (mssf), we use the cfs (correlation-based feature selection) algorithm. the models’', 'A1: This paper presents a dataset of kickstarter and indiegogo projects for 2018. we obtained these data from the kaggle website for 2018. for each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords. we removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing the topic sets. to study the relationship between features and the project category we built three datasets: all_d, tech_d and market_d. the division aims to address the analysis goals: first, it facilitates improvement in the lda topic coherence score. second, it allows us to determine whether a category affects the features that are highly correlated with FS', 'A2: A large dataset of 50,000 kickstarter and 50,00 indiegogo projects for 2018. we obtained these data from the kaggle website for 2018. for each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count (LIWC) outputs, feelings words, explanation words, and Latent Dirichlet Allocation (LDA) outputs. we removed lda topics that overlapped with liwc topics by comparing the topic sets. we built three datasets: all_d, tech_d and market_d. the original features space was reduced into three Principal Components (PCs) (pcs) the sum of the explained variance ratio of these three pcs is', 'A3: The dataset used in this paper is a dataset of 50,000 kickstarter and 50,00 indiegogo projects for 2018. we obtained these data from the kaggle website for 2018. for each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count (LIWC) outputs, feelings words, explanation words, and Latent Dirichlet Allocation (LDA) outputs. we removed lda topics that overlapped with liwc topics by comparing the topic sets. to study the relationship between features and the project category, we built three datasets: all_d, tech_d and market_d. the division aims to address the analysis', 'A4: We used a dataset of 50,000 kickstarter projects and 50,00 indiegogo campaigns.', 'A5: A dataset of kickstarter and indiegogo projects for 2018. a dataset for the combined model', 'A6: A dataset of kickstarter and indiegogo projects for 2018.', 'A7: A dataset of 50,000 kickstarter and 50,00 indiegogo projects for 2018.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(74)TAKEN?False RQUGE:2.1516
Q1:Who was Funding Success (FS)?
Q2:What is the buzzword dataset used in a study?
Best ans: Buzzwords used in this study are words from different categories. the buzzword dataset used in the study contains words from the following categories: general conversation. the buzzword dataset is used in a study. the final answer: Funding Success (FS).
['A0: In this study, buzzwords were used in the description of the project to describe the buzzword dataset. the buzzword data was used in this study to describe buzzword in the buzz word dataset. so the final answer is no.', 'A1: Buzzwords used in this study are words from different categories. the buzzword dataset used in the study contains words from the following categories: general conversation. the buzzword dataset is used in a study. the final answer: Funding Success (FS).', 'A2: A buzzword dataset was used in this study. buzzwores were words from different categories. the buzzword dataset used in the study contains words from general conversation, education, business, sales and marketing, science and technology, politics, and current affairs. so, the final answer is no.', 'A3: ', 'A4: Founder', 'A5: Project', 'A6: Nonprofit', 'A7: Nonprofit organization']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



(75)TAKEN?False RQUGE:1.9536
Q1:How do they measure coherence?
Q2:What does the LDA algorithm consider?
Best ans: Latent Dirichlet Allocation (LDA) algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. to avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of topics.
['A0: Using Latent Dirichlet Allocation (LDA), we used gensim python package lda implementation to execute lda. to avoid overfitting, we chose the one that derived the highest coherence value and. the smallest number of topics.', 'A1: Latent Dirichlet Allocation (LDA) algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. to avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of topics. there were 30 topics that  derived the highest.', 'A2: Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] to avoid overfitting, we chose the one that derived the highest coherency value and the smallest number of topics.', 'A3: Latent Dirichlet Allocation (LDA) algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. to avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of topics.', 'A4: Topic coherence', 'A5: Model coherence', 'A6: The highest coherence value and smallest number of topics', 'A7: To judge how good a given topic model is [16]']
Text:Latent Dirichlet Allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(76)TAKEN?False RQUGE:1.7539
Q1:What are some of the algorithms used in this study?
Q2:What was the training performed on?
Best ans: Svm, j48, random forest, lightgbm, sdg, and dnn. this training was performed on all_d with 10-fold cross-validation.
['A0: Lsvm, j48, random forest, lightgbm, sdg, and dnn.', 'A1: Lsvm, j48, random forest, lightgbm, sdg, and dnn', 'A2: Svm, j48, random forest, lightgbm, sdg, and dnn. this training was performed on all_d with 10-fold cross-validation.', 'A3: Svm, j48, random forest, lightgbm, sdg, and dnn.', 'A4: J48', 'A5: Semantic-model', 'A6: Dnn', 'A7: Latent Dirichlet Allocation (LDA)-model']
Text:The study aims to examine whether the set of features we use for prediction and the dataset on which learning was applied deliver a better model by means of F-score accuracy. We trained the Latent Dirichlet Allocation-Model and the Metadata-Model with the algorithms that were used in the studies above, and with additional algorithms, including SVM, J48, Random forest, LightGBM, SDG, and DNN. This training was performed on All_D with 10-fold cross-validation.  F-score is used for consistency with the earlier studies to which we compare. Figure shows that the model we developed has the highest accuracy. The accuracy of the Semantic-Model is similar to that of the LDA-Model and the Metadata-Model.



(77)TAKEN?False RQUGE:1.7208
Q1:Are there any other feelings that you can deduce from the following statement?
Q2:is the sentence about explaining something?
Best ans: The sentence is about explaining something. the sentence has no other feelings. the answer: no.
['A0: Angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, amazed.', 'A1: Angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, amazed', 'A2: Absurdity is a feeling that is not explained. awed is derived from the word awe.', 'A3: The sentence is about explaining something. the sentence has no other feelings. the answer: no.', 'A4: Alarmed', 'A5: Yes', 'A6: No', 'A7: ']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(78)TAKEN?False RQUGE:1.6044
Q1:What is a feature of the cfs algorithm?
Q2:What is the difference between the top 10 features across datasets
Best ans: Among the top 10 features, about 70% are the same across datasets. for example, the features feelings_num and buzz_num appear in the three top 10 lists. in contrast, the parameter wc is unique in the r_tech _d top 10 list and the parameter number is distinctive
['A0: Among the top 10 features, about 70% are the same across datasets.', 'A1: Among the top 10 features, about 70% are the same across datasets. for example, the features feelings_num and buzz_num appear in the three top 10 lists. in contrast, the parameter wc is unique in the r_tech _d top 10 list and the parameter number is distinctive.', 'A2: Among the top 10 features, about 70% are the same across datasets. for example, the features feelings_num and buzz_num appear in the three top 10 lists. in contrast, the parameter wc is unique in the r_tech _d top 10 list and the parameter number is distinctive', 'A3: Cfs algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them. the semantic features buzzwords and Linguistic Inquiry and Word Count (LIWC) are among the features that have a higher correlation in all datasets.', 'A4: Feature selection process.', 'A5: Feature selection process', 'A6: Features correlated to Funding Success (FS) are dependent on the project category', 'A7: The features correlated to Funding Success (FS) are dependent on the project category']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(79)TAKEN?False RQUGE:1.6044
Q1:How many features are the same across dataset
Q2:What is the difference between the top 10 features across datasets?
Best ans: Among the top 10 features, about 70% are the same across datasets. for example, the features feelings_num and buzz_num appear in the three top 10 lists. in contrast, the parameter wc is unique in the r_tech _d top 10 list and the parameter number is distinctive.
['A0: We used the following dataset:', 'A1: A dataset of ten companies was used to evaluate Funding Success (FS). the data was collected from the following sources:', 'A2: Among the top 10 features, about 70% are the same across datasets.', 'A3: Among the top 10 features, about 70% are the same across datasets. for example, the features feelings_num and buzz_num appear in the three top 10 lists. in contrast, the parameter wc is unique in the r_tech _d top 10 list and the parameter number is distinctive.', 'A4: 70 percent', 'A5: 70', 'A6: About 70%', 'A7: 70%']
Text:In this section, we provide details of the data setup, the usage of the Latent Dirichlet Allocation algorithm, the feature correlation analysis, and the feature selection process. In what follows we describe the way in which we found the features that are most influential for Funding Success. The CFS algorithm evaluates subsets of features based on the individual predictive ability of each feature along with the degree of redundancy between them.  The semantic features buzzwords and Linguistic Inquiry and Word Count are among the features that have a higher correlation in all datasets. An important, unique conclusion of our study is that the features correlated to FS are dependent on the project category. Among the top 10 features, about 70% of the features are the same across datasets. For example, the features feelings_num and buzz_num appear in the three top 10 lists. In contrast, the parameter WC is unique in the R_Tech _D top 10 list and the parameter number is distinctive.



(80)TAKEN?False RQUGE:1.505
Q1:How do we combine semantic and meta-data features to predict Funding Success (FS)?
Q2:What packages were used to conduct the models' design, derivation and operational tests?
Best ans: The dataset used in this paper is a dataset of 50,000 kickstarter and 50,00 indiegogo projects. we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count (LIWC) outputs. we removed Latent Dirichlet Allocation (LDA) topics that overlapped with liwc topics by comparing topic sets. we built three datasets: all_d, tech_d and market_d. the original features space was reduced into three Principal Components (PCs). to identify the most Significant Set of Features (MSSF) (mssf), we use the cfs (correlation-based feature selection) algorithm. the models’ design, derivation and operational tests were conducted using the python packages: sci
['A0: In this paper we combine semantic and meta-data features to predict Funding Success (FS) (fs) using a dataset of 50,000 kickstarter and 50,00 indiegogo projects.', 'A1: The dataset used in this paper is a dataset of 50,000 kickstarter and 50,00 indiegogo projects. we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count (LIWC) outputs. we removed Latent Dirichlet Allocation (LDA) topics that overlapped with liwc topics by comparing topic sets. we built three datasets: all_d, tech_d and market_d. the original features space was reduced into three Principal Components (PCs). to identify the most Significant Set of Features (MSSF) (mssf), we use the cfs (correlation-based feature selection) algorithm. the models’ design, derivation and operational tests were conducted using the python packages: sci', 'A2: For each record, we obtained the values of five metadata features and about 120 semantic features. we removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing the topic sets. we built three datasets: all_d, tech_d and market_d. the original features space was reduced into three Principal Components (PCs). to identify the most Significant Set of Features (MSSF) (mssf), we use the cfs (correlation-based feature selection) algorithm. the models’ design, derivation and operational tests were conducted using the python packages: scikit-learn, scikits-feature and lightgbm. we employed a 10-fold cross-validation test to evaluate the prediction performance. to this', 'A3: For each record, we obtained the values of five metadata features and about 120 semantic features. we removed Latent Dirichlet Allocation (LDA) topics that overlapped with Linguistic Inquiry and Word Count (LIWC) topics by comparing the topic sets. we built three datasets: all_d, tech_d and market_d. the original features space was reduced into three Principal Components (PCs). to identify the most Significant Set of Features (MSSF) (mssf), we use the cfs (correlation-based feature selection) algorithm. the models’ design, derivation and operational tests were conducted using the python packages: scikit-learn, scikits-feature and lightgbm. we employed a 10-fold cross-validation test to evaluate the prediction performance.', 'A4: Dataset: kickstarter and indiegogo datasets. model: combining semantic and meta-data features to predict FS', 'A5: In this paper, we combine semantic and meta-data features to predict Funding Success (FS) (fs) by combining the following:', 'A6: The combined-model is a model that combines semantic and meta-data features to predict Funding Success (FS) (fs)', 'A7: In this paper, we combine semantic and meta-data features to predict Funding Success (FS) (fs)']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(81)TAKEN?False RQUGE:1.4513
Q1:A combined model for Funding Success (FS) in kickstarter and indiegogo projects.
Q2:What is the proposed model evaluated on?
Best ans: This paper presents a combined model for Funding Success (FS) (fs) in kickstarter projects and indiegogo project data. the model is based on the combination of semantic and meta-data feature. the proposed model is evaluated on a 10-fold cross-validation test.
['A0: This paper presents a combined model for Funding Success (FS) (fs) in kickstarter projects and indiegogo project data.', 'A1: This paper presents a combined model for Funding Success (FS) (fs) in kickstarter projects and indiegogo project data. the model is based on the combination of semantic and meta-data feature.', 'A2: This paper presents a combined model for Funding Success (FS) (fs) in kickstarter projects and indiegogo project data. the model is based on the combination of semantic and meta-data feature. the proposed model is evaluated on a 10-fold cross-validation test.', 'A3: In this paper, we combine the following features: buzzwords, sentimental words, feeling words, explanation words, and Latent Dirichlet Allocation (LDA) topics. the model is based on a dataset of 50,000 kickstarter and 50,00 indiegogo projects.', 'A4: Modeling Funding Success (FS) in kickstarter & indiegogo projects', 'A5: The paper presents a combined model for Funding Success (FS) of kickstarter and indiegogo projects.', 'A6: We developed a combined model for Funding Success (FS) (fs) in kickstarter and indiegogo projects.', 'A7: A combined model for Funding Success (FS) (fs) in kickstarter and indiegogo projects.']
Text:We used a dataset of 50,000 Kickstarter and 50,00 Indiegogo projects. We obtained these data from the Kaggle website for 2018. We used Beautifulsup (a Python web scraping package) to gain additional metadata features, such as, description content, number of updates, etc. For each record in the dataset, we obtained the values of five metadata features and about 120 semantic features, including buzzwords, Linguistic Inquiry and Word Count outputs, feelings words, explanation words, and Latent Dirichlet Allocation outputs. We removed LDA topics that overlapped with LIWC topics by comparing the topic sets.  To study the relationship between features and the project category, we built three datasets: All_D, Tech_D and Market_D. The division aims to address the following analysis goals: First, it facilitates improvement in the LDA topic coherence score. Second, it allows us to determine whether a category affects the features that are highly correlated with Funding Success. Third, to study the correlation between buzzwords and FS, it is preferable for datasets to be separated into specific subdomains.  For each dataset, the original features space was reduced into three Principal Components The sum of the explained variance ratio of these three PCs is 78% of the total variance. To identify the most Significant Set of Features (MSSF) that has the highest impact on FS, we use the CFS (correlation-based feature selection) algorithm. The Principal Component Analysis plots in Fig 1 present the distribution of the All_D, Market_D and Tech_D datasets. Dots in blue represent unsuccessfully funded projects and red represent successfully funded projects.  The models’ design, derivation, and operational tests were conducted using the following Python packages: scikit-learn, scikits-feature and LightGBM. We employed a 10-fold cross-validation test to evaluate the prediction performance. Fig 2 includes a flowchart of the methodology. To validate the results of our study, we compared our model’s accuracy to previous research models by applying their methods on our dataset. To this end, we have developed the combined-model, whose novelty lies in the combination of semantic and meta-data features.



(82)TAKEN?False RQUGE:1.4039
Q1:Does Linguistic Inquiry and Word Count (LIWC) analysis measure dictionary words in a specific word?
Q2:What does LIWC analysis measure?
Best ans: Linguistic Inquiry and Word Count (LIWC) (liwc) analysis measures the appearance of dictionary words in a specific text. liwc analysis is used to analyze the textual description part of projects. liwd analysis measures dictionary words. lidc analysis measures a numeric value in the range [0.1]. liwc is a tool used to extract features from the text.
['A0: No. Linguistic Inquiry and Word Count (LIWC) (liwc) analysis measures the appearance of dictionary words in a specific text. liwc analysis is used to analyze the textual description part of projects. liwd is used for textual descriptions. lidc is used as a textual analysis tool for crowdfunding.', 'A1: Linguistic Inquiry and Word Count (LIWC) (liwc) analysis measures the appearance of dictionary words in a specific text. liwc analysis is used to analyze the textual description part of projects. liwd analysis measures dictionary words.', 'A2: Linguistic Inquiry and Word Count (LIWC) (liwc) analysis measures the appearance of dictionary words in a specific text. liwc analysis is used to analyze the textual description part of projects. liwd analysis measures dictionary words. lidc analysis measures a numeric value in the range [0.1].', 'A3: Linguistic Inquiry and Word Count (LIWC) (liwc) analysis measures the appearance of dictionary words in a specific text. liwc analysis is used to analyze the textual description part of projects. liwd analysis measures dictionary words. lidc analysis measures a numeric value in the range [0.1]. liwc is a tool used to extract features from the text.', 'A4: No', 'A5: Yes', 'A6: ', 'A7: ']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(83)TAKEN?False RQUGE:1.3945
Q1:How many percent of the data was used for the experiments?
Q2:What is the final answer for the semantic-model?
Best ans: The experiments used 91.2% of the data. the semantic-model was developed on a dataset of. the data was used as input for the experiments. the final answer: 91.
['A0: The experiments used 91.2% of the data. the semantic-model was developed on a dataset of.', 'A1: The experiments used 91.2% of the data. the semantic-model was developed on a dataset of. the data was used as input for the experiments. the final answer: 91.', 'A2: The experiments used 91.2% of the data.', 'A3: 91.2% of the data was used for experiments. the semantic-model was developed on the data set all_d.', 'A4: 90', 'A5: 90%', 'A6: 95', 'A7: 100']
Text:For the development of the Semantic-Model, semantic features were used as input. We utilized several machine learning algorithms (including SVM, J48, Random Forest, LightGBM, SDG, DNN, and more) on All_D. Table 2 presents the accuracies metrics of the combined-model: Combined model.  The semantic-model we have developed is comparable in accuracy with the state of the art (metadata) models, exhibiting an accuracy level of 91.2%. The LightGBM algorithm exhibited a high accuracy level for all three datasets. In particular, the accuracy of the prediction model is greater than 94% for all datasets.



(84)TAKEN?False RQUGE:1.3393
Q1:Who created this article?
Q2:Who performed the analysis?
Best ans: A team of researchers.
['A0: A team of researchers.', 'A1: Data was collected from projects’ posts.', 'A2: The article was written by a team of researchers.', 'A3: Projects’ posts were extracted from projects’ posts via python web scraping.', 'A4: Projects', 'A5: Team', 'A6: Data science', 'A7: The authors']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(85)TAKEN?False RQUGE:1.309
Q1:How many topics did the Latent Dirichlet Allocation (LDA) model have?
Q2:How many topics were in the datasets?
Best ans: The all_d dataset had 30 topics. the tech_d had 15 topics. and the market_d was 15 topics for the all_d dataset. the total number of topics was 30. the final answer: 30.
['A0: The all_d dataset had 30 topics. the tech_d had 15 topics. and the market_d was 15 topics for the all_d dataset. the total number of topics was 30.', 'A1: The all_d dataset had 30 topics. the tech_d had 15 topics. and the market_d was 15 topics for the all_d dataset. the total number of topics was 30. the final answer: 30.', 'A2: Using gensim python package Latent Dirichlet Allocation (LDA), we used 30 topics for the all_d dataset, 15 for the tech_d and 15 for market_d. the final answer: 15.', 'A3: There were 30 topics that derived. the highest. coherence values for the all_d dataset, 15 for the tech_d and 15 for market_d. the final answer: 30. the', 'A4: Thirty', 'A5: Fifteen', 'A6: 30 topics', 'A7: 15']
Text:Latent Dirichlet Allocation (LDA) is a widely used topic modeling method. LDA algorithm considers each document as a collection of topics, where each word in the document belongs to one or some of these topics. Previous studies on crowdfunding used LDA to perform topic analysis on the text of project updates. We used the Gensim Python package LDA implementation (from GitHub.com) to execute LDA. The latter is known to run faster than other implementations and generates better topic segregation [16].  The three main inputs to the LDA topic model are the dictionary, the corpus, and the number of topics. Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is [16] To avoid overfitting, we chose the one that derived the highest coherence value and the smallest number of. topics. There were 30 topics that derived. the highest. coherence values for the All_D dataset, 15 for the Tech_D and 15 for Market_D datasets.



(86)TAKEN?False RQUGE:1.2997
Q1:What are the most common features of projects?
Q2:What was the most common feature of projects?
Best ans: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects. the final answer: the number.
['A0: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects. the final answer: the number. number of. photos.', 'A1: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by.', 'A2: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects. the final answer: the number.', 'A3: The number of photos, the number of videos, the. number. of updates, the the. amount of previously created projects by. were the most common features of projects.', 'A4: Uploaded', 'A5: Posts', 'A6: Video', 'A7: Videos']
Text:We incorporated metadata features known to affect Funding Success. The metadata features we used were extracted from projects’ posts via Python web scraping. The set we used for our analysis included the number of photos, the. number of videos, the number. of updates, the. number of previously created projects by.



(87)TAKEN?False RQUGE:1.2001
Q1:Who used metric's to measure the performance of their models.
Q2:What country does the metric F-score come from?
Best ans: Us
['A0: Models are used by people to measure performance. we used metrics to measure the performance.', 'A1: Our used metrics to measure the performance of our models.', 'A2: A model’s performance is measured by f-score (also denoted f1). we used metrics.', "A3: Using metric's is a way to measure the performance. we used metrics to measure performance of our models. the final answer: we.", 'A4: The', 'A5: Us', 'A6: Company', 'A7: Our models']
Text:We used the following metrics to measure the performance of our models. Recall is the ratio of correctly predicted positive records to the all records in actual class. F-score (also denoted F1) is the weighted harmonic mean of a model’s precision and recall.



(88)TAKEN?False RQUGE:1.151
Q1:Which of these is not a feeling?
Q2:Which of these is not a feeling?
Best ans: Absurdity is a feeling. absurdies are not feelings.
['A0: Absurdity is a feeling. absurdies are not feelings.', 'A1: Awed means that you are amazed. awed is a feeling. the final answer: awked.', 'A2: Awed is a feeling. awe is derived from the greek word aesop.', 'A3: Alarmed is a feeling. awed means that you are amazed at something or someone. the final answer: awed.', 'A4: Alarmed', 'A5: Awed', 'A6: Awkward', 'A7: Loving']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(89)TAKEN?False RQUGE:1.1407
Q1:How do you feel about the person who is describing something?
Q2:What are two examples of feeling words?
Best ans: You feel awed about the person who is explaining something. you feel affectionate about someone who is describing something.
['A0: Absurdity is a feeling that comes from not understanding something. awed means that you feel like you are awed.', 'A1: Describe something means to explain something. awed is a feeling that comes from being awed or astonished at something.', 'A2: Awed is the feeling you get when you are awed. awe is the emotion you get if you are amazed.', 'A3: You feel awed about the person who is explaining something. you feel affectionate about someone who is describing something.', 'A4: Loving', 'A5: Loved', 'A6: Awkward', 'A7: Annoyed']
Text:The following additional topics were deduced: Explanation words: example, explain, for instance, i.e., mean, in other words, in that, that is, etc. Feelings words: angry, annoyed, afraid, awkward, affectionate, anxious, alarmed, awed, aggravated, amazed, amazed.



(90)TAKEN?False RQUGE:1.0816
Q1:What is the metric used to measure the accuracy of the models?
Q2:What is one feature that we use to improve our models?
Best ans: Sentence completion
['A0: This section presents the metric used to measure accuracy. for semantic analysis, the projects’ textual data is required.', 'A1: For semantic analysis, the projects’ textual data is required.', 'A2: We present the metric that is used to measure the accuracy of the models. for semantic analysis, the projects’ textual data is required.', 'A3: For semantic analysis, the projects’ textual data is required. the metric used to measure the accuracy of the models is:', 'A4: Metric', 'A5: Accuracy', 'A6: Sentence completion', 'A7: Semantic analysis']
Text:In this section, we discuss the features used in our models, highlighting where we differ from previous studies. We also present the metric used to measure the accuracy of the models. For semantic analysis, the projects’ textual data is required.



(91)TAKEN?False RQUGE:1.0019
Q1:How many dictionary words were used in the text?
Q2:What is the lowest value of LIWC?
Best ans: 0.1
['A0: Word count analysis measures the appearance of dictionary words in the text. Linguistic Inquiry and Word Count (LIWC) analysis measures how closely the text is related to the dictionary. liwd analysis measures dictionary words.', 'A1: A Linguistic Inquiry and Word Count (LIWC) analyzer measures the appearance of dictionary words in a specific text. liwc analysis measures the degree to which the text being analyzed is related to the theme of the dictionary. liwd = 0.1 = 0.0.', 'A2: The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. as a result, we arrive at much higher model accuracy. the number of dictionary words used in the text is much larger then previously published.', 'A3: We used the Linguistic Inquiry and Word Count (LIWC) software tool to extract features from the text. liwc analysis measures the appearance of dictionary words in a specific text. the output of liwc is vd = sd/nt, which is the value of the feature that corresponds to d.', 'A4: 1]', 'A5: Zero', 'A6: 0.1', 'A7: 1']
Text:We used the Linguistic Inquiry and Word Count software tool to extract features from the text. LIWC analysis measures the appearance of dictionary words in a specific text. The measure is a numeric value in the range [0..1]. It reflects the degree to which the text being analyzed is related to the theme of the dictionary. The output of LIWC is VD = SD/NT, which is the value of the feature that corresponds to D. Previous studies on crowdfunding used LIWC to analyze the textual description part of projects.  The number and diversity of features in our study are much larger than previously published, and our dataset is significantly greater as well. As a result, we arrive at much higher model accuracy.



(92)TAKEN?False RQUGE:0.9961
Q1:Does the buzz word dataset contain words from different categories?
Q2:did funding success and buzzwords have a relationship
Best ans: This study uses a buzzword dataset. the buzzword dataset contains words from different categories. the final answer: yes.
['A0: This study uses a buzzword dataset. the buzzword dataset contains words from different categories. the final answer: yes. :', 'A1: This study uses a buzzword dataset. the buzzword dataset contains words from different categories. the final answer: yes.', 'A2: There are different categories of words in the buzzword dataset. the buzzwords are from different categories. the final answer: yes.', 'A3: ', 'A4: Yeah', 'A5: No', 'A6: Yes', 'A7: ']
Text:The relationship between Funding Success and buzzwords used in the description of the project was not examined. The buzzword dataset used in this study contains words from different categories: general conversation, education, business, sales and marketing, science and technology, politics, and current affairs.



