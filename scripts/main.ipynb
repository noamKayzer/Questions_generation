{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    import spacy\n",
    "import json\n",
    "from datetime import datetime\n",
    "name = 'finance_course1'\n",
    "JSON_path = '/home/ubuntu/Questions_generation/Financial Markets Course 2.json'\n",
    "sections_num_max= 'all'\n",
    "%pdb on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions\n",
    "fig = mcq.plot_similarity_matrix((questions_df[questions_used]).to_list())\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=2000,\n",
    "    height=2000)\n",
    "fig.show()\n",
    "#both\n",
    "fig = mcq.plot_similarity_matrix((questions_df[questions_used]+' '+questions_df['selected_ans']).to_list())\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=2000,\n",
    "    height=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit questions to be from the all first sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation \t Definition\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 376])\n",
      "Qs:Who were some of these institutions that collapsed?\n",
      "Qs:The speaker is going to talk about what in the context?\n",
      "Qs:What happened after the stock market collapsed\n",
      "Qs:How did this lecture begin?\n",
      "Qs:When the stock market collapsed around the world, what happened?\n",
      "Qs:Why was there a pre-break around 2000?\n",
      "Qs:What is the speaker going to talk about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 244])\n",
      "Qs:Who built models that are built on theory fluid dynamics?\n",
      "Qs:When was probability coined?\n",
      "Qs:Why can't we predict hurricanes?\n",
      "Qs:How do we build mathematical models of the outcomes of financial events?\n",
      "Qs:What is the word probability in its present meaning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:26, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:Why can't you lose more than you put into an investment in finance?\n",
      "Qs:Who has a limited liability economy?\n",
      "Qs:Which of these is not a positive number?\n",
      "Qs:How do you calculate the expected value for a random variable?\n",
      "Qs:In finance, what is the most basic concept that returns can be positive or negative?\n",
      "Qs:When you invest in something, what is the increase in the price?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:51, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 467])\n",
      "Qs:Who recommends using geometric mean in evaluating investments, and why\n",
      "Qs:Who recommends using geometric mean in evaluating investments\n",
      "Qs:Does the speaker recommend using gross return in evaluating investments?\n",
      "Qs:Which of these is not a measure of covariance?\n",
      "Qs:In what way does the geometric mean make sense?\n",
      "Qs:How does variance differ from variance and covariance?\n",
      "Qs:What is the square root of the variance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:07, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:When was value at risk calculated?\n",
      "Qs:Why did many people get in trouble dealing with this crises?\n",
      "Qs:Which of these is a core concept in finance?\n",
      "Qs:What is the new idea coming up after this recent crisis?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:20, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 62])\n",
      "Qs:Where did professor brunnermeier work?\n",
      "Qs:In which field does professor brunnermeier teach?\n",
      "Qs:The author of this passage is a professor at what university?\n",
      "Qs:Who emphasized the need for change in analysis of variance?\n",
      "Qs:Why do we need to change analysis of variance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:28, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 488])\n",
      "Qs:In what year was the stock market down by almost half?\n",
      "Qs:Which stocks did joe mcnay invest?\n",
      "Qs:Did apple computer go up 25 times or down?\n",
      "Qs:How many times did apple computer go up in value between 2000 and 2002?\n",
      "Qs:What happened to the stock market between 2000 and 2002?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:37, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 484])\n",
      "Qs:Where does aaron carroll work?\n",
      "Qs:When was walmart going to be such success?\n",
      "Qs:What is the best success of apple?\n",
      "Qs:Who started liquidating in 2000?\n",
      "Qs:How many great men and women of history got squashed?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:49, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 197])\n",
      "Qs:Where does this story take place?\n",
      "Qs:When was next computer founded?\n",
      "Qs:What is the name of the company that jobs founded?\n",
      "Qs:In which month did apple start to really tank?\n",
      "Qs:Who founded apple?\n",
      "Qs:Which company was founded by steve jobs?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:03, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 469])\n",
      "Qs:In which month and year did stock market go up 12.53%?\n",
      "Qs:On what date was stock market up 12.53% on october 30, 1929?\n",
      "Qs:When did stock market go up 12.53%?\n",
      "Qs:What is the bell-shaped curve thought to be?\n",
      "Qs:Which stock market went up 12.53% on october 30, 1929?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:15, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 196])\n",
      "Qs:Where does this story take place?\n",
      "Qs:Who was a student of bob greene?\n",
      "Qs:In what year did the stock market collapse?\n",
      "Qs:How many times has bob greene been teaching this course?\n",
      "Qs:What is the probability of a decline that's that negative?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:31, 13.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1:../outputs/03_01_23_finance_course1/GQ.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2:../outputs/03_01_23_finance_course1/QG+QA.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/60 [00:18<00:54,  1.19s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3:../outputs/03_01_23_finance_course1/GQ+QA+GQ.pickle has been saved\n",
      "Stage 4:../outputs/03_01_23_finance_course1/questions_full.txt has been saved\n",
      "Stage 5:../outputs/03_01_23_finance_course1/questions.txt has been saved\n"
     ]
    }
   ],
   "source": [
    "from MCQ import flanT5MCQ\n",
    "from datetime import datetime\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "dir_path = '../outputs/'+datetime.now().strftime(\"%d_%m_%y\")+'_'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "if sections_num_max!='all':\n",
    "  sections = sections[:sections_num_max]\n",
    "print(f'Limit questions to be from the {sections_num_max} first sections')\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "if 'mcq' in locals():\n",
    "  del mcq\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
    "'''\n",
    "cpu_model = mcq.model.to('cpu')\n",
    "del mcq.model\n",
    "mcq.model = cpu_model\n",
    "mcq.push_model_to_GPU(mcq.model)\n",
    "'''\n",
    "with torch.no_grad():\n",
    "  questions_df = mcq.generate_questions(sections=sections,org_sections=org_text,sections_ranks=np.ones(len(sections)))\n",
    "questions_df.to_pickle(dir_path+'GQ.pickle')\n",
    "print(f\"Stage 1:{dir_path+'GQ.pickle'} has been saved\")\n",
    "torch.cuda.empty_cache()\n",
    "if mcq.COMPUTE_ANSWERS:\n",
    "  mcq.push_model_to_GPU(mcq.QA.model)\n",
    "  questions_df = mcq.QA.select_best_answer(questions_df)\n",
    "  questions_df.to_pickle(dir_path+'QG+QA.pickle')\n",
    "  print(f\"Stage 2:{dir_path+'QG+QA.pickle'} has been saved\")\n",
    "  questions_df = mcq.QG.create_question_MixQG(questions_df)\n",
    "  questions_df.to_pickle(dir_path+'GQ+QA+GQ.pickle')\n",
    "  print(f\"Stage 3:{dir_path+'GQ+QA+GQ.pickle'} has been saved\")\n",
    "  questions_used = 'new_question'\n",
    "  questions_df['RQUGE'] = questions_df.apply(lambda x: mcq.rquge.scorer(x.text, x[questions_used], x.selected_ans)[0]['pred_score'] ,axis='columns')\n",
    "  questions_df = questions_df.sort_values('RQUGE',ascending=False).reset_index()\n",
    "  q_sim_mat,q_ans_sim_mat = mcq.find_similarity(questions_df[questions_used].to_list(),\n",
    "                                            answers = (questions_df[questions_used]+' '+questions_df['selected_ans']).to_list())\n",
    "  filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                  q_sim_mat = q_sim_mat, ans_sim_mat = q_ans_sim_mat,\n",
    "                                  n_thrs=20, return_index=True)\n",
    "  questions_df['use_question']=False\n",
    "  questions_df.loc[filter_idx,'use_question']=True\n",
    "  #questions_df = questions_df.iloc[filter_idx,:]\n",
    "with open(f'{dir_path}questions_full.txt', 'w') as f:\n",
    "    qs_text =[mcq.show_qs(questions_df,i) for i in questions_df.index.values]\n",
    "    text_outuput=''\n",
    "    for i,qs in zip(questions_df.index.values,qs_text):\n",
    "        text_outuput += f\"({i})TAKEN?{questions_df['use_question'][i]} RQUGE:{round(questions_df['RQUGE'][i],4)}\"+'\\n'+ qs+\"\\n\\n\"\n",
    "    f.write(text_outuput)\n",
    "    print(f\"Stage 4:{dir_path}questions_full.txt has been saved\")\n",
    "    \n",
    "with open(f'{dir_path}questions.txt', 'w') as f:\n",
    "    text_outuput=''\n",
    "    for i in np.unique(questions_df.section_n_chunk):\n",
    "      qs_in_sections = questions_df.query(f'section_n_chunk == {i} and use_question == True')\n",
    "      for q,a in zip(qs_in_sections[questions_used],qs_in_sections.selected_ans):\n",
    "        text_outuput+=f'Q:{q}\\nA:{a}\\n'\n",
    "      text_outuput+= '-'*50 + '\\n'\n",
    "    f.write(text_outuput)\n",
    "    print(f\"Stage 5:{dir_path}questions.txt has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/miniforge3/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniforge3/lib/python3.9/site-packages (0.1.97)\n",
      "Requirement already satisfied: sentence_transformers in /home/ubuntu/miniforge3/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (0.1.97)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (1.23.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (4.25.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (1.13.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (0.14.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from sentence_transformers) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (63.2.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.37.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
      "Requirement already satisfied: click in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from nltk->sentence_transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from torchvision->sentence_transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.11)\n",
      "Collecting scispacy\n",
      "  Using cached scispacy-0.5.1-py3-none-any.whl (44 kB)\n",
      "Collecting conllu\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scispacy) (2.28.1)\n",
      "Collecting nmslib>=1.7.3.6\n",
      "  Downloading nmslib-2.1.1-cp39-cp39-manylinux2010_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pysbd\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scispacy) (1.23.5)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scispacy) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scispacy) (1.2.0)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scispacy) (3.4.3)\n",
      "Collecting pybind11<2.6.2\n",
      "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.26.11)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scikit-learn>=0.20.3->scispacy) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (21.3)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (63.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (8.1.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->scispacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->scispacy) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->scispacy) (4.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->scispacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->scispacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->scispacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->scispacy) (2.1.1)\n",
      "Installing collected packages: pysbd, pybind11, conllu, nmslib, scispacy\n",
      "Successfully installed conllu-4.5.2 nmslib-2.1.1 pybind11-2.6.1 pysbd-0.3.4 scispacy-0.5.1\n",
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.23.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (63.2.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (8.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniforge3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.1)\n",
      "Building wheels for collected packages: en-core-sci-sm\n",
      "  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870856 sha256=e340369d04126ffde1ac6e74861fd5179807f56201403f3db49e00f6d1dcb424\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/f5/2e/39/9c9d425a1d34c06409420f7c65c5e10a56f7b149a3c37cdfa6\n",
      "Successfully built en-core-sci-sm\n",
      "Installing collected packages: en-core-sci-sm\n",
      "Successfully installed en-core-sci-sm-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install -U sentencepiece \n",
    "!pip install sentence_transformers\n",
    "!pip install scispacy\n",
    "!pip install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (768 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation \t Definition\n",
      "{'SGD': 'Stochastic Gradient Descent', 'ReLU': 'Rectified Linear Unit', 'RGB': 'Red, Green, Blue', 'CIFAR': 'Canadian Institute for Advanced Research', 'GPUs': 'Graphics Processing Units', 'RNN': 'Recurrent Neural Network', 'CNN': 'Convolution Neural Network', 'RNNs': 'Recurrent Neural Networks', 'LSTM': 'Long Short-term Memory'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 256])\n",
      "Qs:Increasingly, these applications make use what?\n",
      "Qs:Which of these is not a type of machine learning?\n",
      "Qs:Machine-learning systems are used to identify objects in images, what?\n",
      "Qs:How does deep learning differ from conventional machine learning techniques?\n",
      "Qs:What is the key aspect of deep learning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:16, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 462])\n",
      "Qs:To properly adjust the weight vector, what is done to the weights in the learning algorithm\n",
      "Qs:Which of these methods has dramatically improved the state-of-the-art in speech recognition?\n",
      "Qs:In what domains can deep learning be used?\n",
      "Qs:When a weight is increased by a tiny amount, what would the error increase or decrease?\n",
      "Qs:What is the name of the method that most practitioners use to adjust the weights?\n",
      "Qs:How do you adjust the weights in a typical deep learning system?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:39, 20.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 311])\n",
      "Qs:Why do we need to use a non-linear function?\n",
      "Qs:At each hidden layer, we compute what?\n",
      "Qs:A neural network can be used to learn to recognize what kind of objects\n",
      "Qs:How are reluc functions used in neural networks?\n",
      "Qs:In which layer do we compute the error derivative?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:52, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 474])\n",
      "Qs:Which of these is not a serious issue in general in general?\n",
      "Qs:In what year was the deep net architecture revived?\n",
      "Qs:The idea of deep learning was discovered by several different groups during which decade?\n",
      "Qs:When was interest in deep feedforward networks renewed?\n",
      "Qs:How are hidden layers different from hidden layers in deep learning?\n",
      "Qs:What is the main idea of this passage?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:09, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 352])\n",
      "Qs:Who designed convnets?\n",
      "Qs:Where is a filter bank located?\n",
      "Qs:The architecture of a typical convnet is structured as what?\n",
      "Qs:How many layers are in a typical convnet?\n",
      "Qs:What are the two types of layers in a typical convnet?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:21, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 228])\n",
      "Qs:The following are examples for the use of convnets:\n",
      "Qs:In what technology are convnets being used?\n",
      "Qs:What are some applications of convnets?\n",
      "Qs:How many layers of relus are in a convnet architecture?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:33, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 365])\n",
      "Qs:In what way do neural networks have two different exponential advantages? choose from:\n",
      "Qs:When a neural network is used to model language, what are some of the features that are learned?\n",
      "Qs:Using distributed representations, what are two advantages of deep nets over classic learning algorithms?\n",
      "Qs:A neural network is used to model what?\n",
      "Qs:Which of these features is used to predict the target outputs of a multilayer neural network?\n",
      "Qs:In what way do neural networks have two different exponential advantages?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:51, 15.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 472])\n",
      "Qs:Who first introduced backpropagation?\n",
      "Qs:Long Short-term Memory (LSTM) network and memory network are examples for what type of network?\n",
      "Qs:In which task is the encoder better than the decoder for machine translation?\n",
      "Qs:How can Long Short-term Memory (LSTM) networks be used to train Recurrent Neural Networks (RNNs)?\n",
      "Qs:Which of these networks is used for machine translation?\n",
      "Qs:What is the advantage of using Long Short-term Memory (LSTM) networks?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:12, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 213])\n",
      "Qs:How are Recurrent Neural Networks (RNNs) trained? no answer>\n",
      "Qs:Which of these is not a type of neural network?\n",
      "Qs:Why do we discover the structure of the world by watching it?\n",
      "Qs:How are Recurrent Neural Networks (RNNs) trained?\n",
      "Qs:What is the main idea of the passage?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:25, 16.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1:../outputs/16_01_23_deep_learning2/GQ.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2:../outputs/16_01_23_deep_learning2/QG+QA.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10/48 [00:12<00:54,  1.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 48/48 [01:10<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3:../outputs/16_01_23_deep_learning2/GQ+QA+GQ.pickle has been saved\n",
      "Stage 4:../outputs/16_01_23_deep_learning2/deep_learning2_questions_full.txt has been saved\n",
      "Stage 5:../outputs/16_01_23_deep_learning2/deep_learning2_questions.txt has been saved\n",
      "--------------------------------------------------\n",
      "distractors not found! What do machine-learning systems increasingly use?\n",
      "--------------------------------------------------\n",
      "not \n",
      "Which of these is not a type of machine learning? Which of these is  a type of machine learning?\n",
      "Which of these is not a type of machine learning? --- Human engineering\n",
      " ['Pattern recognition', 'Deep neural networks', 'Match news items, posts or products with users’ interests.']\n",
      "--------------------------------------------------\n",
      "distractors not found! What are some of the applications of machine learning?\n",
      "--------------------------------------------------\n",
      "distractors not found! What procedure do most practitioners use?\n",
      "--------------------------------------------------\n",
      "distractors not found! What does deep learning allow?\n",
      "--------------------------------------------------\n",
      "distractors not found! What does the gradient vector do?\n",
      "--------------------------------------------------\n",
      "distractors not found! What has deep learning dramatically improved?\n",
      "--------------------------------------------------\n",
      "distractors not found! What does the SGD process do?\n",
      "--------------------------------------------------\n",
      "+++++++++++Each hidden layer\n",
      "NP\n",
      "Where do we compute the error derivative with respect to the output of each unit? --- Each hidden layer\n",
      " ['Several layers', '| multilayer neural networks']\n",
      "--------------------------------------------------\n",
      "distractors not found! How does a deep learning network learn to recognize objects?\n",
      "--------------------------------------------------\n",
      "distractors not found! When was interest in deep feedforward networks revived?\n",
      "--------------------------------------------------\n",
      "not \n",
      "Which of these is not a serious issue in general in general? Which of these is  a serious issue in general in general?\n",
      "moving model to cpu!\n",
      "Which of these is not a serious issue in general in general? --- Local minima are not a serious issue in general in general.\n",
      " ['Hidden layers can be seen as distorting the input']\n",
      "--------------------------------------------------\n",
      "distractors not found! What do many applications of deep learning use?\n",
      "--------------------------------------------------\n",
      "During what decade was the idea of backpropagation discovered? --- 1980s\n",
      " ['2009', '2012']\n",
      "--------------------------------------------------\n",
      "+++++++++++feature maps\n",
      "[0.6516057, 0.54329145, 0.5181426, 0.49618453]\n",
      "NP\n",
      "Where are units in a ConvNet organized? --- In feature maps\n",
      " ['In higher-level features', 'In recognition', 'In classifiers', 'In images']\n",
      "--------------------------------------------------\n",
      "distractors not found! What are the first few stages of a ConvNet composed of?\n",
      "--------------------------------------------------\n",
      "[0.841715395450592, 0.5957365036010742, 0.5876209139823914, 0.5450937747955322]\n",
      "How many layers of relus are in a convnet architecture? --- 10 to 20 layers of relus are in a convnet architecture.\n",
      " ['15 layers of relus are in a convnet architecture.', '3 layers of relus are in a convnet architecture.', '4 layers of relus are in a convnet architecture.', '2 layers of relus are in a convnet architecture.']\n",
      "--------------------------------------------------\n",
      "distractors not found! What other applications are ConvNets being used for?\n",
      "--------------------------------------------------\n",
      "distractors not found! What are two examples of ConvNet-based applications that are gaining in importance?\n",
      "--------------------------------------------------\n",
      "distractors not found! What are the word vectors in a language model composed of?\n",
      "--------------------------------------------------\n",
      "distractors not found! What does deep-learning theory show about deep nets?\n",
      "--------------------------------------------------\n",
      "distractors not found! What are the two advantages of deep nets over classic learning algorithms?\n",
      "--------------------------------------------------\n",
      "distractors not found! What can one do instead of translating a French sentence into an English sentence?\n",
      "--------------------------------------------------\n",
      "distractors not found! What was backpropagation first introduced to?\n",
      "--------------------------------------------------\n",
      "distractors not found! What have long short-term memory networks proven to be more effective than conventional RNNs?\n",
      "--------------------------------------------------\n",
      "distractors not found! What do we expect to see in the future?\n",
      "--------------------------------------------------\n",
      "distractors not found! What type of learning is largely unsupervised?\n",
      "--------------------------------------------------\n",
      "distractors not found! What are the parameters of a RNN?\n",
      "--------------------------------------------------\n",
      "not \n",
      "Which of these is not a type of neural network? Which of these is  a type of neural network?\n",
      "Which of these is not a type of neural network? --- An unfolded network.\n",
      " ['Reinforcement learning', 'Convnets']\n",
      "Stage 6:../outputs/16_01_23_deep_learning2/deep_learning2_questions_with_distractors.txt has been saved\n"
     ]
    }
   ],
   "source": [
    "try:import spacy\n",
    "except: import spacy\n",
    "%pdb on\n",
    "import torch\n",
    "torch.manual_seed(10)\n",
    "#!python -m spacy download en_core_web_lg\n",
    "try:\n",
    "  from compute_question_and_answer import compute_question_and_answer\n",
    "except:\n",
    "  from compute_question_and_answer import compute_question_and_answer\n",
    "import json\n",
    "name = 'deep_learning2'\n",
    "sections_num_max = 'all'\n",
    "JSON_path = '/home/ubuntu/Questions_generation/Deep learning.json'\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "if sections_num_max!='all':\n",
    "  sections = sections[:sections_num_max]\n",
    "  print(f'Limit questions to be from the {sections_num_max} first sections')\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "out = compute_question_and_answer(sections,org_text,save_name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "s2v.from_disk(\"/path/to/s2v_reddit_2015_md\")\n",
    "\n",
    "doc = nlp(\"A sentence about natural language processing.\")\n",
    "assert doc[3:6].text == \"natural language processing\"\n",
    "freq = doc[3:6]._.s2v_freq\n",
    "vector = doc[3:6]._.s2v_vec\n",
    "most_similar = doc[3:6]._.s2v_most_similar(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader(kind, name, load):\n",
    "  name = kind + \":\" + name\n",
    "  if name not in pre_loaded:\n",
    "    pre_loaded[name] = load()\n",
    "  return pre_loaded[name]\n",
    "\n",
    "pre_loaded = {}\n",
    "s2v = model_loader(\n",
    "    'sense2vec',\n",
    "    \"s2v\",\n",
    "    lambda : Sense2Vec().from_disk('./support/models/sense2vec/s2v_reddit_2019_lg') # large \n",
    ")\n",
    "\n",
    "def find_distractors_s2v(term, n=5):\n",
    "    try:\n",
    "        candidates = [\n",
    "                    result[0].split(\"|\")[0].replace(\"_\",\" \")\n",
    "                    for result in s2v.most_similar(term, n=30)\n",
    "                    if result[0].split(\"|\")[1] == term.split(\"|\")[1]\n",
    "                    and result[0] == s2v.get_best_sense(result[0].split(\"|\")[0].replace(\"_\",\" \"))\n",
    "        ]\n",
    "        # print(candidates)\n",
    "        \n",
    "        distractors = filter_distractors(term.split(\"|\")[0].replace(\"_\",\" \"), candidates, n=n ,threshold = 0.7)\n",
    "\n",
    "        if distractors:\n",
    "            return distractors\n",
    "    except ValueError as e:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RQUGE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RQUGE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/Questions_generation/scripts/main.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m section_output[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(q\u001b[39m.\u001b[39mnew_question)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m section_output[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(q[\u001b[39m'\u001b[39m\u001b[39mselected_ans\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m section_output[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(q[\u001b[39m'\u001b[39;49m\u001b[39mRQUGE\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m section_output[\u001b[39m'\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(q[\u001b[39m'\u001b[39m\u001b[39mdetails\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m section_output[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(q[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RQUGE'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/miniforge3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m(3805)\u001b[0;36mget_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   3803 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3804 \u001b[0;31m            \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 3805 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3806 \u001b[0;31m            \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3807 \u001b[0;31m                \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "questions_df = pd.read_pickle('/home/ubuntu/Questions_generation/outputs/16_01_23_deep_learning/GQ+QA+GQ.pickle')\n",
    "import numpy as np\n",
    "\n",
    "output = []\n",
    "for i in range(max(questions_df.section_n)+1):\n",
    "    section_output = {'question':[],'answer':[],'score':[]}\n",
    "    for chunk in np.unique(questions_df.loc[questions_df.section_n==i,'section_n_chunk']):\n",
    "        chunk_questions = questions_df.query(f'section_n_chunk=={chunk}')\n",
    "        for _,q in chunk_questions.iterrows():\n",
    "                section_output['question'].append(q.new_question)\n",
    "                section_output['answer'].append(q['selected_ans'])\n",
    "                section_output['score'].append(q['RQUGE'])\n",
    "                section_output['details'].append(q['details'])\n",
    "                section_output['context'].append(q['text'])\n",
    "    output.append(section_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_n</th>\n",
       "      <th>section_n_chunk</th>\n",
       "      <th>section_rank</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>question_ppl</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>short_answer_1</th>\n",
       "      <th>short_answer_2</th>\n",
       "      <th>short_answer_3</th>\n",
       "      <th>short_answer_4</th>\n",
       "      <th>generated_selected_ans</th>\n",
       "      <th>selected_ans</th>\n",
       "      <th>details</th>\n",
       "      <th>new_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>Increasingly, these applications make use what?</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>Deep neural networks</td>\n",
       "      <td>Class of techniques called deep learning.</td>\n",
       "      <td>Increasingly, these applications make use of a...</td>\n",
       "      <td>A class of techniques called deep learning.</td>\n",
       "      <td>Depth</td>\n",
       "      <td>Deep learning.</td>\n",
       "      <td>Deep learning</td>\n",
       "      <td></td>\n",
       "      <td>class of techniques called deep learning.</td>\n",
       "      <td>Class of techniques called deep learning.</td>\n",
       "      <td></td>\n",
       "      <td>What do machine-learning systems increasingly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>Which of these is not a type of machine learning?</td>\n",
       "      <td>0.008745</td>\n",
       "      <td>Human engineering.</td>\n",
       "      <td>Human engineering</td>\n",
       "      <td>Deep neural networks</td>\n",
       "      <td>Identify objects in images, transcribe speech ...</td>\n",
       "      <td>Human engineering</td>\n",
       "      <td>Humans</td>\n",
       "      <td>Deep neural networks</td>\n",
       "      <td>Domain expertise</td>\n",
       "      <td>human engineering</td>\n",
       "      <td>Human engineering</td>\n",
       "      <td>NEGATIVE: not</td>\n",
       "      <td>Which of these is not a type of machine learning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>Transcribing speech into text requires careful...</td>\n",
       "      <td>The key aspect of deep learning is that these ...</td>\n",
       "      <td>Search</td>\n",
       "      <td>Text</td>\n",
       "      <td>Transscribe speech into text</td>\n",
       "      <td>Translate speech into text</td>\n",
       "      <td>machine-learning systems are used to identify ...</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td></td>\n",
       "      <td>What are some of the applications of machine l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>How does deep learning differ from conventiona...</td>\n",
       "      <td>0.065550</td>\n",
       "      <td>This is a list of machine-learning application...</td>\n",
       "      <td>Machine-learning systems can process natural d...</td>\n",
       "      <td>These layers of features are not designed by h...</td>\n",
       "      <td>The key aspect of deep learning is that these ...</td>\n",
       "      <td>They are learned from data using a general-pur...</td>\n",
       "      <td>Requires very little engineering by hand</td>\n",
       "      <td>Required careful engineering and considerable ...</td>\n",
       "      <td>They are learned from data using a general-pur...</td>\n",
       "      <td>these layers of features are not designed by h...</td>\n",
       "      <td>These layers of features are not designed by h...</td>\n",
       "      <td></td>\n",
       "      <td>What is the key aspect of deep learning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Machine-learning systems are used to identify ...</td>\n",
       "      <td>What is the key aspect of deep learning?</td>\n",
       "      <td>0.077957</td>\n",
       "      <td>The key aspect of deep learning is that these ...</td>\n",
       "      <td>These layers of features are not designed by h...</td>\n",
       "      <td>It has turned out to be very good at discoveri...</td>\n",
       "      <td>Deep learning is making major advances in solv...</td>\n",
       "      <td>No engineering by hand</td>\n",
       "      <td>Requires very little engineering by hand</td>\n",
       "      <td>They are learned from data using a general-pur...</td>\n",
       "      <td>These layers of features are not designed by h...</td>\n",
       "      <td>these layers of features are not designed by h...</td>\n",
       "      <td>These layers of features are not designed by h...</td>\n",
       "      <td></td>\n",
       "      <td>What is the key aspect of deep learning?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   section_n  section_n_chunk  section_rank  \\\n",
       "0          0              0.0           1.0   \n",
       "1          0              0.0           1.0   \n",
       "2          0              0.0           1.0   \n",
       "3          0              0.0           1.0   \n",
       "4          0              0.0           1.0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Machine-learning systems are used to identify ...   \n",
       "1  Machine-learning systems are used to identify ...   \n",
       "2  Machine-learning systems are used to identify ...   \n",
       "3  Machine-learning systems are used to identify ...   \n",
       "4  Machine-learning systems are used to identify ...   \n",
       "\n",
       "                                            question  question_ppl  \\\n",
       "0    Increasingly, these applications make use what?      0.000630   \n",
       "1  Which of these is not a type of machine learning?      0.008745   \n",
       "2  Machine-learning systems are used to identify ...      0.016903   \n",
       "3  How does deep learning differ from conventiona...      0.065550   \n",
       "4           What is the key aspect of deep learning?      0.077957   \n",
       "\n",
       "                                            answer_1  \\\n",
       "0                               Deep neural networks   \n",
       "1                                 Human engineering.   \n",
       "2  Machine-learning systems are used to identify ...   \n",
       "3  This is a list of machine-learning application...   \n",
       "4  The key aspect of deep learning is that these ...   \n",
       "\n",
       "                                            answer_2  \\\n",
       "0          Class of techniques called deep learning.   \n",
       "1                                  Human engineering   \n",
       "2  Machine-learning systems are used to identify ...   \n",
       "3  Machine-learning systems can process natural d...   \n",
       "4  These layers of features are not designed by h...   \n",
       "\n",
       "                                            answer_3  \\\n",
       "0  Increasingly, these applications make use of a...   \n",
       "1                               Deep neural networks   \n",
       "2  Transcribing speech into text requires careful...   \n",
       "3  These layers of features are not designed by h...   \n",
       "4  It has turned out to be very good at discoveri...   \n",
       "\n",
       "                                            answer_4  \\\n",
       "0        A class of techniques called deep learning.   \n",
       "1  Identify objects in images, transcribe speech ...   \n",
       "2  The key aspect of deep learning is that these ...   \n",
       "3  The key aspect of deep learning is that these ...   \n",
       "4  Deep learning is making major advances in solv...   \n",
       "\n",
       "                                      short_answer_1  \\\n",
       "0                                              Depth   \n",
       "1                                  Human engineering   \n",
       "2                                             Search   \n",
       "3  They are learned from data using a general-pur...   \n",
       "4                             No engineering by hand   \n",
       "\n",
       "                             short_answer_2  \\\n",
       "0                            Deep learning.   \n",
       "1                                    Humans   \n",
       "2                                      Text   \n",
       "3  Requires very little engineering by hand   \n",
       "4  Requires very little engineering by hand   \n",
       "\n",
       "                                      short_answer_3  \\\n",
       "0                                      Deep learning   \n",
       "1                               Deep neural networks   \n",
       "2                       Transscribe speech into text   \n",
       "3  Required careful engineering and considerable ...   \n",
       "4  They are learned from data using a general-pur...   \n",
       "\n",
       "                                      short_answer_4  \\\n",
       "0                                                      \n",
       "1                                   Domain expertise   \n",
       "2                         Translate speech into text   \n",
       "3  They are learned from data using a general-pur...   \n",
       "4  These layers of features are not designed by h...   \n",
       "\n",
       "                              generated_selected_ans  \\\n",
       "0          class of techniques called deep learning.   \n",
       "1                                  human engineering   \n",
       "2  machine-learning systems are used to identify ...   \n",
       "3  these layers of features are not designed by h...   \n",
       "4  these layers of features are not designed by h...   \n",
       "\n",
       "                                        selected_ans         details  \\\n",
       "0          Class of techniques called deep learning.                   \n",
       "1                                  Human engineering   NEGATIVE: not   \n",
       "2  Machine-learning systems are used to identify ...                   \n",
       "3  These layers of features are not designed by h...                   \n",
       "4  These layers of features are not designed by h...                   \n",
       "\n",
       "                                        new_question  \n",
       "0  What do machine-learning systems increasingly ...  \n",
       "1  Which of these is not a type of machine learning?  \n",
       "2  What are some of the applications of machine l...  \n",
       "3           What is the key aspect of deep learning?  \n",
       "4           What is the key aspect of deep learning?  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs True\n",
      "Apple False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from negspacy.negation import Negex\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"negex\", config={\"ent_types\":[\"PERSON\",\"ORG\"]})\n",
    "\n",
    "doc = nlp(\"She does not like Steve Jobs but likes Apple products.\")\n",
    "for e in doc.ents:\n",
    "    print(e.text, e._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am not': 'am',\n",
       " 'is not': 'is',\n",
       " 'are not': 'are',\n",
       " 'was not': 'was',\n",
       " 'were not': 'were',\n",
       " \"haven't been\": 'have been',\n",
       " \"hasn't been\": 'has been',\n",
       " \"hadn't been\": 'had been',\n",
       " 'will not be': 'will be',\n",
       " 'would not be': 'would be',\n",
       " 'should not be': 'should be',\n",
       " 'could not be': 'could be',\n",
       " 'must not be': 'must be',\n",
       " 'might not be': 'might be',\n",
       " \"don't\": 'do',\n",
       " 'does not': 'does',\n",
       " 'did not': 'did',\n",
       " 'has not': 'has',\n",
       " 'have not': 'have',\n",
       " 'had not': 'had',\n",
       " 'can not': 'can',\n",
       " 'could not': 'could',\n",
       " 'shall not': 'shall',\n",
       " 'should not': 'should',\n",
       " 'will not': 'will',\n",
       " 'would not': 'would',\n",
       " 'may not': 'may',\n",
       " 'might not': 'might',\n",
       " 'must not': 'must',\n",
       " 'ought not': 'ought',\n",
       " 'need not': 'need',\n",
       " 'dare not': 'dare',\n",
       " 'used not': 'used',\n",
       " \"needn't\": 'need',\n",
       " \"daren't\": 'dare',\n",
       " \"mustn't\": 'must',\n",
       " \"shouldn't\": 'should',\n",
       " \"wouldn't\": 'would',\n",
       " \"couldn't\": 'could',\n",
       " \"wasn't\": 'was',\n",
       " \"weren't\": 'were',\n",
       " \"isn't\": 'is',\n",
       " \"aren't\": 'are',\n",
       " \"didn't\": 'did',\n",
       " \"doesn't\": 'does',\n",
       " \"hasn't\": 'has',\n",
       " \"haven't\": 'have',\n",
       " \"hadn't\": 'had',\n",
       " \"can't\": 'can',\n",
       " \"we're not\": \"we're\",\n",
       " \"they're not\": \"they're\",\n",
       " \"you're not\": \"you're\",\n",
       " \"i'm not\": \"i'm\",\n",
       " \"he's not\": \"he's\",\n",
       " \"she's not\": \"she's\",\n",
       " \"it's not\": \"it's\",\n",
       " 'we are not': \"we're\",\n",
       " 'they are not': \"they're\",\n",
       " 'you are not': \"you're\",\n",
       " 'i am not': \"i'm\",\n",
       " 'he is not': \"he's\",\n",
       " 'she is not': \"she's\",\n",
       " 'it is not': \"it's\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"am not\":\"am\",\"is not\":\"is\",\"are not\":\"are\",\"was not\":\"was\",\"were not\":\"were\",\"haven't been\":\"have been\",\"hasn't been\":\"has been\",\"hadn't been\":\"had been\",\"will not be\":\"will be\",\"would not be\":\"would be\",\"should not be\":\"should be\",\"could not be\":\"could be\",\"must not be\":\"must be\",\"might not be\":\"might be\",\"don't\":\"do\",\"does not\":\"does\",\"did not\":\"did\",\"has not\":\"has\",\"have not\":\"have\",\"had not\":\"had\",\"can not\":\"can\",\"could not\":\"could\",\"shall not\":\"shall\",\"should not\":\"should\",\"will not\":\"will\",\"would not\":\"would\",\"may not\":\"may\",\"might not\":\"might\",\"must not\":\"must\",\"ought not\":\"ought\",\"need not\":\"need\",\"dare not\":\"dare\",\"used not\":\"used\",\"needn't\":\"need\",\"daren't\":\"dare\",\"mustn't\":\"must\",\"shouldn't\":\"should\",\"wouldn't\":\"would\",\"couldn't\":\"could\",\"wasn't\":\"was\",\"weren't\":\"were\",\"isn't\":\"is\",\"aren't\":\"are\",\"didn't\":\"did\",\"doesn't\":\"does\",\"hasn't\":\"has\",\"haven't\":\"have\",\"hadn't\":\"had\",\"can't\":\"can\",\"we're not\":\"we're\",\"they're not\":\"they're\",\"you're not\":\"you're\",\"i'm not\":\"i'm\",\"he's not\":\"he's\",\"she's not\":\"she's\",\"it's not\":\"it's\",\"we're not\":\"we are\",\"they're not\":\"they are\",\"you're not\":\"you are\",\"i'm not\":\"i am\",\"he's not\":\"he is\",\"she's not\":\"she is\",\"it's not\":\"it is\",\"we are not\":\"we're\",\"they are not\":\"they're\",\"you are not\":\"you're\",\"i am not\":\"i'm\"\n",
    "                          ,\"he is not\":\"he's\",\"she is not\":\"she's\",\"it is not\":\"it's\",\"we are not\": \"we're\",\n",
    "    \"they are not\": \"they're\",\n",
    "    \"you are not\": \"you're\",\n",
    "    \"i am not\": \"i'm\",\n",
    "    \"he is not\": \"he's\",\n",
    "    \"she is not\": \"she's\",\n",
    "    \"it is not\": \"it's\",\n",
    "    \"am not\": \"am\",\n",
    "    \"is not\": \"is\",\n",
    "    \"are not\": \"are\",\n",
    "    \"was not\": \"was\",\n",
    "    \"were not\": \"were\",\n",
    "    \"haven't been\": \"have been\",\n",
    "    \"hasn't been\": \"has been\",\n",
    "    \"hadn't been\": \"had been\",\n",
    "    \"will not be\": \"will be\",\n",
    "    \"would not be\": \"would be\",\n",
    "    \"should not be\": \"should be\",\n",
    "    \"could not be\": \"could be\",\n",
    "    \"must not be\": \"must be\",\n",
    "    \"might not be\": \"might be\",\n",
    "    \"don't\": \"do\",\n",
    "    \"does not\": \"does\",\n",
    "    \"did not\": \"did\",\n",
    "    \"has not\": \"has\",\n",
    "    \"have not\": \"have\",\n",
    "    \"had not\": \"had\",\n",
    "    \"can not\": \"can\",\n",
    "    \"could not\": \"could\",\n",
    "    \"shall not\": \"shall\",\n",
    "    \"should not\": \"should\",\n",
    "    \"will not\": \"will\",\n",
    "    \"would not\": \"would\",\n",
    "    \"may not\": \"may\",\n",
    "    \"might not\": \"might\",\n",
    "    \"must not\": \"must\",\n",
    "    \"ought not\": \"ought\",\n",
    "    \"need not\": \"need\",\n",
    "    \"dare not\": \"dare\",\n",
    "    \"used not\": \"used\",\n",
    "    \"needn't\": \"need\",\n",
    "    \"daren't\": \"dare\",\n",
    "    \"mustn't\": \"must\",\n",
    "    \"shouldn't\": \"should\",\n",
    "    \"wouldn't\": \"would\",\n",
    "    \"couldn't\": \"could\",\n",
    "    \"wasn't\": \"was\",\n",
    "    \"weren't\": \"were\",\n",
    "    \"isn't\": \"is\",\n",
    "    \"aren't\": \"are\",\n",
    "    \"didn't\": \"did\",\n",
    "    \"doesn't\": \"does\",\n",
    "    \"hasn't\": \"has\",\n",
    "    \"haven't\": \"have\",\n",
    "    \"hadn't\": \"had\",\n",
    "    \"can't\": \"can\",\n",
    "    \"we're not\": \"we're\",\n",
    "    \"they're not\": \"they're\",\n",
    "    \"you're not\": \"you're\",\n",
    "    \"i'm not\": \"i'm\",\n",
    "    \"he's not\": \"he's\",\n",
    "    \"she's not\": \"she's\",\n",
    "    \"it's not\": \"it's\",\n",
    "    \"am not\": \"am\",\n",
    "    \"is not\": \"is\",\n",
    "    \"are not\": \"are\",\n",
    "    \"was not\": \"was\",\n",
    "    \"were not\": \"were\",\n",
    "    \"haven't been\": \"have been\",\n",
    "    \"hasn't been\": \"has been\",\n",
    "    \"hadn't been\": \"had been\",\n",
    "    \"will not be\": \"will be\",\n",
    "    \"would not be\": \"would be\",\n",
    "    \"should not be\": \"should be\",\n",
    "    \"could not be\": \"could be\",\n",
    "    \"must not be\": \"must be\",\n",
    "    \"might not be\": \"might be\",\n",
    "    \"don't\": \"do\",\n",
    "    \"does not\": \"does\",\n",
    "    \"did not\": \"did\",\n",
    "    \"has not\": \"has\",\n",
    "    \"have not\": \"have\",\n",
    "    \"had not\": \"had\",\n",
    "    \"can not\": \"can\",\n",
    "    \"could not\": \"could\",\n",
    "    \"shall not\": \"shall\",\n",
    "    \"should not\": \"should\",\n",
    "    \"will not\": \"will\",\n",
    "    \"would not\": \"would\",\n",
    "    \"may not\": \"may\",\n",
    "    \"might not\": \"might\",\n",
    "    \"must not\": \"must\",\n",
    "    \"ought not\": \"ought\",\n",
    "    \"need not\": \"need\",\n",
    "    \"dare not\": \"dare\",\n",
    "    \"used not\": \"used\",\n",
    "    \"needn't\": \"need\",\n",
    "    \"daren't\": \"dare\",\n",
    "    \"mustn't\": \"must\",\n",
    "    \"shouldn't\": \"should\",\n",
    "    \"wouldn't\": \"would\",\n",
    "    \"couldn't\": \"could\",\n",
    "    \"wasn't\": \"was\",\n",
    "    \"weren't\": \"were\",\n",
    "    \"isn't\": \"is\",\n",
    "    \"aren't\": \"are\",\n",
    "    \"didn't\": \"did\",\n",
    "    \"doesn't\": \"does\",\n",
    "    \"hasn't\": \"has\",\n",
    "    \"haven't\": \"have\",\n",
    "    \"hadn't\": \"had\",\n",
    "    \"can't\": \"can\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find key Apple|PROPN in table",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/Questions_generation/scripts/main.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m freq \u001b[39m=\u001b[39m doc[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39ms2v_freq\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m vector \u001b[39m=\u001b[39m doc[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39ms2v_vec\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B150.136.120.65/home/ubuntu/Questions_generation/scripts/main.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m most_similar \u001b[39m=\u001b[39m doc[\u001b[39m3\u001b[39;49m]\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39;49ms2v_most_similar(\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sense2vec/component.py:207\u001b[0m, in \u001b[0;36mSense2VecComponent.s2v_most_similar\u001b[0;34m(self, obj, n)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\"\"\"Extension attribute method. Get the most similar entries.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[39mobj (Token / Span): The object the attribute is called on.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m    ((word, sense), score) tuples.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms2v_key(obj)\n\u001b[0;32m--> 207\u001b[0m results \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mdoc\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39;49m_s2v\u001b[39m.\u001b[39;49mmost_similar([key], n\u001b[39m=\u001b[39;49mn)\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms2v\u001b[39m.\u001b[39msplit_key(result), score) \u001b[39mfor\u001b[39;00m result, score \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sense2vec/sense2vec.py:209\u001b[0m, in \u001b[0;36mSense2Vec.most_similar\u001b[0;34m(self, keys, n, batch_size)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys:\n\u001b[1;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m in table\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache[\u001b[39m\"\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n:\n\u001b[1;32m    211\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectors), n)\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find key Apple|PROPN in table"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/miniforge3/lib/python3.9/site-packages/sense2vec/sense2vec.py\u001b[0m(209)\u001b[0;36mmost_similar\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    207 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    208 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 209 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find key {key} in table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    210 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    211 \u001b[0;31m            \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk(\"/home/ubuntu/Questions_generation/s2v_old\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "#s2v.from_disk(\"/path/to/s2v_reddit_2015_md\")\n",
    "\n",
    "doc = nlp(\"A sentence about Apple.\")\n",
    "assert doc[3].text == \"Apple\"\n",
    "freq = doc[3]._.s2v_freq\n",
    "vector = doc[3]._.s2v_vec\n",
    "most_similar = doc[3]._.s2v_most_similar(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inference for factors: rome, the final answer is yes',\n",
       " '(2) inference for factors: rome,',\n",
       " 'Inference for factors: rome is a method for editing the model of a transformer language.',\n",
       " 'Model editing of factual associations in transformer language models.',\n",
       " 'Model editing of factual associations in transformer language models',\n",
       " 'Rome is a method for modifying the weights of mid-layer feed-forward modules in a transformer-language model.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui=['a) Inference for factors: rome, the final answer is yes ', \n",
    "    '(2) Inference for factors: rome, the final answer is (p)',\n",
    "    'Inference for factors: rome is a method for editing the model of a transformer language.', 'Model editing of factual associations in transformer language models.', 'Model editing of factual associations in transformer language models', 'Rome is a method for modifying the weights of mid-layer feed-forward modules in a transformer-language model.']\n",
    "output_a =[]\n",
    "import re\n",
    "for a in ui:\n",
    "          cur_a = a.strip()\n",
    "          cur_a = re.sub(r\"^\\([a-zA-Z]\\)|^[a-zA-Z]\\)\", \"\", cur_a)\n",
    "          cur_a = re.sub(r'[Tt]he final answer(:| is: |s are:| is) ?\\([A-Za-z]\\)\\.?$','',cur_a, re.IGNORECASE)\n",
    "          output_a.append(cur_a.strip().capitalize())\n",
    "          \n",
    "output_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go home, yes. Do you want to come with me, no\n"
     ]
    }
   ],
   "source": [
    "\n",
    "string = \"I want to go home, yes. Do you want to come with me, yes\"\n",
    "\n",
    "# Use the sub() function to replace \"yes\" at the end of a sentence with \"no\"\n",
    "modified_string = re.sub(r\"\\byes\\.?$\", \"no\", string, flags=re.IGNORECASE)\n",
    "\n",
    "print(modified_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in np.unique(questions_df.section_n):\n",
    "    section_output = {'question':[],'answer':[],'score':[]}\n",
    "    for chunk in np.unique(questions_df.loc[questions_df.section_n==i,'section_n_chunk']):\n",
    "        chunk_questions = questions_df.query(f'section_n_chunk=={chunk}')\n",
    "        for _,q in chunk_questions.iterrows():\n",
    "            if q['take']:\n",
    "                section_output['question'].append(q.new_question)\n",
    "                section_output['answer'].append(q['selected_ans'])\n",
    "                section_output['score'].append(q['RQUGE'])\n",
    "    output.append(section_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections\n",
    "dp = [len(x.split())>350 for x in sections]\n",
    "print(dp)\n",
    "len(list(filter(lambda x: x[1] is not None and dp[x[0]], enumerate(sections))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 ms ± 281 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "121 ms ± 894 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = ['Q:What was the derived uncertainty of the model?',\n",
    "'A:4.1 minutes',\n",
    "'Q:What is the uncertainty of the GBT model?',\n",
    "'A:Gbt is the best performing model. the mean absolute error values in the table suggest that the performance is similar across all three algorithms. the derived uncertainty of the model was 4.1 minutes.',\n",
    "'Q:What type of model is Barket-FM-DOSM?',\n",
    "'A:Duration of surgery model',\n",
    "'Q:What did the comparison lead to?',\n",
    "'A:This comparison led to the conclusion that neither machine learning algorithms nor the dataset are the source of differences in the models performance. the major effector is that the set of features is the source.',\n",
    "'Q:What is the main effector of differences in the model performance?',\n",
    "'A:Dr. barket developed the model. the model is based on the surgery dataset. the main effector of such differences is the set of features.',\n",
    "'Q:What is the most important feature?',\n",
    "'A:Duration of surgery is the most important feature.',\n",
    "'Q:3 out of 8 features selected are the same for which two methods?',\n",
    "'A:Shap and pearson correlation.',\n",
    "'Q:Where is Table 1 located?',\n",
    "'A:Using machine learning techniques to develop supervised models that predict duration of surgery from features related to patients, physicians, and surgeries.',\n",
    "'Q:What can be done with the predictions?',\n",
    "'A:Use the dos value predicted by our model for surgery scheduling can decrease patient waiting time and maximize surgical staff idle time.']\n",
    "\n",
    "%timeit mcq.sentence_model.encode(text)\n",
    "%timeit [mcq.sentence_model.encode(t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/generation/beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 13])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "def check(args,prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    print(input_ids.shape)\n",
    "    res = model.generate(input_ids, **args)\n",
    "    return res['sequences'].shape\n",
    "    output = tokenizer.batch_decode(res['sequences'], skip_special_tokens=True)\n",
    "\n",
    "    output = [item.split(\"<sep>\") for item in output]\n",
    "    if all([len(cur_sen)==1 for cur_sen in output]):  \n",
    "        output = [cur_sen[0] for cur_sen in output]\n",
    "    if len(output)==1 and isinstance(output[0],list):\n",
    "        output = output[0]\n",
    "    output = [qs.strip() for qs in output]\n",
    "    output = [*Counter(output)] #remove exactly similar questions if exist\n",
    "\n",
    "    # Data originally sorted by probabilities, multiply be the penalities activated (`length_penalty` and/or `diversity_penalty`).\n",
    "    # We want it to be sorted by the perplexity (PPL) score, the coherence of the sentence - probability normalized by the length of the sentence. \n",
    "    ppl = [np.exp(np.array(log_likelihoods.cpu()).mean() / np.array(len(cur_output.split())).mean()) for log_likelihoods,cur_output in zip(res['sequences_scores'],output)]\n",
    "    sorted_idx  = np.argsort(ppl)\n",
    "    return [output[id] for id in sorted_idx], [ppl[id] for id in sorted_idx]\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration\n",
    "checkpoint = \"google/flan-t5-large\"#\"IsaacBot/flan-t5-small-mfaq-finetuned-question-generation-context-only\"# \"google/flan-t5-xl\"#\"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "COMES_sections = ['COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.',\n",
    "                  ' Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.',\n",
    "                  ' For a comprehensive survey on the summary evaluation resources see Koto et al. and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task – based on question answering, similarity between summary and reference embeddings.',\n",
    "                  ' COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.']\n",
    "print(check(generator_args,['generate question:'+i for i in COMES_sections]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is there? <re.Match object; span=(0, 13), match='Who is there?'>\n",
      "True\n",
      "who is there? <re.Match object; span=(0, 13), match='who is there?'>\n",
      "True\n",
      "WHO is there? <re.Match object; span=(0, 13), match='WHO is there?'>\n",
      "True\n",
      "Where is he? <re.Match object; span=(0, 12), match='Where is he?'>\n",
      "False\n",
      "Where is he None\n",
      "True\n",
      "Where is he ?  <re.Match object; span=(0, 13), match='Where is he ?'>\n",
      "False\n",
      "Where is he   None\n",
      "True\n",
      "What is the final answer to the question ?  <re.Match object; span=(0, 42), match='What is the final answer to the question ?'>\n",
      "True\n",
      "Who are you? <re.Match object; span=(0, 12), match='Who are you?'>\n",
      "True\n",
      "who are you? <re.Match object; span=(0, 12), match='who are you?'>\n",
      "True\n",
      "WHO are you? <re.Match object; span=(0, 12), match='WHO are you?'>\n",
      "True\n",
      "WHO are you None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "def filter_questions_by_re(q):\n",
    "\n",
    "    # excluded expressions\n",
    "    #start with\n",
    "    who_start_re = r'([Ww]ho|WHO .*)'\n",
    "    # What is the final answer to the question\n",
    "    q_ref_re = r'[Ww]hat is|are the final answer|answers to the question|questions'\n",
    "    starts_excluded_re = [who_start_re,q_ref_re]\n",
    "    # must have expressions\n",
    "    q_mark_re = r'.*\\?'\n",
    "    start_filter = [re.match(cur_re, q)!=None for cur_re in starts_excluded_re]\n",
    "    print(q,re.search(q_mark_re, q))\n",
    "    #return True if question is discard\n",
    "    return any(start_filter) or re.search(q_mark_re, q)==None\n",
    "\n",
    "print(filter_questions_by_re(\"Who is there?\"))  # True\n",
    "print(filter_questions_by_re(\"who is there?\"))  # True\n",
    "print(filter_questions_by_re(\"WHO is there?\"))  # True\n",
    "\n",
    "print(filter_questions_by_re(\"Where is he?\"))   # False\n",
    "print(filter_questions_by_re(\"Where is he\"))   # False\n",
    "print(filter_questions_by_re(\"Where is he ? \"))   # False\n",
    "print(filter_questions_by_re(\"Where is he  \"))   # False\n",
    "print(filter_questions_by_re(\"What is the final answer to the question ? \"))   # False\n",
    "\n",
    "print(filter_questions_by_re(\"Who are you?\"))   # True\n",
    "print(filter_questions_by_re(\"who are you?\"))   # True\n",
    "print(filter_questions_by_re(\"WHO are you?\"))   # True\n",
    "print(filter_questions_by_re(\"WHO are you\"))   # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def check_start(text):\n",
    "    regex = r'([Ww]ho|WHO .*)'\n",
    "    match = re.match(regex, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(check_start(\"Who is there?\"))  # True\n",
    "print(check_start(\"who is there?\"))  # True\n",
    "print(check_start(\"WHO is there?\"))  # True\n",
    "print(check_start(\"Where is he?\"))   # False\n",
    "print(check_start(\"Who are you?\"))   # True\n",
    "print(check_start(\"who are you?\"))   # True\n",
    "print(check_start(\"WHO are you?\"))   # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n",
      "Prediction triplets sentence 0\n",
      "[{'head': 'John F. Kennedy School of Government', 'type': 'part of', 'tail': 'Harvard University'}]\n",
      "Prediction triplets sentence 1\n",
      "[{'head': 'World War II', 'type': 'has effect', 'tail': 'Holocaust'}]\n",
      "Prediction triplets sentence 2\n",
      "[{'head': 'World War I', 'type': 'followed by', 'tail': 'World War II'}, {'head': 'World War II', 'type': 'follows', 'tail': 'World War I'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 256,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 3,\n",
    "    \"num_return_sequences\": 3,\n",
    "}\n",
    "\n",
    "# Text to extract triplets from\n",
    "text = 'Punta Cana is a resort town in the municipality of Higüey, in La Altagracia Province, the easternmost province of the Dominican Republic.'\n",
    "name = 'finance_course2'\n",
    "import json\n",
    "JSON_path = '/home/ubuntu/Questions_generation/Financial Markets Course 2.json'\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "\n",
    "text = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/outputs/03_01_23_finance_course2/questions.txt') as f:\n",
    "    text = f.read()\n",
    "text = text.replace('\\nQ:','.\\n ').replace('\\nA:',' ').replace ('--------------------------------------------------','').split('.')\n",
    "text = [' ']#text[:2]\n",
    "# Tokenizer text\n",
    "model_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n",
    "\n",
    "# Generate\n",
    "generated_tokens = model.generate(\n",
    "    model_inputs[\"input_ids\"].to(model.device),\n",
    "    attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n",
    "    **gen_kwargs,\n",
    ")\n",
    "\n",
    "# Extract text\n",
    "decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "# Extract triplets\n",
    "print(text)\n",
    "for idx, sentence in enumerate(decoded_preds):\n",
    "    print(f'Prediction triplets sentence {idx}')\n",
    "    print(extract_triplets(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Israel 10 16 GPE\n",
      "1948 34 38 DATE\n",
      "what year Israel was established? 1948\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ab9e570fac70419d919e3e605a8edcd0-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">what</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">year</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Israel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">established?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">1948</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ab9e570fac70419d919e3e605a8edcd0-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ab9e570fac70419d919e3e605a8edcd0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ab9e570fac70419d919e3e605a8edcd0-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ab9e570fac70419d919e3e605a8edcd0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ab9e570fac70419d919e3e605a8edcd0-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ab9e570fac70419d919e3e605a8edcd0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ab9e570fac70419d919e3e605a8edcd0-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ab9e570fac70419d919e3e605a8edcd0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try: import spacy   \n",
    "except: import spacy\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/outputs/03_01_23_finance_course2/questions.txt') as f:\n",
    "    text = f.read()\n",
    "text = text.replace('\\nQ:','.\\n ').replace('\\nA:',' ').replace ('--------------------------------------------------','').split('.')\n",
    "text = \"what year Israel was established? 1948\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "if isinstance(text,list):\n",
    "    doc= nlp(\"\\n\".join(text))\n",
    "else:\n",
    "    doc =nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "from spacy import displacy\n",
    "print(text)\n",
    "def only_ents(text,doc):\n",
    "    questions_mark_idx = text.find('?')\n",
    "\n",
    "#displacy.serve(doc, style=\"ent\")\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Israel 10 16 GPE\n",
      "1948 50 54 DATE\n",
      "years 1948\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"8ea65da2d2e24135a09e22e085a843d7-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">what</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">year</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Israel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">established?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">few</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">after</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">1948.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-8ea65da2d2e24135a09e22e085a843d7-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-8ea65da2d2e24135a09e22e085a843d7-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6093b2876bf14afcb71957f25ee676f7-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">few</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">after</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">1948</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6093b2876bf14afcb71957f25ee676f7-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6093b2876bf14afcb71957f25ee676f7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6093b2876bf14afcb71957f25ee676f7-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6093b2876bf14afcb71957f25ee676f7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6093b2876bf14afcb71957f25ee676f7-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6093b2876bf14afcb71957f25ee676f7-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    punct_or_stopwords = set([*stopwords.words('english'), *string.punctuation])\n",
    "    return \" \".join([token for token in tokens if token.lower() not in punct_or_stopwords])\n",
    "def only_ents(text,doc):\n",
    "    ans_idx = text.find('?')+2\n",
    "    filtered_ans = remove_stop_words(text[ans_idx:])\n",
    "    print(filtered_ans)\n",
    "    return any([filtered_ans.strip() == cur_ent.text for cur_ent in doc.ents])\n",
    "\n",
    "text = \"what year Israel was established? few years after 1948.\"\n",
    "doc =nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "print(only_ents(text,doc))\n",
    "displacy.render(doc, style='dep')\n",
    "displacy.render(nlp('few years after 1948'), style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun phrase as adverbial modifier'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('npadvmod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/spacy/displacy/__init__.py:103: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5f09888092be4f93b799ff26ddc5f733-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">few</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">after</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">1948</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5f09888092be4f93b799ff26ddc5f733-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5f09888092be4f93b799ff26ddc5f733-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5f09888092be4f93b799ff26ddc5f733-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5f09888092be4f93b799ff26ddc5f733-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5f09888092be4f93b799ff26ddc5f733-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5f09888092be4f93b799ff26ddc5f733-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.render(nlp('few years after 1948'), style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object of preposition'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('pobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script src=\"lib/bindings/utils.js\"></script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 700px;\n",
       "                 height: 700px;\n",
       "                 background-color: #eeeeee;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"color\": \"#00FF00\", \"id\": \"Great Depression\", \"label\": \"Great Depression\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"1930\", \"label\": \"1930\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"1930s\", \"label\": \"1930s\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"2003\", \"label\": \"2003\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Variance\", \"label\": \"Variance\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Probability\", \"label\": \"Probability\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Hard and soft science\", \"label\": \"Hard and soft science\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Science\", \"label\": \"Science\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Mathematical model\", \"label\": \"Mathematical model\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Statistical model\", \"label\": \"Statistical model\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Mathematics\", \"label\": \"Mathematics\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Investor\", \"label\": \"Investor\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Investment\", \"label\": \"Investment\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Risk\", \"label\": \"Risk\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Discipline\", \"label\": \"Discipline\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Finance\", \"label\": \"Finance\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Standard deviation\", \"label\": \"Standard deviation\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Covariance\", \"label\": \"Covariance\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Random variable\", \"label\": \"Random variable\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"2000\", \"label\": \"2000\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"2002\", \"label\": \"2002\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Crisis\", \"label\": \"Crisis\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Stock market\", \"label\": \"Stock market\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Insurance\", \"label\": \"Insurance\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Apple\", \"label\": \"Apple\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"S\\u0026P 500\", \"label\": \"S\\u0026P 500\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Warren Buffett\", \"label\": \"Warren Buffett\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"The Home Depot\", \"label\": \"The Home Depot\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Walmart\", \"label\": \"Walmart\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Steve Jobs\", \"label\": \"Steve Jobs\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Pancreatic cancer\", \"label\": \"Pancreatic cancer\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"NeXT Computer\", \"label\": \"NeXT Computer\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Benoit Mandelbrot\", \"label\": \"Benoit Mandelbrot\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Probability theory\", \"label\": \"Probability theory\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Parabola\", \"label\": \"Parabola\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Function (mathematics)\", \"label\": \"Function (mathematics)\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Fat-tailed distribution\", \"label\": \"Fat-tailed distribution\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Normal distribution\", \"label\": \"Normal distribution\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Wall Street Crash of 1929\", \"label\": \"Wall Street Crash of 1929\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"1929\", \"label\": \"1929\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"1928\", \"label\": \"1928\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Black Monday (1987)\", \"label\": \"Black Monday (1987)\", \"shape\": \"circle\"}, {\"color\": \"#00FF00\", \"id\": \"Law of large numbers\", \"label\": \"Law of large numbers\", \"shape\": \"circle\"}]);\n",
       "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"Great Depression\", \"label\": \"start time\", \"title\": \"start time\", \"to\": \"1930\"}, {\"arrows\": \"to\", \"from\": \"Great Depression\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"1930s\"}, {\"arrows\": \"to\", \"from\": \"2003\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"2003\"}, {\"arrows\": \"to\", \"from\": \"Variance\", \"label\": \"facet of\", \"title\": \"facet of\", \"to\": \"Probability\"}, {\"arrows\": \"to\", \"from\": \"Variance\", \"label\": \"part of\", \"title\": \"part of\", \"to\": \"Probability\"}, {\"arrows\": \"to\", \"from\": \"Hard and soft science\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Science\"}, {\"arrows\": \"to\", \"from\": \"Mathematical model\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Statistical model\"}, {\"arrows\": \"to\", \"from\": \"Mathematics\", \"label\": \"has part\", \"title\": \"has part\", \"to\": \"Statistical model\"}, {\"arrows\": \"to\", \"from\": \"Statistical model\", \"label\": \"part of\", \"title\": \"part of\", \"to\": \"Mathematics\"}, {\"arrows\": \"to\", \"from\": \"Investor\", \"label\": \"field of this occupation\", \"title\": \"field of this occupation\", \"to\": \"Investment\"}, {\"arrows\": \"to\", \"from\": \"Investment\", \"label\": \"practiced by\", \"title\": \"practiced by\", \"to\": \"Investor\"}, {\"arrows\": \"to\", \"from\": \"Risk\", \"label\": \"instance of\", \"title\": \"instance of\", \"to\": \"Discipline\"}, {\"arrows\": \"to\", \"from\": \"Risk\", \"label\": \"studied by\", \"title\": \"studied by\", \"to\": \"Finance\"}, {\"arrows\": \"to\", \"from\": \"Standard deviation\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Variance\"}, {\"arrows\": \"to\", \"from\": \"Covariance\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Variance\"}, {\"arrows\": \"to\", \"from\": \"Covariance\", \"label\": \"facet of\", \"title\": \"facet of\", \"to\": \"Random variable\"}, {\"arrows\": \"to\", \"from\": \"2000\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"2000\"}, {\"arrows\": \"to\", \"from\": \"2002\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"2002\"}, {\"arrows\": \"to\", \"from\": \"Crisis\", \"label\": \"facet of\", \"title\": \"facet of\", \"to\": \"Stock market\"}, {\"arrows\": \"to\", \"from\": \"Insurance\", \"label\": \"part of\", \"title\": \"part of\", \"to\": \"Finance\"}, {\"arrows\": \"to\", \"from\": \"Insurance\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Finance\"}, {\"arrows\": \"to\", \"from\": \"Insurance\", \"label\": \"facet of\", \"title\": \"facet of\", \"to\": \"Finance\"}, {\"arrows\": \"to\", \"from\": \"Apple\", \"label\": \"part of\", \"title\": \"part of\", \"to\": \"S\\u0026P 500\"}, {\"arrows\": \"to\", \"from\": \"Apple\", \"label\": \"owned by\", \"title\": \"owned by\", \"to\": \"Warren Buffett\"}, {\"arrows\": \"to\", \"from\": \"Warren Buffett\", \"label\": \"owner of\", \"title\": \"owner of\", \"to\": \"Apple\"}, {\"arrows\": \"to\", \"from\": \"The Home Depot\", \"label\": \"parent organization\", \"title\": \"parent organization\", \"to\": \"Walmart\"}, {\"arrows\": \"to\", \"from\": \"Walmart\", \"label\": \"inception\", \"title\": \"inception\", \"to\": \"2000\"}, {\"arrows\": \"to\", \"from\": \"Apple\", \"label\": \"subsidiary\", \"title\": \"subsidiary\", \"to\": \"Walmart\"}, {\"arrows\": \"to\", \"from\": \"Steve Jobs\", \"label\": \"significant event\", \"title\": \"significant event\", \"to\": \"Pancreatic cancer\"}, {\"arrows\": \"to\", \"from\": \"Apple\", \"label\": \"founded by\", \"title\": \"founded by\", \"to\": \"Steve Jobs\"}, {\"arrows\": \"to\", \"from\": \"Steve Jobs\", \"label\": \"employer\", \"title\": \"employer\", \"to\": \"Apple\"}, {\"arrows\": \"to\", \"from\": \"Apple\", \"label\": \"stock exchange\", \"title\": \"stock exchange\", \"to\": \"Stock market\"}, {\"arrows\": \"to\", \"from\": \"Apple\", \"label\": \"instance of\", \"title\": \"instance of\", \"to\": \"Stock market\"}, {\"arrows\": \"to\", \"from\": \"S\\u0026P 500\", \"label\": \"instance of\", \"title\": \"instance of\", \"to\": \"Stock market\"}, {\"arrows\": \"to\", \"from\": \"NeXT Computer\", \"label\": \"founded by\", \"title\": \"founded by\", \"to\": \"Steve Jobs\"}, {\"arrows\": \"to\", \"from\": \"Benoit Mandelbrot\", \"label\": \"field of work\", \"title\": \"field of work\", \"to\": \"Probability theory\"}, {\"arrows\": \"to\", \"from\": \"Parabola\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Function (mathematics)\"}, {\"arrows\": \"to\", \"from\": \"Fat-tailed distribution\", \"label\": \"opposite of\", \"title\": \"opposite of\", \"to\": \"Normal distribution\"}, {\"arrows\": \"to\", \"from\": \"Wall Street Crash of 1929\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"1929\"}, {\"arrows\": \"to\", \"from\": \"1929\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"1929\"}, {\"arrows\": \"to\", \"from\": \"1928\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"1928\"}, {\"arrows\": \"to\", \"from\": \"Black Monday (1987)\", \"label\": \"point in time\", \"title\": \"point in time\", \"to\": \"Black Monday (1987)\"}, {\"arrows\": \"to\", \"from\": \"Law of large numbers\", \"label\": \"subclass of\", \"title\": \"subclass of\", \"to\": \"Probability\"}, {\"arrows\": \"to\", \"from\": \"Law of large numbers\", \"label\": \"instance of\", \"title\": \"instance of\", \"to\": \"Probability\"}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\n",
       "    \"configure\": {\n",
       "        \"enabled\": false\n",
       "    },\n",
       "    \"edges\": {\n",
       "        \"color\": {\n",
       "            \"inherit\": true\n",
       "        },\n",
       "        \"smooth\": {\n",
       "            \"enabled\": true,\n",
       "            \"type\": \"dynamic\"\n",
       "        }\n",
       "    },\n",
       "    \"interaction\": {\n",
       "        \"dragNodes\": true,\n",
       "        \"hideEdgesOnDrag\": false,\n",
       "        \"hideNodesOnDrag\": false\n",
       "    },\n",
       "    \"physics\": {\n",
       "        \"enabled\": true,\n",
       "        \"repulsion\": {\n",
       "            \"centralGravity\": 0.2,\n",
       "            \"damping\": 0.09,\n",
       "            \"nodeDistance\": 200,\n",
       "            \"springConstant\": 0.05,\n",
       "            \"springLength\": 200\n",
       "        },\n",
       "        \"solver\": \"repulsion\",\n",
       "        \"stabilization\": {\n",
       "            \"enabled\": true,\n",
       "            \"fit\": true,\n",
       "            \"iterations\": 1000,\n",
       "            \"onlyDynamicEdges\": false,\n",
       "            \"updateInterval\": 50\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    import spacy\n",
    "try:\n",
    "    from get_knowledge_graph import from_text_to_kb\n",
    "except:\n",
    "    from get_knowledge_graph import from_text_to_kb\n",
    "\n",
    "if isinstance(text,list):\n",
    "    kb = from_text_to_kb(\" \".join(text), \"\", verbose=True)\n",
    "else:\n",
    "    kb = from_text_to_kb(text, \"\",span_length=128, verbose=True)\n",
    "kb.print()\n",
    "'''\n",
    "filename = \"network_1_napoleon.html\"\n",
    "\n",
    "import IPython\n",
    "#save_network_html(kb, filename=filename)\n",
    "#save_kb(kb, filename.split(\".\")[0] + \".p\")\n",
    "IPython.display.HTML(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.8/site-packages (4.25.1)\n",
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Collecting newspaper3k\n",
      "  Using cached newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Collecting GoogleNews\n",
      "  Using cached GoogleNews-1.6.6-py3-none-any.whl (8.0 kB)\n",
      "Collecting pyvis\n",
      "  Using cached pyvis-0.3.1.tar.gz (748 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.local/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.8/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting lxml>=3.6.0\n",
      "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tinysegmenter==0.3\n",
      "  Using cached tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Using cached feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "Collecting tldextract>=2.0.1\n",
      "  Using cached tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Using cached feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Requirement already satisfied: nltk>=3.2.1 in ./.local/lib/python3.8/site-packages (from newspaper3k) (3.7)\n",
      "Collecting cssselect>=0.9.2\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.local/lib/python3.8/site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Using cached jieba3k-0.35.1.zip (7.4 MB)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in ./.local/lib/python3.8/site-packages (from newspaper3k) (9.3.0)\n",
      "Collecting dateparser\n",
      "  Using cached dateparser-1.1.5-py2.py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: ipython>=5.3.0 in ./.local/lib/python3.8/site-packages (from pyvis) (8.7.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /usr/lib/python3/dist-packages (from pyvis) (2.10.1)\n",
      "Collecting jsonpickle>=1.4.1\n",
      "  Using cached jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: networkx>=1.11 in ./.local/lib/python3.8/site-packages (from pyvis) (2.8.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting sgmllib3k\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
      "Collecting requests-file>=1.4\n",
      "  Using cached requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: six in ./.local/lib/python3.8/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: pytz in ./.local/lib/python3.8/site-packages (from dateparser->GoogleNews) (2022.6)\n",
      "Collecting tzlocal\n",
      "  Using cached tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: backcall in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: stack-data in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: pickleshare in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (2.13.0)\n",
      "Requirement already satisfied: decorator in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (5.7.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.local/lib/python3.8/site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Collecting backports.zoneinfo; python_version < \"3.9\"\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.local/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.local/lib/python3.8/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.local/lib/python3.8/site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in ./.local/lib/python3.8/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Collecting tzdata; python_version >= \"3.6\"\n",
      "  Using cached tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
      "Building wheels for collected packages: wikipedia, pyvis, tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11685 sha256=66b31882f894c351de1b755890ffb6582464f5339c35f48fbb5725f838719eac\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/07/93/05/72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
      "  Building wheel for pyvis (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyvis: filename=pyvis-0.3.1-py3-none-any.whl size=755828 sha256=f57750c0cc91b17c061963c93e8af2d153d5980930b4866299d4da2c5b0d5ed0\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a4/0c/61/8469ca276f96ab772c3acc7f47d71e9737cbdf6f446f017f48\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=f7795882609d0fd1d81aba57f8c02fc03d0e695ac76832a95aadfd3237ad5720\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/99/74/83/8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3355 sha256=a8e3b9c1ee0398cc350b633c11fbceb6adbdb2f64b0fb0eef7c8febdaf21cc17\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b6/09/68/a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398406 sha256=dc6d1e4c5ef398a117382784234d2887eae00dc8ece4c50f1ac943f2edbde3fb\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/7e/0c/54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6064 sha256=34843a1265fca19decb6d86e457aa7b93ae039973b56c8fbde2bac98fc4bbaaf\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built wikipedia pyvis tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia, lxml, tinysegmenter, sgmllib3k, feedparser, requests-file, tldextract, feedfinder2, cssselect, jieba3k, newspaper3k, backports.zoneinfo, tzdata, pytz-deprecation-shim, tzlocal, dateparser, GoogleNews, jsonpickle, pyvis\n",
      "Successfully installed GoogleNews-1.6.6 backports.zoneinfo-0.2.1 beautifulsoup4-4.11.1 cssselect-1.2.0 dateparser-1.1.5 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 jsonpickle-3.0.1 lxml-4.9.2 newspaper3k-0.2.8 pytz-deprecation-shim-0.1.0.post0 pyvis-0.3.1 requests-file-1.5.1 sgmllib3k-1.0.0 soupsieve-2.3.2.post1 tinysegmenter-0.3 tldextract-3.4.0 tzdata-2022.7 tzlocal-4.2 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers wikipedia newspaper3k GoogleNews pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MIT CSAIL.', 'Northeastern University.', 'MIT CSAIL Yonatan Belinkov† Technion – IIT.', 'We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model’s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset of difficult counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/.', ' Large language models can predict factual statements about the world. For example, given the prefix “The Space Needle is located in the city of,” GPT will reliably predict the true answer: “Seattle” Factual knowledge has been observed to emerge in both autoregressive GPT models and masked BERT models. In this paper, we investigate how such factual associations are stored within GPT-like transformer models.  Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name. Second, we test this finding in model weights by introducing a Rank-One Model Editing method.', ' Causal Traces compute the causal effect of neuron activations by running the network twice. We find that ROME is similarly effective to other modelediting approaches on a standard zero-shot relation extraction benchmark (Section 3.2) Compared to previous fine-tuning (Zhu et al., 2020), interpretability-based. methods, ROME achieves good generalization and specificity simultaneously simultaneously, whereas previous approaches sacrifice one or the other.', ' Autoregressive transformer language model G : X → Y over vocabulary V maps a token sequence x = [x1,..., xT ] to a probability distribution y ⊂ R|V | that predicts next-token continuations of x. Within the transformer, the ith token is embedded as a series of hidden state vectors h(l) i, beginning with h(0) i = emb(xi) + pos(i) ∈ RH. The final output y = decode(h(L) T ) is read from the last hidden state.  Strong causality at a ‘late site’ in the last layers at the last token is unsurprising, but strongly causal states at an ‘early site” in middle layers is a new discovery. (b) MLP contributions dominate the early site. (c) Attention is important at the late site. Each layer’s MLP is a two-layer neural network.', ' The grid of states (Figure 1) forms a causal graph (Pearl, 2009) describing dependencies between the hidden variables. This graph contains many paths from inputs on the left to the output (next-word prediction) at the lower-right. We wish to understand if there are specific hidden state variables that are more important than others when recalling a fact. As Vig et al. (2020b) have shown, this is a natural case for causal mediation analysis.  G is allowed to continue normally, giving us a set of corrupted activations. Because G loses some information about the subject, it will likely return an incorrect answer. The corrupted-with-restoration run, lets G run computations on the noisy embeddings as in the corrupted baseline. The ability of a few clean states to recover the correct fact, despite many other states being corrupted by the obfuscated subject, will indicate their causal importance in the computation graph. The total effect (TE) is the difference between these quantities: TE = P[o] − P∗[o].  The indirect effect (IE) of a specific mediating state h(l) i is defined as the difference between the probability of o under the corrupted version and the probability when that state is set to its clean version, while the subject remains corrupted. Our methods also apply to GPT variants such as Wang & Komatsuzaki (2021) that put attention in parallel to the MLP. We select ν to be 3 times larger than the empirical standard deviation of embeddings.', ' Figure 2 plots the AIE of the internal components of GPT-2 XL (1.5B parameters) The ATE of this experiment is 18.6%, and we note that a large portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the last subject token. The presence of strong causal states at a late site immediately before the prediction is unsurprising, but their emergence at an early site is a new discovery. Decomposing the causal effects suggests a decisive role for MLP modules at the early site.  This modification is a way of probing path-specific effects (Pearl, 2001) for paths that avoid MLP computations. Causal Tracing is more informative than gradient-based salience methods such as integrated gradients. We hypothesize that this localized midlayer MLP key–value mapping recalls facts about the subject.', ' Based on causal traces, we posit a specific mechanism for storage of factual associations. Each midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall memorized properties about that subject. Middle layer MLP outputs accumulate information, then the summed information is copied to the last token by attention at high layers. This hypothesis localizes factual association along three dimensions, placing it (i) in the MLP modules (ii) at specific middle layers (iii) and specifically at the processing of the subject’s last token.  (e) To write new value vector v∗ into the layer, (f) we calculate a rank-one update to cause ˆ W (l) projk∗ = v, while minimizing interference with other memories stored in the layer. could be equivalently stored in any one of the middle MLP layers.', ' Causal Tracing has implicated MLP modules in recalling factual associations. Geva et al. (2021) observed that MLP layers can act as two-layer key–value memories. We hypothesize that MLPs can be modeled as a linear associative memory.', ' A new key–value pair (k∗, v∗) can be inserted optimally into the memory by solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using an optimization, but in a fully-connected layer, we can derive a closed form solution: minimize ∥ ˆWK − V. The solution is based on the Moore-Penrose pseudoinverse: W = V K+.  Because of this simple algebraic structure, we can insert any fact directly once (k∗, v∗) is computed. Step 1: Choosing k∗ to Select the Subject, we choose inputs that represent the subject at its last token as the lookup key. Step 2: We choose some vector value vâ� that encodes the new relation (r, o∗), as a property of s. Step 3: We sample xj by generating 50 random token sequences of length 2 to 10 using G.  Eqn.4a seeks a vector z that, when substituted as the output of the MLP at the token i at the end of the subject (notated G(m(l∗) i := z), will cause the network to predict the target object o∗ in response to the factual prompt p′ (of the form “{subject} is a”) The optimization does not directly alter model weights; it identifies a vector representation v∗ that when output at the targeted MLP module, represents the new property for the subject s.', ' We evaluate ROME on the Zero-Shot Re-  Table 1: zsRE Editing Results on GPT-2 XL. lation Extraction (zsRE) task. Our evaluation slice contains 10,000 records, each containing one factual statement, its paraphrase, and one unrelated fact. “Efficacy” and “Paraphrase” measure post-edit accuracy I o∗ = argmaxoPG′ [o], while “Specificity” measures the edited model’s accuracy on an unrelated fact.  ROME is competitive with hypernetworks and fine-tuning methods despite its simplicity. We find that it is not hard for ROME to insert an association that can be regurgitated by the model. Robustness under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks KE-zsRE and MEND-ZsRE.', ' Hase et al. (2021) observed that standard model-editing benchmarks underestimate difficulty by testing only proposals that the model previously scored as likely. We compile a set of more difficult false facts (s, r, oc) and report Paraphrase Scores (PS) and (PM) to measure efficacy of significant changes. To test generalization–specificity tradeoff, we report the harmonic mean of ES, PS, NS as Score (S) We also wish to measure semantic consistency of G′’s generations.  COUNTERFACT is a challenging evaluation dataset for evaluating counterfactual edits in language models. It contains 21,919 records with a diverse set of subjects, relations, and linguistic variations. The goal is to differentiate robust storage of new facts from the superficial regurgitation of target words. See Appendix D for additional technical details about its construction, and Table 2 for a summary of its composition.', ' In Section 2, we used Causal Tracing to identify decisive hidden states. To confirm that factual associations are stored in the MLP modules that output those states, we test ROME’s effectiveness when targeted at various layers and tokens. Figure 5 plots four metrics evaluating both generalization and specificity. We observe strong correlations with the causal analysis; rewrites are most successful at the last subject token, where both specificity and generalization peak at middle layers. The layers at which edits generalize best correspond to the middle layers of the early site identified by 7.  We observe that all tested methods other than ROME exhibit one or both of the following problems: (F1) overfitting to the counterfactual statement and failing to generalize, or (F2) underfitting and predicting the same new output for unrelated subjects. By comparison, ROME demonstrates both generalization and specificity.', ' Figure 6 compares generated text after applying counterfactual “Pierre Curie’s area of work is medicine” to GPT-2 XL. In this case, FT and ROME generalize well to paraphrases, describing the subject as a physician rather than a physicist for various wordings. FT+L, KE and MEND have problems with specificity, changing the profession of a totally unrelated subject.', ' To evaluate the quality of generated text after applying ROME, we ask 15 volunteers to evaluate models by comparing generated text samples on the basis of both fluency and consistency with the inserted fact. Evaluators find text generated by ROME to be somewhat less fluent than models editing using FT+L.', ' The purpose of ROME is to serve as a tool for understanding mechanisms of knowledge storage. It only edits a single fact at a time, and it is not intended as a practical method for large-scale model training. ROME and Causal Tracing shed light on factual association within GPT, but we have not investigated other kinds of learned beliefs such as logical, spatial, or numerical knowledge.', ' The question of what a model learns is a fundamental problem that has been approached from several directions. Causal effects have been used to probe important information within a network in a way that avoids misleading spurious correlations. The Causal Tracing method introduces paired interventions that allow explicit measurement of causal indirect effects of individual hidden state vectors. Another line of work aims to assess the knowledge within LMs by evaluating whether the model predicts pieces of knowledge. We use our Causal tracing method to measure the causal structure of models.  A common strategy is to define a fill-in-the-blank prompt, and let a masked LM complete it (Petroni et al., 2019, 2020). Later work showed that knowledge extraction can be improved by diversifying the prompts. We use it as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions. Different from prior work, we do not strive to extract the most knowledge from a model, but rather wish to understand mechanisms of knowledge recall.  Mitchell et al. (2021) presents a hyper-network method that learns to transform the decomposed terms of the gradient in order to efficiently predict a knowledge update. We compare with all these methods in our experiments, and find that our single-layer ROME parameter intervention has comparable capabilities.', ' We have clarified information flow during knowledge recall in autoregressive transformers. We have developed a simple, principled model editor called ROME. Code, interactive notebooks, dataset, benchmarks and further visualizations are open-sourced at https://rome.baulab.info.', ' The capability to edit large models also has the potential for abuse, such as adding malicious misinformation, bias, or other adversarial data to a model. We stress that large language models should not be used as an authoritative source of factual knowledge in critical settings.', None, 'Figure 7: Mean causal traces of GPT-XL over a sample of 1000 factual statements, shown as a line plot with.', None, ' Figure 9: Comparing mean causal traces across a wide range of different model sizes. Figure 15: Similar to Figure 7, but with an additional token corrupted after the subject token. Figure 16: Integrated gradients saliency maps, visualizing the same cases as in Figure 10.', 'Figure 18: GPT-J hyperparameter sweeps. The experimental setup is identical to that of GPT-2 XL.', ' Figure 23: Unconstrained Optimization Sweeps. Figure 25: Generation Samples for ROME v.s. AttnEdit. Figure 27: Human evaluation, random sample 1.']\n"
     ]
    }
   ],
   "source": [
    "with open('/home/ubuntu/Questions_generation/Locating and Editing Factual Associations in GPT.json') as f:\n",
    "    text = json.load(f)\n",
    "print([s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in text['sections'].items() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/spacy/displacy/__init__.py:103: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at Google in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generate question:COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.',\n",
       " 'generate question: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.',\n",
       " 'generate question: For a comprehensive survey on the summary evaluation resources see Koto et al. and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task – based on question answering, similarity between summary and reference embeddings.',\n",
       " 'generate question: COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['generate question:'+i for i in COMES_sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>section_n</th>\n",
       "      <th>section_rank</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>question_ppl</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>short_answer_1</th>\n",
       "      <th>short_answer_2</th>\n",
       "      <th>short_answer_3</th>\n",
       "      <th>short_answer_4</th>\n",
       "      <th>generated_selected_ans</th>\n",
       "      <th>selected_ans</th>\n",
       "      <th>new_question</th>\n",
       "      <th>RQUGE</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>What is the purpose of this paper?</td>\n",
       "      <td>0.053599</td>\n",
       "      <td>To evaluate text summarization systems using C...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>An evaluation model for text summarization sys...</td>\n",
       "      <td>This paper introduces a variant of the COMET m...</td>\n",
       "      <td>to assess machine translation quality</td>\n",
       "      <td>Use COMET for text summarization evaluation</td>\n",
       "      <td>evaluate text summarization systems</td>\n",
       "      <td>To evaluate the quality of text summarization ...</td>\n",
       "      <td>to evaluate text summarization systems using c...</td>\n",
       "      <td>To evaluate text summarization systems using C...</td>\n",
       "      <td>What is the purpose of this paper?</td>\n",
       "      <td>4.803611</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>The COMET score is calculated by what method?</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>semantic similarities between the translated a...</td>\n",
       "      <td>The COMET score is calculated by a stack of fe...</td>\n",
       "      <td>pre-trained multilingual language model to ext...</td>\n",
       "      <td>stack of feed-forward layers</td>\n",
       "      <td>human perception of translation quality</td>\n",
       "      <td>pre-trained multilingual language model</td>\n",
       "      <td>semantic similarities between the translated a...</td>\n",
       "      <td>pre-trained multilingual language model to ext...</td>\n",
       "      <td>pre-trained multilingual language model to ext...</td>\n",
       "      <td>What does COMET use?</td>\n",
       "      <td>4.583428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Annotation-based generative models: a new appr...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Using annotated datasets, we propose an automa...</td>\n",
       "      <td>In this paper we propose an annotated version ...</td>\n",
       "      <td>In this paper we propose an annotated version ...</td>\n",
       "      <td>COMES2 is a variant of the Metric Shared Task ...</td>\n",
       "      <td>an annotation-based generative model</td>\n",
       "      <td>Using annotated dataset to evaluate quality of...</td>\n",
       "      <td>Computational Evaluation of Summary Quality of...</td>\n",
       "      <td>Annotation-based generative models: a new appr...</td>\n",
       "      <td>in this paper we propose an annotated version ...</td>\n",
       "      <td>In this paper we propose an annotated version ...</td>\n",
       "      <td>How is the model evaluated?</td>\n",
       "      <td>4.370208</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>In this paper we propose an automatic method f...</td>\n",
       "      <td>In this paper we propose an automatic method f...</td>\n",
       "      <td>Using annotated MT data, we propose a variant ...</td>\n",
       "      <td>We propose a variant of the model – COMES2 – t...</td>\n",
       "      <td>Computational Modeling for Summary Evaluation</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>Using annotated MT data to predict summary eva...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>in this paper we propose an automatic method f...</td>\n",
       "      <td>In this paper we propose an automatic method f...</td>\n",
       "      <td>How is the COMES2 model evaluated?</td>\n",
       "      <td>4.240835</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>How to evaluate the quality of a generative mo...</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>This paper presents an approach to evaluate th...</td>\n",
       "      <td>Using annotated datasets, we propose COMES2 to...</td>\n",
       "      <td>In this paper, we propose COMES2 – a variant o...</td>\n",
       "      <td>COMES2 is a variant of the model that uses the...</td>\n",
       "      <td>CoMES2</td>\n",
       "      <td>the use of the COMET metrics</td>\n",
       "      <td>we introduce COMET2 and evaluate its performan...</td>\n",
       "      <td>COMET metric by Rei et al. (2020)</td>\n",
       "      <td>comes2 is a variant of the model that uses the...</td>\n",
       "      <td>COMES2 is a variant of the model that uses the...</td>\n",
       "      <td>How is COMES2 evaluated?</td>\n",
       "      <td>4.126691</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Using annotated WMT data for pre-training and ...</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>This paper presents an approach to automatical...</td>\n",
       "      <td>Using annotated WMT data for pre-training and ...</td>\n",
       "      <td>The COMET metric is a framework for evaluating...</td>\n",
       "      <td>COMES2 is a variant of the Metrics Shared Task...</td>\n",
       "      <td>Computational Evaluation of Summary Models usi...</td>\n",
       "      <td>Computational Evaluation of Summary Models</td>\n",
       "      <td>Using annotated WMT data for pre-training and ...</td>\n",
       "      <td>COMET2: Using annotated WMT data for pre-train...</td>\n",
       "      <td>comes2 is a variant of the metrics shared task...</td>\n",
       "      <td>COMES2 is a variant of the Metrics Shared Task...</td>\n",
       "      <td>How is COMES2 evaluated?</td>\n",
       "      <td>4.089975</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>In what way does COMET compare to human percep...</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>based on semantic similarities between the tra...</td>\n",
       "      <td>based on semantic similarities between the tra...</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>trained</td>\n",
       "      <td>input sequences</td>\n",
       "      <td>scores</td>\n",
       "      <td>semantic similarities</td>\n",
       "      <td>based on semantic similarities between the tra...</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>What kind of model does COMET use?</td>\n",
       "      <td>4.015011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>COMET is a trained method that learns to outpu...</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>Score</td>\n",
       "      <td>an output</td>\n",
       "      <td>score that resembles the human perception of t...</td>\n",
       "      <td>The human perception of translation quality. C...</td>\n",
       "      <td>scores</td>\n",
       "      <td>numerical value</td>\n",
       "      <td>translation quality</td>\n",
       "      <td>score</td>\n",
       "      <td>score that resembles the human perception of t...</td>\n",
       "      <td>score that resembles the human perception of t...</td>\n",
       "      <td>What does COMET learn to output?</td>\n",
       "      <td>4.006101</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>In this paper, we explore using the COMET eval...</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>This paper explores the usage of COMET for eva...</td>\n",
       "      <td>A variant of the model, COMET is trained on th...</td>\n",
       "      <td>The use COMET for text summarization systems.</td>\n",
       "      <td>The use COMET for text summarization</td>\n",
       "      <td>The use COMET for text summarization systems</td>\n",
       "      <td>we introduce a variant of the model</td>\n",
       "      <td>a variant of the model, comet is trained on th...</td>\n",
       "      <td>A variant of the model, COMET is trained on th...</td>\n",
       "      <td>What datasets are used to examine COMET's perf...</td>\n",
       "      <td>3.862159</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>Which machine translation metrics can be used ...</td>\n",
       "      <td>0.040325</td>\n",
       "      <td>A new training method for COMET is proposed to...</td>\n",
       "      <td>MT outputs can be used to evaluate text summar...</td>\n",
       "      <td>We introduce a variant of the model – COMET – ...</td>\n",
       "      <td>We introduce a variant of the model – COMET – ...</td>\n",
       "      <td>Comet</td>\n",
       "      <td>Human judgement</td>\n",
       "      <td>human judgments</td>\n",
       "      <td>summarization output quality</td>\n",
       "      <td>we introduce a variant of the model – comet – ...</td>\n",
       "      <td>We introduce a variant of the model – COMET – ...</td>\n",
       "      <td>What is COMET?</td>\n",
       "      <td>3.602739</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>Using multilingual MT outputs, we introduce a ...</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>We introduce a variant of the COMET model – CO...</td>\n",
       "      <td>COMET: A Neural Evaluation Model of Text Summa...</td>\n",
       "      <td>COMET: A Neural Evaluation Model of Text Summa...</td>\n",
       "      <td>The paper introduces a novel training method f...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>the comet model is trained on multilingual mt ...</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>What datasets are used to examine COMET's perf...</td>\n",
       "      <td>3.302905</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>The COMES2 model is a variant of the COMET mod...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>COMES2 is a variant of the COMET model that us...</td>\n",
       "      <td>A variant of the COMES2 model is proposed that...</td>\n",
       "      <td>Annotation-based summary evaluation metrics fo...</td>\n",
       "      <td>Computational Modeling for Summary Evaluation</td>\n",
       "      <td>Using annotated MT data to predict summary eva...</td>\n",
       "      <td>COMES2: Predicting summary evaluation metrics ...</td>\n",
       "      <td>predicting summary evaluation metrics using an...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>What model do we propose?</td>\n",
       "      <td>3.224199</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>What is the purpose of the COMET2 model?</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>To evaluate the quality of summary output of a...</td>\n",
       "      <td>Using annotated MT data for pre-training and p...</td>\n",
       "      <td>Using annotated MT data for pre-training and p...</td>\n",
       "      <td>A variant of the WMT model – COMES2 that uses ...</td>\n",
       "      <td>To evaluate summary quality</td>\n",
       "      <td>evaluate summary quality of generative models</td>\n",
       "      <td>evaluate summary quality</td>\n",
       "      <td>summary evaluation metrics</td>\n",
       "      <td>to evaluate the quality of summary output of a...</td>\n",
       "      <td>To evaluate the quality of summary output of a...</td>\n",
       "      <td>What is the purpose of metrics?</td>\n",
       "      <td>2.896609</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>How does COMET compare to the human perception...</td>\n",
       "      <td>0.081186</td>\n",
       "      <td>Learning to output scores resembles the human ...</td>\n",
       "      <td>An output score that is based on semantic simi...</td>\n",
       "      <td>An output score that is based on semantic simi...</td>\n",
       "      <td></td>\n",
       "      <td>outputs</td>\n",
       "      <td>that,</td>\n",
       "      <td>trained</td>\n",
       "      <td>score</td>\n",
       "      <td>an output score that is based on semantic simi...</td>\n",
       "      <td>An output score that is based on semantic simi...</td>\n",
       "      <td>What is COMET trained to output?</td>\n",
       "      <td>2.841806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>This paper presents an approach to automatical...</td>\n",
       "      <td>Using annotated MT data, we propose an automat...</td>\n",
       "      <td>A variant of the Metrics Shared Task model – C...</td>\n",
       "      <td>We propose a variant of the Metrics Shared Tas...</td>\n",
       "      <td>Computational Modeling for Summary Evaluation</td>\n",
       "      <td>Using annotated MT data to predict summary eva...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>COMES2: Predicting summary evaluation metrics ...</td>\n",
       "      <td>a variant of the metrics shared task model – c...</td>\n",
       "      <td>A variant of the Metrics Shared Task model – C...</td>\n",
       "      <td>What do we propose?</td>\n",
       "      <td>2.820311</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>What is the purpose of the COMET2 model? How i...</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>Predicting several aspects a quality score for...</td>\n",
       "      <td>To evaluate the quality of summary output from...</td>\n",
       "      <td>This paper presents a variant of the COMET mod...</td>\n",
       "      <td>This paper presents a variant of the COMET mod...</td>\n",
       "      <td>summary evaluation metrics</td>\n",
       "      <td>Predicting several aspects of summary quality</td>\n",
       "      <td>to measure progress during training and compar...</td>\n",
       "      <td>predicting several aspects of summary quality</td>\n",
       "      <td>to evaluate the quality of summary output from...</td>\n",
       "      <td>This paper presents a variant of the COMET mod...</td>\n",
       "      <td>What datasets are used to evaluate the COMET2 ...</td>\n",
       "      <td>2.503120</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>Why do you need to train your COMET model?</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>Multilingual language models are used in trans...</td>\n",
       "      <td>To use COMET, you need to train your model.</td>\n",
       "      <td>To use COMET, you need to train your model. Mu...</td>\n",
       "      <td>Pre-trained multilingual language models are u...</td>\n",
       "      <td>input sequence</td>\n",
       "      <td>interprets the output</td>\n",
       "      <td>pre-trained multilingual language model</td>\n",
       "      <td>human perception of translation quality</td>\n",
       "      <td>to use comet, you need to train your model. mu...</td>\n",
       "      <td>To use COMET, you need to train your model. Mu...</td>\n",
       "      <td>How is COMET trained?</td>\n",
       "      <td>2.296661</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>0.005796</td>\n",
       "      <td>This paper introduces a variant of the model –...</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>A variant of the COMET model – COMES – trained...</td>\n",
       "      <td>COMET: A Neural Evaluation Metric for Text Sum...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>An evaluation metric for text summarization sy...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>a variant of the comet model – comes – trained...</td>\n",
       "      <td>A variant of the COMET model – COMES – trained...</td>\n",
       "      <td>What is COMET?</td>\n",
       "      <td>1.652104</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>How can we use COMET to evaluate text summariz...</td>\n",
       "      <td>0.041881</td>\n",
       "      <td>Using the COMET model, we evaluate the quality...</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>This paper introduces a variant of the COMET m...</td>\n",
       "      <td>A variant of the model – COMET trained on the ...</td>\n",
       "      <td>Annotated summarization outputs that uses mult...</td>\n",
       "      <td>Annotated summarization outputs</td>\n",
       "      <td>The paper explores the usage of COMET for eval...</td>\n",
       "      <td>we explore the usage of COMET for text summari...</td>\n",
       "      <td>a variant of the model – comet trained on the ...</td>\n",
       "      <td>A variant of the model – COMET trained on the ...</td>\n",
       "      <td>What is COMET?</td>\n",
       "      <td>1.644758</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>Using annotated summarization outputs, we intr...</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>Using annotated summarization outputs, we intr...</td>\n",
       "      <td>This work presents a novel training method to ...</td>\n",
       "      <td>COMET: A Neural Evaluation Model for Text Summ...</td>\n",
       "      <td>COMET: A Neural Evaluation Model for Text Summ...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>using annotated summarization outputs, we intr...</td>\n",
       "      <td>Using annotated summarization outputs, we intr...</td>\n",
       "      <td>What datasets are used to evaluate COMET?</td>\n",
       "      <td>1.592180</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>what are some examples that use COMET?</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>CoMet scores were created by the University of...</td>\n",
       "      <td>Translation quality metrics are used to evalua...</td>\n",
       "      <td>An example of this is the translation of a Wik...</td>\n",
       "      <td>An example of this is the translation of a Wik...</td>\n",
       "      <td>Translate</td>\n",
       "      <td>Translation Quality</td>\n",
       "      <td>Google Translate</td>\n",
       "      <td>Machine translation</td>\n",
       "      <td>an example of this is the translation of a wik...</td>\n",
       "      <td>An example of this is the translation of a Wik...</td>\n",
       "      <td>What is an example of a COMET score?</td>\n",
       "      <td>1.102774</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  section_n  section_rank  \\\n",
       "0       6        0.0           1.0   \n",
       "1      18        2.0           1.0   \n",
       "2       7        1.0           1.0   \n",
       "3       9        1.0           1.0   \n",
       "4      14        1.0           1.0   \n",
       "5      12        1.0           1.0   \n",
       "6      19        2.0           1.0   \n",
       "7      17        2.0           1.0   \n",
       "8       0        0.0           1.0   \n",
       "9       4        0.0           1.0   \n",
       "10      3        0.0           1.0   \n",
       "11      8        1.0           1.0   \n",
       "12     13        1.0           1.0   \n",
       "13     20        2.0           1.0   \n",
       "14     11        1.0           1.0   \n",
       "15     10        1.0           1.0   \n",
       "16     16        2.0           1.0   \n",
       "17      1        0.0           1.0   \n",
       "18      5        0.0           1.0   \n",
       "19      2        0.0           1.0   \n",
       "20     15        2.0           1.0   \n",
       "\n",
       "                                                 text  \\\n",
       "0   COMET is a recently proposed trainable neuralb...   \n",
       "1    COMET is a trained metric that, based on sema...   \n",
       "2    Since manual annotation for any generative ta...   \n",
       "3    Since manual annotation for any generative ta...   \n",
       "4    Since manual annotation for any generative ta...   \n",
       "5    Since manual annotation for any generative ta...   \n",
       "6    COMET is a trained metric that, based on sema...   \n",
       "7    COMET is a trained metric that, based on sema...   \n",
       "8   COMET is a recently proposed trainable neuralb...   \n",
       "9   COMET is a recently proposed trainable neuralb...   \n",
       "10  COMET is a recently proposed trainable neuralb...   \n",
       "11   Since manual annotation for any generative ta...   \n",
       "12   Since manual annotation for any generative ta...   \n",
       "13   COMET is a trained metric that, based on sema...   \n",
       "14   Since manual annotation for any generative ta...   \n",
       "15   Since manual annotation for any generative ta...   \n",
       "16   COMET is a trained metric that, based on sema...   \n",
       "17  COMET is a recently proposed trainable neuralb...   \n",
       "18  COMET is a recently proposed trainable neuralb...   \n",
       "19  COMET is a recently proposed trainable neuralb...   \n",
       "20   COMET is a trained metric that, based on sema...   \n",
       "\n",
       "                                             question  question_ppl  \\\n",
       "0                  What is the purpose of this paper?      0.053599   \n",
       "1       The COMET score is calculated by what method?      0.002099   \n",
       "2   Annotation-based generative models: a new appr...      0.000009   \n",
       "3   Predicting summary evaluation metrics using an...      0.000261   \n",
       "4   How to evaluate the quality of a generative mo...      0.007374   \n",
       "5   Using annotated WMT data for pre-training and ...      0.000979   \n",
       "6   In what way does COMET compare to human percep...      0.006779   \n",
       "7   COMET is a trained method that learns to outpu...      0.000992   \n",
       "8   Using COMET to evaluate text summarization sys...      0.002084   \n",
       "9   Which machine translation metrics can be used ...      0.040325   \n",
       "10  A novel training method for evaluating text su...      0.018356   \n",
       "11  Predicting summary evaluation metrics using an...      0.000033   \n",
       "12           What is the purpose of the COMET2 model?      0.003426   \n",
       "13  How does COMET compare to the human perception...      0.081186   \n",
       "14  Predicting summary evaluation metrics using an...      0.000755   \n",
       "15  What is the purpose of the COMET2 model? How i...      0.000289   \n",
       "16         Why do you need to train your COMET model?      0.000634   \n",
       "17  Using COMET to evaluate text summarization sys...      0.005796   \n",
       "18  How can we use COMET to evaluate text summariz...      0.041881   \n",
       "19  A novel training method for evaluating text su...      0.007119   \n",
       "20             what are some examples that use COMET?      0.000057   \n",
       "\n",
       "                                             answer_1  \\\n",
       "0   To evaluate text summarization systems using C...   \n",
       "1   Based on semantic similarities between the tra...   \n",
       "2   Using annotated datasets, we propose an automa...   \n",
       "3   In this paper we propose an automatic method f...   \n",
       "4   This paper presents an approach to evaluate th...   \n",
       "5   This paper presents an approach to automatical...   \n",
       "6   based on semantic similarities between the tra...   \n",
       "7                                               Score   \n",
       "8   In this paper, we explore using the COMET eval...   \n",
       "9   A new training method for COMET is proposed to...   \n",
       "10  The COMET model is trained on multilingual MT ...   \n",
       "11  The COMES2 model is a variant of the COMET mod...   \n",
       "12  To evaluate the quality of summary output of a...   \n",
       "13  Learning to output scores resembles the human ...   \n",
       "14  This paper presents an approach to automatical...   \n",
       "15  Predicting several aspects a quality score for...   \n",
       "16  Multilingual language models are used in trans...   \n",
       "17  This paper introduces a variant of the model –...   \n",
       "18  Using the COMET model, we evaluate the quality...   \n",
       "19  Annotated summarization outputs are used for p...   \n",
       "20  CoMet scores were created by the University of...   \n",
       "\n",
       "                                             answer_2  \\\n",
       "0   Using COMET to evaluate text summarization sys...   \n",
       "1   semantic similarities between the translated a...   \n",
       "2   In this paper we propose an annotated version ...   \n",
       "3   In this paper we propose an automatic method f...   \n",
       "4   Using annotated datasets, we propose COMES2 to...   \n",
       "5   Using annotated WMT data for pre-training and ...   \n",
       "6   based on semantic similarities between the tra...   \n",
       "7                                           an output   \n",
       "8   Annotated summarization outputs are used for p...   \n",
       "9   MT outputs can be used to evaluate text summar...   \n",
       "10  Using multilingual MT outputs, we introduce a ...   \n",
       "11  Predicting summary evaluation metrics using an...   \n",
       "12  Using annotated MT data for pre-training and p...   \n",
       "13  An output score that is based on semantic simi...   \n",
       "14  Using annotated MT data, we propose an automat...   \n",
       "15  To evaluate the quality of summary output from...   \n",
       "16        To use COMET, you need to train your model.   \n",
       "17  Annotated summarization outputs are used for p...   \n",
       "18  Annotated summarization outputs are used for p...   \n",
       "19  Using annotated summarization outputs, we intr...   \n",
       "20  Translation quality metrics are used to evalua...   \n",
       "\n",
       "                                             answer_3  \\\n",
       "0   An evaluation model for text summarization sys...   \n",
       "1   The COMET score is calculated by a stack of fe...   \n",
       "2   In this paper we propose an annotated version ...   \n",
       "3   Using annotated MT data, we propose a variant ...   \n",
       "4   In this paper, we propose COMES2 – a variant o...   \n",
       "5   The COMET metric is a framework for evaluating...   \n",
       "6   Based on semantic similarities between the tra...   \n",
       "7   score that resembles the human perception of t...   \n",
       "8   This paper explores the usage of COMET for eva...   \n",
       "9   We introduce a variant of the model – COMET – ...   \n",
       "10  The COMET model is trained on multilingual MT ...   \n",
       "11  COMES2 is a variant of the COMET model that us...   \n",
       "12  Using annotated MT data for pre-training and p...   \n",
       "13  An output score that is based on semantic simi...   \n",
       "14  A variant of the Metrics Shared Task model – C...   \n",
       "15  This paper presents a variant of the COMET mod...   \n",
       "16  To use COMET, you need to train your model. Mu...   \n",
       "17  Using COMET to evaluate text summarization sys...   \n",
       "18  This paper introduces a variant of the COMET m...   \n",
       "19  The COMET model is trained on multilingual MT ...   \n",
       "20  An example of this is the translation of a Wik...   \n",
       "\n",
       "                                             answer_4  \\\n",
       "0   This paper introduces a variant of the COMET m...   \n",
       "1   pre-trained multilingual language model to ext...   \n",
       "2   COMES2 is a variant of the Metric Shared Task ...   \n",
       "3   We propose a variant of the model – COMES2 – t...   \n",
       "4   COMES2 is a variant of the model that uses the...   \n",
       "5   COMES2 is a variant of the Metrics Shared Task...   \n",
       "6   Based on semantic similarities between the tra...   \n",
       "7   The human perception of translation quality. C...   \n",
       "8   A variant of the model, COMET is trained on th...   \n",
       "9   We introduce a variant of the model – COMET – ...   \n",
       "10  We introduce a variant of the COMET model – CO...   \n",
       "11  A variant of the COMES2 model is proposed that...   \n",
       "12  A variant of the WMT model – COMES2 that uses ...   \n",
       "13                                                      \n",
       "14  We propose a variant of the Metrics Shared Tas...   \n",
       "15  This paper presents a variant of the COMET mod...   \n",
       "16  Pre-trained multilingual language models are u...   \n",
       "17  A variant of the COMET model – COMES – trained...   \n",
       "18  A variant of the model – COMET trained on the ...   \n",
       "19  Using annotated summarization outputs, we intr...   \n",
       "20  An example of this is the translation of a Wik...   \n",
       "\n",
       "                                       short_answer_1  \\\n",
       "0               to assess machine translation quality   \n",
       "1                        stack of feed-forward layers   \n",
       "2                an annotation-based generative model   \n",
       "3       Computational Modeling for Summary Evaluation   \n",
       "4                                              CoMES2   \n",
       "5   Computational Evaluation of Summary Models usi...   \n",
       "6                                             trained   \n",
       "7                                              scores   \n",
       "8       The use COMET for text summarization systems.   \n",
       "9                                               Comet   \n",
       "10  COMET: A Neural Evaluation Model of Text Summa...   \n",
       "11  Annotation-based summary evaluation metrics fo...   \n",
       "12                        To evaluate summary quality   \n",
       "13                                            outputs   \n",
       "14      Computational Modeling for Summary Evaluation   \n",
       "15                         summary evaluation metrics   \n",
       "16                                     input sequence   \n",
       "17  COMET: A Neural Evaluation Metric for Text Sum...   \n",
       "18  Annotated summarization outputs that uses mult...   \n",
       "19  This work presents a novel training method to ...   \n",
       "20                                          Translate   \n",
       "\n",
       "                                       short_answer_2  \\\n",
       "0         Use COMET for text summarization evaluation   \n",
       "1             human perception of translation quality   \n",
       "2   Using annotated dataset to evaluate quality of...   \n",
       "3   Predicting summary evaluation metrics using an...   \n",
       "4                        the use of the COMET metrics   \n",
       "5          Computational Evaluation of Summary Models   \n",
       "6                                     input sequences   \n",
       "7                                     numerical value   \n",
       "8                The use COMET for text summarization   \n",
       "9                                     Human judgement   \n",
       "10  COMET: A Neural Evaluation Model of Text Summa...   \n",
       "11      Computational Modeling for Summary Evaluation   \n",
       "12      evaluate summary quality of generative models   \n",
       "13                                              that,   \n",
       "14  Using annotated MT data to predict summary eva...   \n",
       "15      Predicting several aspects of summary quality   \n",
       "16                              interprets the output   \n",
       "17  Using COMET to evaluate text summarization sys...   \n",
       "18                    Annotated summarization outputs   \n",
       "19  COMET: A Neural Evaluation Model for Text Summ...   \n",
       "20                                Translation Quality   \n",
       "\n",
       "                                       short_answer_3  \\\n",
       "0                 evaluate text summarization systems   \n",
       "1             pre-trained multilingual language model   \n",
       "2   Computational Evaluation of Summary Quality of...   \n",
       "3   Using annotated MT data to predict summary eva...   \n",
       "4   we introduce COMET2 and evaluate its performan...   \n",
       "5   Using annotated WMT data for pre-training and ...   \n",
       "6                                              scores   \n",
       "7                                 translation quality   \n",
       "8        The use COMET for text summarization systems   \n",
       "9                                     human judgments   \n",
       "10  The paper introduces a novel training method f...   \n",
       "11  Using annotated MT data to predict summary eva...   \n",
       "12                           evaluate summary quality   \n",
       "13                                            trained   \n",
       "14  Predicting summary evaluation metrics using an...   \n",
       "15  to measure progress during training and compar...   \n",
       "16            pre-trained multilingual language model   \n",
       "17  An evaluation metric for text summarization sy...   \n",
       "18  The paper explores the usage of COMET for eval...   \n",
       "19  COMET: A Neural Evaluation Model for Text Summ...   \n",
       "20                                   Google Translate   \n",
       "\n",
       "                                       short_answer_4  \\\n",
       "0   To evaluate the quality of text summarization ...   \n",
       "1   semantic similarities between the translated a...   \n",
       "2   Annotation-based generative models: a new appr...   \n",
       "3   Predicting summary evaluation metrics using an...   \n",
       "4                   COMET metric by Rei et al. (2020)   \n",
       "5   COMET2: Using annotated WMT data for pre-train...   \n",
       "6                               semantic similarities   \n",
       "7                                               score   \n",
       "8                 we introduce a variant of the model   \n",
       "9                        summarization output quality   \n",
       "10  A novel training method for evaluating text su...   \n",
       "11  COMES2: Predicting summary evaluation metrics ...   \n",
       "12                         summary evaluation metrics   \n",
       "13                                              score   \n",
       "14  COMES2: Predicting summary evaluation metrics ...   \n",
       "15      predicting several aspects of summary quality   \n",
       "16            human perception of translation quality   \n",
       "17  Using COMET to evaluate text summarization sys...   \n",
       "18  we explore the usage of COMET for text summari...   \n",
       "19  A novel training method for evaluating text su...   \n",
       "20                                Machine translation   \n",
       "\n",
       "                               generated_selected_ans  \\\n",
       "0   to evaluate text summarization systems using c...   \n",
       "1   pre-trained multilingual language model to ext...   \n",
       "2   in this paper we propose an annotated version ...   \n",
       "3   in this paper we propose an automatic method f...   \n",
       "4   comes2 is a variant of the model that uses the...   \n",
       "5   comes2 is a variant of the metrics shared task...   \n",
       "6   based on semantic similarities between the tra...   \n",
       "7   score that resembles the human perception of t...   \n",
       "8   a variant of the model, comet is trained on th...   \n",
       "9   we introduce a variant of the model – comet – ...   \n",
       "10  the comet model is trained on multilingual mt ...   \n",
       "11  predicting summary evaluation metrics using an...   \n",
       "12  to evaluate the quality of summary output of a...   \n",
       "13  an output score that is based on semantic simi...   \n",
       "14  a variant of the metrics shared task model – c...   \n",
       "15  to evaluate the quality of summary output from...   \n",
       "16  to use comet, you need to train your model. mu...   \n",
       "17  a variant of the comet model – comes – trained...   \n",
       "18  a variant of the model – comet trained on the ...   \n",
       "19  using annotated summarization outputs, we intr...   \n",
       "20  an example of this is the translation of a wik...   \n",
       "\n",
       "                                         selected_ans  \\\n",
       "0   To evaluate text summarization systems using C...   \n",
       "1   pre-trained multilingual language model to ext...   \n",
       "2   In this paper we propose an annotated version ...   \n",
       "3   In this paper we propose an automatic method f...   \n",
       "4   COMES2 is a variant of the model that uses the...   \n",
       "5   COMES2 is a variant of the Metrics Shared Task...   \n",
       "6   Based on semantic similarities between the tra...   \n",
       "7   score that resembles the human perception of t...   \n",
       "8   A variant of the model, COMET is trained on th...   \n",
       "9   We introduce a variant of the model – COMET – ...   \n",
       "10  The COMET model is trained on multilingual MT ...   \n",
       "11  Predicting summary evaluation metrics using an...   \n",
       "12  To evaluate the quality of summary output of a...   \n",
       "13  An output score that is based on semantic simi...   \n",
       "14  A variant of the Metrics Shared Task model – C...   \n",
       "15  This paper presents a variant of the COMET mod...   \n",
       "16  To use COMET, you need to train your model. Mu...   \n",
       "17  A variant of the COMET model – COMES – trained...   \n",
       "18  A variant of the model – COMET trained on the ...   \n",
       "19  Using annotated summarization outputs, we intr...   \n",
       "20  An example of this is the translation of a Wik...   \n",
       "\n",
       "                                         new_question     RQUGE   take  \n",
       "0                  What is the purpose of this paper?  4.803611   True  \n",
       "1                                What does COMET use?  4.583428   True  \n",
       "2                         How is the model evaluated?  4.370208   True  \n",
       "3                  How is the COMES2 model evaluated?  4.240835   True  \n",
       "4                            How is COMES2 evaluated?  4.126691  False  \n",
       "5                            How is COMES2 evaluated?  4.089975  False  \n",
       "6                  What kind of model does COMET use?  4.015011  False  \n",
       "7                    What does COMET learn to output?  4.006101  False  \n",
       "8   What datasets are used to examine COMET's perf...  3.862159   True  \n",
       "9                                      What is COMET?  3.602739  False  \n",
       "10  What datasets are used to examine COMET's perf...  3.302905  False  \n",
       "11                          What model do we propose?  3.224199   True  \n",
       "12                    What is the purpose of metrics?  2.896609   True  \n",
       "13                   What is COMET trained to output?  2.841806  False  \n",
       "14                                What do we propose?  2.820311   True  \n",
       "15  What datasets are used to evaluate the COMET2 ...  2.503120  False  \n",
       "16                              How is COMET trained?  2.296661  False  \n",
       "17                                     What is COMET?  1.652104  False  \n",
       "18                                     What is COMET?  1.644758  False  \n",
       "19          What datasets are used to evaluate COMET?  1.592180  False  \n",
       "20               What is an example of a COMET score?  1.102774   True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from MCQ import flanT5MCQ\n",
    "from datetime import datetime\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "dir_path = '../outputs/'+datetime.now().strftime(\"%d_%m_%y\")+'_'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "sections = sections[:sections_num_max]\n",
    "print(f'Limit questions to be from the {sections_num_max} first sections')\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
    "'''\n",
    "import pandas as pd\n",
    "questions_df = pd.read_pickle('/home/ubuntu/Questions_generation/outputs/03_01_23_finance_course1/GQ+QA+GQ.pkl')\n",
    "\n",
    "questions_used = 'new_question'\n",
    "questions_df['RQUGE'] = questions_df.apply(lambda x: mcq.rquge.scorer(x.text, x[questions_used], x.selected_ans)[0]['pred_score'] ,axis='columns')\n",
    "questions_df = questions_df.sort_values('RQUGE',ascending=False).reset_index()\n",
    "q_sim_mat,ans_sim_mat = mcq.find_similarity(questions_df[questions_used].to_list(),\n",
    "                                            answers = questions_df['selected_ans'].to_list())\n",
    "filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                  q_sim_mat=q_sim_mat,ans_sim_mat=ans_sim_mat,\n",
    "                                 similarity_thrs=0.75, n_thrs=20, return_index=True)\n",
    "questions_df['use_question']=False\n",
    "questions_df.loc[filter_idx,'use_question']=True\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_n</th>\n",
       "      <th>section_n_chunk</th>\n",
       "      <th>section_rank</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>question_ppl</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>short_answer_1</th>\n",
       "      <th>short_answer_2</th>\n",
       "      <th>short_answer_3</th>\n",
       "      <th>short_answer_4</th>\n",
       "      <th>generated_selected_ans</th>\n",
       "      <th>selected_ans</th>\n",
       "      <th>new_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Economics 252 is a course for undergraduates....</td>\n",
       "      <td>Which course is being filmed for open yale? ec...</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>Financial markets</td>\n",
       "      <td>Economic markets</td>\n",
       "      <td>A lecturer for the course economics 252 is bei...</td>\n",
       "      <td>A lecturer for the course economics 252 is bei...</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Economic markets</td>\n",
       "      <td>Financial markets</td>\n",
       "      <td>Economics 252</td>\n",
       "      <td>a lecturer for the course economics 252 is bei...</td>\n",
       "      <td>A lecturer for the course economics 252 is bei...</td>\n",
       "      <td>Who is being filmed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Economics 252 is a course for undergraduates....</td>\n",
       "      <td>When did yale university first offer this course?</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>Three years ago. the first time this course ha...</td>\n",
       "      <td>It was filmed for open yale in 2008.</td>\n",
       "      <td>In 2008, three years ago. the first time this ...</td>\n",
       "      <td>The first time this course has been filmed for...</td>\n",
       "      <td>2009</td>\n",
       "      <td>2007</td>\n",
       "      <td>In 2008</td>\n",
       "      <td>Three years ago</td>\n",
       "      <td>in 2008, three years ago. the first time this ...</td>\n",
       "      <td>The first time this course has been filmed for...</td>\n",
       "      <td>When was the last time this course was filmed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Economics 252 is a course for undergraduates....</td>\n",
       "      <td>Where will you find the course?</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>You'll find it online as part of open yale.</td>\n",
       "      <td>You'll find it online as part of open yale. it...</td>\n",
       "      <td>You'll find it online as part of open yale. it...</td>\n",
       "      <td>The course will be eventually posted on the in...</td>\n",
       "      <td>Online</td>\n",
       "      <td>Open yale</td>\n",
       "      <td>Yale university</td>\n",
       "      <td>On the internet</td>\n",
       "      <td>you'll find it online as part of open yale. it...</td>\n",
       "      <td>You'll find it online as part of open yale. it...</td>\n",
       "      <td>Where can you take Economics 252?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Economics 252 is a course for undergraduates....</td>\n",
       "      <td>In what year was this lecture recorded for ope...</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>2008</td>\n",
       "      <td>It was recorded for open yale in 2008</td>\n",
       "      <td>2008, three years ago. the first time this cou...</td>\n",
       "      <td>This lecture was recorded for open yale in 2008</td>\n",
       "      <td></td>\n",
       "      <td>2008</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>When was the first time the course was filmed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Economics 252 is a course for undergraduates....</td>\n",
       "      <td>Who was filmed for open yale?</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>A lecturer</td>\n",
       "      <td>A lecturer for economics 252</td>\n",
       "      <td>Economics 252 instructor</td>\n",
       "      <td>A lecturer for economics 252 at yale university.</td>\n",
       "      <td>Economists</td>\n",
       "      <td>Professor</td>\n",
       "      <td>Economics 252</td>\n",
       "      <td>The speaker</td>\n",
       "      <td>a lecturer for economics 252 at yale university.</td>\n",
       "      <td>A lecturer for economics 252 at yale university.</td>\n",
       "      <td>Who is he?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We're seeing a change in bank regulation that ...</td>\n",
       "      <td>Lecture 13 focuses on what type of contracts?</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>Derivative contracts</td>\n",
       "      <td>Forward and futures markets</td>\n",
       "      <td>Derivative contracts that inject a lot of comp...</td>\n",
       "      <td>Derivative contracts that inject a lot of comp...</td>\n",
       "      <td>Future</td>\n",
       "      <td>Forward</td>\n",
       "      <td>Future markets</td>\n",
       "      <td>Stock options</td>\n",
       "      <td>derivative contracts that inject a lot of comp...</td>\n",
       "      <td>Derivative contracts that inject a lot of comp...</td>\n",
       "      <td>What are stock options?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We're seeing a change in bank regulation that ...</td>\n",
       "      <td>Who manages portfolios?</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>Students in lecture 17 are learning about prof...</td>\n",
       "      <td>You don't have to be a billionaire to manage p...</td>\n",
       "      <td>David swensen is a professional money manager....</td>\n",
       "      <td>David swensen is a professional money manager.</td>\n",
       "      <td>A billionaire</td>\n",
       "      <td>Professionals</td>\n",
       "      <td>Money managers</td>\n",
       "      <td>Professional money managers</td>\n",
       "      <td>david swensen is a professional money manager....</td>\n",
       "      <td>David swensen is a professional money manager....</td>\n",
       "      <td>What did David Swensen say?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We're seeing a change in bank regulation that ...</td>\n",
       "      <td>Which lecture is about forwards and futures ma...</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>Lecture13</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>The first lecture is lecture 13.</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>Lecture 13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>What lecture is about forwards and futures mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We're seeing a change in bank regulation that ...</td>\n",
       "      <td>What is the last lecture about?</td>\n",
       "      <td>0.067513</td>\n",
       "      <td>Financial markets and the financial industry.</td>\n",
       "      <td>Financial markets and the financial industry</td>\n",
       "      <td>The last lecture is about the revolution in fi...</td>\n",
       "      <td>This is a course not about making money</td>\n",
       "      <td>Moral purpose</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Financial ethics</td>\n",
       "      <td>A moral purpose</td>\n",
       "      <td>this is a course not about making money</td>\n",
       "      <td>This is a course not about making money</td>\n",
       "      <td>What is the last lecture about?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We're seeing a change in bank regulation that ...</td>\n",
       "      <td>How are forwards and futures markets different...</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>This is the last lecture of the course.</td>\n",
       "      <td>A derivative contract that inject a lot of com...</td>\n",
       "      <td>The speaker explains that forwards and futures...</td>\n",
       "      <td>Forwards and futures markets are most typicall...</td>\n",
       "      <td>A derivative contract</td>\n",
       "      <td>Forwards and futures markets are most typicall...</td>\n",
       "      <td>Price that goes up and down</td>\n",
       "      <td>Have a price that goes up and down</td>\n",
       "      <td>forwards and futures markets are most typicall...</td>\n",
       "      <td>Forwards and futures markets are most typicall...</td>\n",
       "      <td>What are the derivative contracts?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    section_n  section_n_chunk  section_rank  \\\n",
       "0           0              0.0           1.0   \n",
       "1           0              0.0           1.0   \n",
       "2           0              0.0           1.0   \n",
       "3           0              0.0           1.0   \n",
       "4           0              0.0           1.0   \n",
       "..        ...              ...           ...   \n",
       "69          5              5.2           1.0   \n",
       "70          5              5.2           1.0   \n",
       "71          5              5.2           1.0   \n",
       "72          5              5.2           1.0   \n",
       "73          5              5.2           1.0   \n",
       "\n",
       "                                                 text  \\\n",
       "0    Economics 252 is a course for undergraduates....   \n",
       "1    Economics 252 is a course for undergraduates....   \n",
       "2    Economics 252 is a course for undergraduates....   \n",
       "3    Economics 252 is a course for undergraduates....   \n",
       "4    Economics 252 is a course for undergraduates....   \n",
       "..                                                ...   \n",
       "69  We're seeing a change in bank regulation that ...   \n",
       "70  We're seeing a change in bank regulation that ...   \n",
       "71  We're seeing a change in bank regulation that ...   \n",
       "72  We're seeing a change in bank regulation that ...   \n",
       "73  We're seeing a change in bank regulation that ...   \n",
       "\n",
       "                                             question  question_ppl  \\\n",
       "0   Which course is being filmed for open yale? ec...      0.000339   \n",
       "1   When did yale university first offer this course?      0.001526   \n",
       "2                     Where will you find the course?      0.003963   \n",
       "3   In what year was this lecture recorded for ope...      0.007434   \n",
       "4                       Who was filmed for open yale?      0.007655   \n",
       "..                                                ...           ...   \n",
       "69      Lecture 13 focuses on what type of contracts?      0.006115   \n",
       "70                            Who manages portfolios?      0.006300   \n",
       "71  Which lecture is about forwards and futures ma...      0.036790   \n",
       "72                    What is the last lecture about?      0.067513   \n",
       "73  How are forwards and futures markets different...      0.081761   \n",
       "\n",
       "                                             answer_1  \\\n",
       "0                                   Financial markets   \n",
       "1   Three years ago. the first time this course ha...   \n",
       "2         You'll find it online as part of open yale.   \n",
       "3                                                2008   \n",
       "4                                          A lecturer   \n",
       "..                                                ...   \n",
       "69                               Derivative contracts   \n",
       "70  Students in lecture 17 are learning about prof...   \n",
       "71                                          Lecture13   \n",
       "72      Financial markets and the financial industry.   \n",
       "73            This is the last lecture of the course.   \n",
       "\n",
       "                                             answer_2  \\\n",
       "0                                    Economic markets   \n",
       "1                It was filmed for open yale in 2008.   \n",
       "2   You'll find it online as part of open yale. it...   \n",
       "3               It was recorded for open yale in 2008   \n",
       "4                        A lecturer for economics 252   \n",
       "..                                                ...   \n",
       "69                        Forward and futures markets   \n",
       "70  You don't have to be a billionaire to manage p...   \n",
       "71                                                 15   \n",
       "72       Financial markets and the financial industry   \n",
       "73  A derivative contract that inject a lot of com...   \n",
       "\n",
       "                                             answer_3  \\\n",
       "0   A lecturer for the course economics 252 is bei...   \n",
       "1   In 2008, three years ago. the first time this ...   \n",
       "2   You'll find it online as part of open yale. it...   \n",
       "3   2008, three years ago. the first time this cou...   \n",
       "4                            Economics 252 instructor   \n",
       "..                                                ...   \n",
       "69  Derivative contracts that inject a lot of comp...   \n",
       "70  David swensen is a professional money manager....   \n",
       "71                                                 13   \n",
       "72  The last lecture is about the revolution in fi...   \n",
       "73  The speaker explains that forwards and futures...   \n",
       "\n",
       "                                             answer_4         short_answer_1  \\\n",
       "0   A lecturer for the course economics 252 is bei...                Finance   \n",
       "1   The first time this course has been filmed for...                   2009   \n",
       "2   The course will be eventually posted on the in...                 Online   \n",
       "3     This lecture was recorded for open yale in 2008                          \n",
       "4    A lecturer for economics 252 at yale university.             Economists   \n",
       "..                                                ...                    ...   \n",
       "69  Derivative contracts that inject a lot of comp...                 Future   \n",
       "70     David swensen is a professional money manager.          A billionaire   \n",
       "71                   The first lecture is lecture 13.                          \n",
       "72            This is a course not about making money          Moral purpose   \n",
       "73  Forwards and futures markets are most typicall...  A derivative contract   \n",
       "\n",
       "                                       short_answer_2  \\\n",
       "0                                    Economic markets   \n",
       "1                                                2007   \n",
       "2                                           Open yale   \n",
       "3                                                2008   \n",
       "4                                           Professor   \n",
       "..                                                ...   \n",
       "69                                            Forward   \n",
       "70                                      Professionals   \n",
       "71                                                 15   \n",
       "72                                            Finance   \n",
       "73  Forwards and futures markets are most typicall...   \n",
       "\n",
       "                 short_answer_3                      short_answer_4  \\\n",
       "0             Financial markets                       Economics 252   \n",
       "1                       In 2008                     Three years ago   \n",
       "2               Yale university                     On the internet   \n",
       "3                                                                     \n",
       "4                 Economics 252                         The speaker   \n",
       "..                          ...                                 ...   \n",
       "69               Future markets                       Stock options   \n",
       "70               Money managers         Professional money managers   \n",
       "71                           13                          Lecture 13   \n",
       "72             Financial ethics                     A moral purpose   \n",
       "73  Price that goes up and down  Have a price that goes up and down   \n",
       "\n",
       "                               generated_selected_ans  \\\n",
       "0   a lecturer for the course economics 252 is bei...   \n",
       "1   in 2008, three years ago. the first time this ...   \n",
       "2   you'll find it online as part of open yale. it...   \n",
       "3                                                2008   \n",
       "4    a lecturer for economics 252 at yale university.   \n",
       "..                                                ...   \n",
       "69  derivative contracts that inject a lot of comp...   \n",
       "70  david swensen is a professional money manager....   \n",
       "71                                                 13   \n",
       "72            this is a course not about making money   \n",
       "73  forwards and futures markets are most typicall...   \n",
       "\n",
       "                                         selected_ans  \\\n",
       "0   A lecturer for the course economics 252 is bei...   \n",
       "1   The first time this course has been filmed for...   \n",
       "2   You'll find it online as part of open yale. it...   \n",
       "3                                                2008   \n",
       "4    A lecturer for economics 252 at yale university.   \n",
       "..                                                ...   \n",
       "69  Derivative contracts that inject a lot of comp...   \n",
       "70  David swensen is a professional money manager....   \n",
       "71                                                 13   \n",
       "72            This is a course not about making money   \n",
       "73  Forwards and futures markets are most typicall...   \n",
       "\n",
       "                                         new_question  \n",
       "0                                Who is being filmed?  \n",
       "1      When was the last time this course was filmed?  \n",
       "2                   Where can you take Economics 252?  \n",
       "3      When was the first time the course was filmed?  \n",
       "4                                          Who is he?  \n",
       "..                                                ...  \n",
       "69                            What are stock options?  \n",
       "70                        What did David Swensen say?  \n",
       "71  What lecture is about forwards and futures mar...  \n",
       "72                    What is the last lecture about?  \n",
       "73                 What are the derivative contracts?  \n",
       "\n",
       "[74 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "questions_df = pd.read_pickle('/home/ubuntu/Questions_generation/outputs/03_01_23_finance_course1/GQ+QA+GQ.pkl')\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# initialize model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mdel\u001b[39;00m model\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m TransformersQG(language\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlmqg/t5-large-squad-qg-ae\u001b[39;49m\u001b[39m'\u001b[39;49m,device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# paragraph to generate pairs of question and answer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#context = \"William Turner was an English painter who specialised in watercolour landscapes. He is often known as William Turner of Oxford or just Turner of Oxford to distinguish him from his contemporary, J. M. W. Turner. Many of Turner's paintings depicted the countryside around Oxford. One of his best known pictures is a view of the city of Oxford from Hinksey Hill.\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# model prediction\u001b[39;00m\n\u001b[1;32m     10\u001b[0m question_answer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate_qa(sections[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_39270/3981189669.py\u001b[0m(6)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0;31m\u001b[0;31m# initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m\u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformersQG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lmqg/t5-large-squad-qg-ae'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0;31m# paragraph to generate pairs of question and answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0;31m#context = \"William Turner was an English painter who specialised in watercolour landscapes. He is often known as William Turner of Oxford or just Turner of Oxford to distinguish him from his contemporary, J. M. W. Turner. Many of Turner's paintings depicted the countryside around Oxford. One of his best known pictures is a view of the city of Oxford from Hinksey Hill.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install pytextrank\n",
    "#!pip install lmqg\n",
    "from lmqg import TransformersQG\n",
    "# initialize model\n",
    "del model\n",
    "model = TransformersQG(language='en', model='lmqg/t5-large-squad-qg-ae')\n",
    "# paragraph to generate pairs of question and answer\n",
    "#context = \"William Turner was an English painter who specialised in watercolour landscapes. He is often known as William Turner of Oxford or just Turner of Oxford to distinguish him from his contemporary, J. M. W. Turner. Many of Turner's paintings depicted the countryside around Oxford. One of his best known pictures is a view of the city of Oxford from Hinksey Hill.\"\n",
    "# model prediction\n",
    "question_answer = model.generate_qa(sections[0])\n",
    "# the output is a list of tuple (question, answer)\n",
    "print(question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 10, 13, 14, 15, 21, 22]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                 mcq.find_similarity(questions_df[questions_used].to_list()),\n",
    "                                 similarity_thrs=0.75, n_thrs=20, return_index=True)\n",
    "filter_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'name', 'neg_termset', 'ent_types', 'extension_name', and 'chunk_prefix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_sci_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m'\u001b[39m\u001b[39msentencizer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(Negex(nlp))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'name', 'neg_termset', 'ent_types', 'extension_name', and 'chunk_prefix'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_3955/103320592.py\u001b[0m(5)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mfrom\u001b[0m \u001b[0mnegspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNegex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_sci_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentencizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 5 \u001b[0;31m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNegex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from negspacy.negation import Negex\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe(Negex(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'has_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m MultiLabel_TextCategorizer\n\u001b[1;32m     16\u001b[0m textcat \u001b[39m=\u001b[39m MultiLabel_TextCategorizer(nlp\u001b[39m.\u001b[39mvocab, DEFAULT_MULTI_TEXTCAT_MODEL, threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m textcat\u001b[39m.\u001b[39;49madd_label(\u001b[39m'\u001b[39;49m\u001b[39mchemistry\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m [textcat\u001b[39m.\u001b[39madd_label(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m labels_list]\n\u001b[1;32m     19\u001b[0m textcat\u001b[39m.\u001b[39mlabels\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/textcat.py:340\u001b[0m, in \u001b[0;36mTextCategorizer.add_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels:\n\u001b[1;32m    339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_allow_extra_label()\n\u001b[1;32m    341\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(label)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mresize_output\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mattrs:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:218\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe._allow_extra_label\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'has_dim'"
     ]
    }
   ],
   "source": [
    "# Construction via add_pipe with default model\n",
    "# Use 'textcat_multilabel' for multi-label classification\n",
    "from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.pipeline import MultiLabel_TextCategorizer\n",
    "#textcat = nlp.add_pipe(\"textcat\")\n",
    "labels_list = ['Physics',\t'Chemistry'\t,'Astronomic Physics',\t'Psychology',\t'Business',\t'Mathematics',\t'History',\t'Sociology',\t'Economics',\t'Political Science',\t'Geology'\t,'Philosophy',\t'Art',\t'Materials Science',\t'Geography',\t'Environmental Science',\t'Computer Science',\t'Engineering',\t'Biology'\t,'Medical Science']\n",
    "\n",
    "# Construction via add_pipe with custom model\n",
    "config = {\"model\": {\"@architectures\": \"my_textcat\"}}\n",
    "#parser = nlp.add_pipe(\"textcat\", config=config)\n",
    "\n",
    "# Construction from class\n",
    "# Use 'MultiLabel_TextCategorizer' for multi-label classification\n",
    "from spacy.pipeline import MultiLabel_TextCategorizer\n",
    "textcat = MultiLabel_TextCategorizer(nlp.vocab, DEFAULT_MULTI_TEXTCAT_MODEL, threshold=0.5)\n",
    "textcat.add_label('chemistry')\n",
    "[textcat.add_label(i) for i in labels_list]\n",
    "textcat.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/spacy/util.py:885: UserWarning: [W094] Model 'en_core_web_sm' (2.2.0) specifies an under-constrained spaCy version requirement: >=2.2.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.4.3,<3.5.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /home/ubuntu/.local/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnegspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnegation\u001b[39;00m \u001b[39mimport\u001b[39;00m Negex\n\u001b[1;32m      3\u001b[0m \u001b[39m#!pip install spacy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#!python3 -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m\"\u001b[39m\u001b[39mnegex\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39ment_types\u001b[39m\u001b[39m\"\u001b[39m:[\u001b[39m\"\u001b[39m\u001b[39mPERSON\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mORG\u001b[39m\u001b[39m\"\u001b[39m]})\n\u001b[1;32m      8\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39m\u001b[39mShe does not like Steve Jobs but likes Apple products.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     55\u001b[0m         name,\n\u001b[1;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:432\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:468\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[1;32m    453\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 468\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/en_core_web_sm/__init__.py:12\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:649\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    648\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    650\u001b[0m     data_path,\n\u001b[1;32m    651\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    652\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    653\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    654\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    655\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    656\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    657\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:505\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    503\u001b[0m config_path \u001b[39m=\u001b[39m model_path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconfig.cfg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m--> 505\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39;49moverrides)\n\u001b[1;32m    506\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    507\u001b[0m     config,\n\u001b[1;32m    508\u001b[0m     vocab\u001b[39m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     meta\u001b[39m=\u001b[39mmeta,\n\u001b[1;32m    513\u001b[0m )\n\u001b[1;32m    514\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:681\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config_path \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m config_path\u001b[39m.\u001b[39mis_file():\n\u001b[0;32m--> 681\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE053\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mconfig_path, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig file\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    682\u001b[0m     \u001b[39mreturn\u001b[39;00m config\u001b[39m.\u001b[39mfrom_disk(\n\u001b[1;32m    683\u001b[0m         config_path, overrides\u001b[39m=\u001b[39moverrides, interpolate\u001b[39m=\u001b[39minterpolate\n\u001b[1;32m    684\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config file from /home/ubuntu/.local/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from negspacy.negation import Negex\n",
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"negex\", config={\"ent_types\":[\"PERSON\",\"ORG\"]})\n",
    "\n",
    "doc = nlp(\"She does not like Steve Jobs but likes Apple products.\")\n",
    "for e in doc.ents:\n",
    "    print(e.text, e._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy-3.0.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.64.1)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.9)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "\u001b[K     |████████████████████████████████| 671 kB 72.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 75.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (22.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (45.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.22.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.4.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.0.0-py3-none-any.whl size=13704312 sha256=9005cfa5fcba0510ddcb2d3635fcb34905ac67596c410e72d67b8cc361e6d053\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/8b/21/c1/257748af7399fdaf1b2afc39c92fb839c436f42e67b656ff7e\n",
      "Successfully built en-core-web-sm\n",
      "\u001b[31mERROR: scispacy 0.5.1 has requirement spacy<3.5.0,>=3.4.0, but you'll have spacy 3.0.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: en-core-sci-sm 0.5.1 has requirement spacy<3.5.0,>=3.4.1, but you'll have spacy 3.0.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: click, typer, pydantic, thinc, spacy, en-core-web-sm\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.7.0\n",
      "    Uninstalling typer-0.7.0:\n",
      "      Successfully uninstalled typer-0.7.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.5\n",
      "    Uninstalling thinc-8.1.5:\n",
      "      Successfully uninstalled thinc-8.1.5\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.3\n",
      "    Uninstalling spacy-3.4.3:\n",
      "      Successfully uninstalled spacy-3.4.3\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "Successfully installed click-7.1.2 en-core-web-sm-3.0.0 pydantic-1.8.2 spacy-3.0.9 thinc-8.0.17 typer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Aren’t you coming? Doesn’t he understand? Are you not coming? Does he not understand? Which of the following did not occur? Does Jeff is a real person? Name something David hadn't reveal in his vacation?\")\n",
    "any([token.dep_=='neg' for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rank-one Model Editing is the best\n",
      "The Rank-one Model Editing is the best ROME\n",
      "The Rank-one Model Editing is the best The ROME is the best in Rome\n",
      "The Rank-one Model Editing is the best The ROME is the best in Rome\n",
      "Rank-one Model Editing method for factual knowledge storage\n",
      "Rank-one Model Editing. method for factual knowledge storage\n",
      "Rank-one Model Editing method for factual knowledge storage\n",
      "ROMEgt method for factual knowledge storage\n"
     ]
    }
   ],
   "source": [
    "abrv_dict = {'ROME': 'Rank-one Model Editing', 'TE': 'Total Effect', 'IE': 'Indirect Effect', 'ATE': 'Average Total Effect', 'AIE': 'Average Indirect Effect', 'FT': 'Fine-tuning', 'KE': 'Knowledge Editor', 'ES': 'Efficacy Score', 'EM': 'Efficacy Magnitude', 'PS': 'Paraphrase Scores', 'NS': 'Neighborhood Score', 'KN': 'Knowledge Neurons'}\n",
    "import re\n",
    "def rep(text):\n",
    "    target =lambda abrv,long_form_abrv: f'{long_form_abrv}'\n",
    "    #`only_first` resolve only the first abbreviation in the text (e.g. if the text includes the results section, and the abbreviation was define at the introduction, the method will resolve once the abbreviation in the text the model is exposed to)\n",
    "    for abrv,long_form_abbr in abrv_dict.items():\n",
    "        text = text.replace(f'{long_form_abbr} ({abrv})',abrv) #first the function abbreviate all occurrences\n",
    "        text = text.replace(f'{long_form_abbr}({abrv})',abrv) \n",
    "        #text = text.replace(long_form_abbr,abrv) \n",
    "        text = re.sub(re.compile(r''+long_form_abbr, re.IGNORECASE),abrv , text)\n",
    "        pattern = re.compile(r'(^|\\s|\\.)('+abrv+')(\\W)', re.IGNORECASE)\n",
    "        text = re.sub(pattern,r\"\\1\"+target(abrv,long_form_abbr)+ r'\\3' ,  text, count=1)\n",
    "    return text.strip() #.replace(target(abrv,long_form_abbr)+' ',target(abrv,long_form_abbr))\n",
    "\n",
    "print(rep('The Rome is the best'))\n",
    "print(rep('The ROME is the best ROME'))\n",
    "print(rep('The rome is the best The Rank-one Model Editing is the best in Rome'))\n",
    "print(rep('The rome is the best The rank-one Model Editing is the best in Rome'))\n",
    "print(rep('Rank-one Model Editing (ROME) method for factual knowledge storage'))\n",
    "print(rep('Rank-one Model Editing (ROME). method for factual knowledge storage'))\n",
    "print(rep('Rank-one Model Editing (ROME) method for factual knowledge storage'))\n",
    "print(rep('ROMEgt method for factual knowledge storage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "from MCQ import flanT5MCQ\n",
    "\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def is_similar(string, patterns):\n",
    "    # compile the regex pattern to remove spaces and punctuation marks\n",
    "    string = string.replace('_','')\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    # remove spaces and punctuation marks from the input string and the patterns\n",
    "    string = regex.sub('', string)\n",
    "    patterns = [regex.sub('', p) for p in patterns]\n",
    "\n",
    "    # check if the input string is exactly the same as one of the patterns\n",
    "    return string.lower().strip() in patterns\n",
    "answers_black_list =['we','they','the authors','authors','author','the author','you','you are','we are',\n",
    "                                'the speaker','speaker','the lecturer','lecturer',\n",
    "                                'he','he is','she','she is',\n",
    "                                  'I','these','those','they are','we do','it','is is']\n",
    "inputs = ['we','We','we...','They!','  Go','THEY  _ ']\n",
    "[is_similar(ak,answers_black_list) for ak in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n",
      "244\n",
      "963\n",
      "534\n",
      "1167\n",
      "664\n",
      "====================================================================================================\n",
      "376\n",
      "244\n",
      "512\n",
      "452\n",
      "473\n",
      "62\n",
      "510\n",
      "485\n",
      "174\n",
      "489\n",
      "176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\" The crisis began with bubbles in the stock market, housing market, and also in the commodities market. It's a financial crisis that's bigger than any since the Great Depression of the 1930's. There's many different ways of thinking about a crisis like this. And I wanted to focus on one way that people think about it in terms of probability models. So, that's not the only way, it's not necessarily my favorite way. Excuse my cold. I didn't bring any water. I hope I make it through this lecture.  There was a pre-break around 2000 when the stock market collapsed around the world. But then they came back again after 2003 and they were on another boom, like a roller coaster ride. That's the narrative story. And then, what happened is, we see a bunch of institutional collapses. We saw bank failures in the U.S. and then, we saw international cooperation to prevent this from spreading like a disease. So, we had governments all over the world bailing out their banks and other companies. That's what financial theorists will think about is that actually it's not just those few big events. It's the accumulation of a lot of little events.  I'm going to talk today about probability, variance, and covariance, and regression, and idiosyncratic risk, and systematic risk. But I'm also going to, in the context of the crisis, emphasize in this lecture, breakdowns of some of the most popular assumptions that underlie financial theory. And I'm thinking particularly of two breakdowns. One is the failure of independence. And another one is a tendency for outliers or fat-tailed distributions.\"],\n",
       " [\" The word probability in its present meaning wasn't even coined until the 1600's. We do it by dealing with all of these little incremental shocks that affect our lives in a mathematical way. We have mathematical laws of how they accumulate. And once we understand those laws, we can we can build mathematical models of the outcomes. And then we can ask whether we should be surprised by the financial events that we've seen. It's a little bit like science, real hard science. So, for example, weather forecasters build models that are built on the theory of fluid dynamics.  People who are steeped in this tradition in finance think that what we're doing is very much like what we do when we do financial forecasts. We have a statistical model, we see all of the shocks coming in, and of course there will be hurricanes. And we can only forecast them -- you know there's a limit to how far out we can forecast them. Weather forecasters can't do that. Same thing with financial crises. We understand the probability laws, there's only a certain time horizon before which we can.\"],\n",
       " [\"In finance, the basic, the most basic concept that -- in finance -- is that when you invest in something, you have to do it for a time interval. And so, what is your return to investing in something? It's the increase in the price. That's p t plus 1, minus p t. Returns can be positive or negative. They can never be more than -- never be less than minus 100%. In a limited liability economy that we live in, the law says that you cannot lose more than the money you put in. This is the mathematical expectation of a random variable x, which could be the return, or the gross return, but we're going to substitute something else. The expectation of x is the weighted sum of all possible values of x weighted by their probabilities. And the probabilities have to sum to 1. They're positive numbers, or zero, reflecting the likelihood of that random variable occurring, of that value of the random variable. This is for a discrete random variable that takes on only a finite, only a countable number of values. Gross return is always positive. It's between zero and infinity. If you have n observations on a random variable x, you can take the sum of the x observations, summation over i equals 1 to n, and then divide that by n. That's called the average, or the mean, or sample mean, when you have a sample of n observations, which is an estimate of the expected value of x. This is called the mean or average, which you've learned long ago, OK. So, for example, if we're evaluating an investor who has invested money, you could get n observations and take an average of them. The geometric mean makes sense only when all the x's are non-negative. If you put in a negative value, you might get a negative product, and then, if you took the nth root of that, it could be an imaginary number, so let's forget that. We're not going to apply this formula if there are any negative numbers. But it's often used, and I recommend its use, in evaluating investments. Because if you use gross return, it gives a better measure of the outcome of the investments.\"],\n",
       " [\"If there's ever a year in which the return is minus 100%, then the geometric mean is 0. That's a good discipline. This obviously doesn't make sense as a way to evaluate investment success. But we care about more than just about central tendency when evaluating risk. We have to do other things as well, including the geometric return, variance, variance and variance. And so, you want to talk about risk, this is very fundamental to finance. What could be more fundamental than risk for finance? If x tends to be plus or minus 1% from the mean return, the variance would probably be 1. The standard deviation is the square root of the variance. Covariance is a measure of how two different random variables move together. When IBM goes up, does General Motors go up or not? We're getting through these concepts, but I'm not going to get into these ideas here, so I'm just trying to be very basic and simple here, but they're very basic. A measure of the co-movement of the two would be to take deviation of x from its mean times the deviation of y from it's mean, and take the average product of those. It's a positive number if, when x is high relative to its mean, y is. And it's a negative number if they tend to go in opposite directions. If GM tends to do well when IBM does poorly, then we have a negative covariance. And this is the core concept that I was talking about. Some idea of unrelatedness underlies a lot of our thinking in risk. If two variables have a plus 1 correlation, that means they move exactly together. If they are independent, then their correlation should be zero. That's true if the random variables are independent of each other. But we're going to see that breakdown of independence is the story of this lecture. We want to think about independence as mattering a lot. And it's a model, or a core idea, but when do we know that things are independent?\"],\n",
       " [\"The crisis that we've seen here in the stock market is the accumulation of -- you see all these ups and downs. There were relatively more downs in the period from 2000 and 2002. But how do we understand the cumulative effect of it, which is what matters? So, we have to have some kind of probability Model. And that is a core question that made it so difficult for us to understand how to deal with such a crisis, and why so many people got in trouble dealing with this crisis. After the 1987 crash, companies started to compute a measure of the risk to their company, called Value at Risk. Many companies had calculated numbers like this, and told their investors, we can't do too badly because there's no way that we could lose. But they were implicitly making assumptions about independence, or at least relative independence. And so, you need a probability Model to make these calculations, which is based on probability theory in order to do that. And it's not one that is easy to be precise about. It's a core concept in finance. Companies all over the world were estimating very small numbers here, relative to what actually happened. The law of large numbers says that if I have a lot of independent shocks, and average them out, on average there's not going to be much uncertainty. It says that the variance of the average of n random variables that are all independent and identically distributed goes to 0 as the number of elements in the average goes to infinity. And so, that's a fundamental concept that underlies both finance and insurance. The law of large numbers has to do with the idea that if I have a large number of random variables, what is the variance of -- the square root of the variance. If they're all independent, then all of the covariances are 0. So, as n goes large, you can see that the standard deviation of the mean goes to 0. The mean is divided by n. The standard deviation is equal to the squareroot of n times the squared root of one of the variables. There's a new idea coming up now, after this recent crisis, and it's called CoVaR.\"],\n",
       " [\"It's a concept emphasized by Professor Brunnermeier at Princeton and some of his colleagues, that we have to change analysis of variance to recognize that portfolios can sometimes co-vary more than we thought. In the present environment, I think, we recognize the need for that.\"],\n",
       " ['The stock market lost something like almost half of its value between 2000 and 2002. But when I put Apple on the same plot, the computer had to, because Apple did such amazing things, it had to compress. Apple computer is the one of the breakout cases of dramatic success in investing. It went up 25 times. This incidentally is the adjusted price for Apple, because in 2005 Apple did a 2-for-1 split. You know what that means? You can see this. You might be surprised to say, wait a minute, did I hear you right? A lot of companies, when the price hits $60 or something like that, they say, well let\\'s just split all the shares in two. An investment in Apple went up 25 times, whereas an investment in the S & P 500 went up only -- well, it didn\\'t go up, actually, it\\'s down. Now, this is a plot showing the monthly returns on Apple. You can\\'t tell from this plot that Apple went. up 25-fold. That matters a lot to an investor. Buy Apple and your money will go up 25-fold, says Warren Buffett. Buffett: \"It wasn\\'t an even ride. It\\'s a scary ride\" Buffett: Buy Apple in one month, you lose 30% in another month, but you can\\'t tell what\\'s driving it up and down. He says the ride, as you\\'re observing this happen, every month it goes opposite. I just goes big swings. Buffett: The ride is not so obvious because it\\'s a rollercoaster ride. You can\\'t see it happening unless you look at your portfolio. In 1979, the Yale class of 1954 had a 25th reunion, and asked an investor to take a risky portfolio investment for Yale and let\\'s give it to Yale on our 50th anniversary, all right? So, they got a portfolio manager, his name was Joe McNay, and they said -- they put together -- it was $375,000. It\\'s like one house, you know, for all the whole class, no big deal. So, McNay decided to invest in Home Depot, Walmart, and internet stocks. On their 50th reunion in 2004, they presented Yale University with $90 million dollars. He started liquidating in 2000, right the peak of the market. So, it must be partly luck.'],\n",
       " ['No one could have known that Walmart was going to be such a success. For every one of the great men and women of history, there\\'s 1,000 of them that got squashed. And I think that history is like that. The people you read about in history are often just phenomenal risk takers like Joe McNey. But maybe they\\'re just lucky, maybe they are just lucky. Apple lost about a third of its value in one month in 2008. The company\\'s founder Steve Jobs had pancreatic cancer in 2004, but the doctors said it\\'s curable, no problem, so the stock didn\\'t do anything. So, it quickly rebounded because he wasn\\'t, and the company\\'s stock went up because he was not cancer-stricken. Bob Greene: Maybe it\\'s all those poor, all those ordinary people, living the little house, the $400,000 house, they don\\'t risk it. Maybe they\\'re the smart ones. Aaron Carroll: I\\'ve just told you about one blip here, but they were so many of these blips on the way, and they all have some story about the success of some Apple product, or people aren\\'t buying some product. Each point represents one of the points that we saw on the market, Carroll says. Carroll: \"It looks totally different, and it shows such complexity that I can\\'t tell a simple narrative. Every month looks different. The best success was in December, January of 2001, where the stock price went up 50% in one month. The reason why it looks kind of compressed on this way is, because the stock market doesn\\'t move as much as Apple. The return for a stock, for the i-th stock, is equal to the market return, which is represented here by the S & P 500, plus idiosyncratic return. The variance of the stock returns is the variance -- the variance of a stock return is the sum of the market. Apple shows a magnified response to the stock market. It goes up and down approximately one and a half times as much as stock market does on any day. Apple has a lot of idiosyncratic risk, but the aggregate economy matters, right?'],\n",
       " [\"If you think that maybe because Apple is kind of a vulnerable company, that if the economy tanks, Apple will tank even more than the economy. If the market goes up, then it's even better news for Apple, even though it's a volatile, dangerous strategy company. He founded Apple and Apple prospered, then had a falling out with the management, and got kind of kicked out of his own company. And then he founded Next Computer. But meanwhile, Apple started to really tank, and they finally realized they needed Steve Jobs, so they brought him back. And it turned out to be the same month that's the Lehman Brothers collapse occurred. This line, I thought it would have an even higher beta, but I think it's this point which is bringing the beta down.\"],\n",
       " [\"A lot of probability theory works on the assumption that variables are normally distributed. But random variables have a habit of not behaving that way, especially in finance it seems. Benoit Mandelbrot was the discoverer of this concept, and I think the most important figure in it. Pierre Paul Levy invented the concept, as discussed in the next lecture in this week's Lecture on the idea of the 'normal' distribution of random shocks to the financial economy. The bell-shaped curve is thought to be a parabola, a mathematical function. In nature the normal distribution is not the only distribution that occurs, and that especially in certain kinds of circumstances we have more fat-tailed distributions. The way you find out that they're not the same, is that in extremely rare circumstances there'll be a sudden major jump in the variable that you might have thought couldn't happen. Whether it's Cauchy or normal, they look about the same; they look pretty much the same. But the pink line has tremendously large probability of being far out. These are the tails of the distribution. Stock market went up 12.53% on October 30, 1929. That's the biggest one-day increase in the history of the stock market. But there were maybe like 20 days, I can't read off the chart when it did this since 1928. You can go through ten years on Wall Street and never see a drop of that magnitude. So, eventually you get kind of assured. It can't happen. What about an 8% drop? Well, I look at this, I say, I've never seen that. It just doesn't happen. The stock market crash of 1929 had two consecutive days. It went down about 12% on October 28, and then the next day it did it again. That's way off the charts, and if you compute the normal distribution, what's the probability of that? If it's a normal distribution and it fits the central portion, it would say it's virtually zero. It couldn't happen. Anyone have any idea what happened on October 30, 1929? It's obvious to me, but it's not obvious to you. If you believe in normality, October 19, 1987 couldn't happen, Bob Greene says.\"],\n",
       " ['He says a student raised his hand and said the stock market is \"totally falling apart\" Greene: The probability of a decline that\\'s that negative? It\\'s 10 to the minus 71 power. 1 over 10 power. That\\'s an awfully small number. But there it is. It happened, Greene says, and in fact, I\\'ve been teaching this course for 25 years. It just came as a complete surprise to me. I went downtown to Merrill Lynch. The two themes are that independence leads to the law of large numbers, and it leads to some sort of stability. But that\\'s not what happened in this crisis and that\\'s the big question. You get big incredible shocks that you thought couldn\\'t happen, and they just come up with a certain low probability.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_slicer( text):\n",
    "        print(len(mcq.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]))\n",
    "        if len(mcq.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]) <= mcq.tokenizer.model_max_length :\n",
    "            return [text]\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        length = 0\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            _len = len(mcq.tokenizer.tokenize(sentence))\n",
    "            if length + _len <= mcq.tokenizer.model_max_length :\n",
    "                length += _len\n",
    "                chunk.append(sentence)\n",
    "            elif not chunk:\n",
    "                # Can a sentence be applicable for splitting on to chunks?\n",
    "                chunks.append(sentence.strip())\n",
    "                length = 0\n",
    "            else:\n",
    "                chunks.append(' '.join(chunk).strip())\n",
    "                chunk = [sentence]\n",
    "                length = _len\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk).strip())\n",
    "        return chunks\n",
    "sections_chunks = []\n",
    "sections_n = []\n",
    "for i,cur_section in enumerate(sections):\n",
    "    cur_section_chunks = text_slicer(cur_section)\n",
    "    n_chunks = len(cur_section_chunks)\n",
    "    sections_chunks.extend(cur_section_chunks)\n",
    "    if n_chunks==1:\n",
    "        sections_n.append(float(i))\n",
    "    else:\n",
    "        sections_n.extend([i+0.1*k for k in range(n_chunks)])\n",
    "print('='*100)\n",
    "[text_slicer(cur_section) for cur_section in sections_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    719\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 376 at dim 1 (got 244)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(mcq\u001b[39m.\u001b[39;49mtokenizer(sections, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]) \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2519\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2520\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2521\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2601\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2602\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2603\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2604\u001b[0m         )\n\u001b[1;32m   2605\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2607\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2608\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2609\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2610\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2611\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2612\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2613\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2614\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2615\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2616\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2617\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2618\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2619\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2620\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2621\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2622\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2623\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2624\u001b[0m     )\n\u001b[1;32m   2625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2626\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2627\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2628\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2645\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2797\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2788\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2789\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2790\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2795\u001b[0m )\n\u001b[0;32m-> 2797\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2798\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2799\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2800\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2801\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2802\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2803\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2804\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2805\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2806\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2807\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2808\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2809\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2810\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2811\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2812\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2813\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2814\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2815\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:737\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 737\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[1;32m    738\u001b[0m     input_ids,\n\u001b[1;32m    739\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    740\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    741\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    742\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    743\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    744\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    745\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    746\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    747\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    748\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    749\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    751\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    752\u001b[0m )\n\u001b[1;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:817\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    807\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m    809\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    810\u001b[0m     batch_outputs,\n\u001b[1;32m    811\u001b[0m     padding\u001b[39m=\u001b[39mpadding_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    815\u001b[0m )\n\u001b[0;32m--> 817\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             )\n\u001b[0;32m--> 733\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "len(mcq.tokenizer(sections, return_tensors=\"pt\")[\"input_ids\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb5dccc8e3c7e786daf1eda69742c93f8becc3cb00853abb54bb5063887fb73d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
