{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    import spacy\n",
    "import json\n",
    "from datetime import datetime\n",
    "name = 'finance_course1'\n",
    "JSON_path = '/home/ubuntu/Questions_generation/Financial Markets Course 2.json'\n",
    "sections_num_max= 'all'\n",
    "%pdb on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions\n",
    "fig = mcq.plot_similarity_matrix((questions_df[questions_used]).to_list())\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=2000,\n",
    "    height=2000)\n",
    "fig.show()\n",
    "#both\n",
    "fig = mcq.plot_similarity_matrix((questions_df[questions_used]+' '+questions_df['selected_ans']).to_list())\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=2000,\n",
    "    height=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit questions to be from the all first sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation \t Definition\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 376])\n",
      "Qs:Who were some of these institutions that collapsed?\n",
      "Qs:The speaker is going to talk about what in the context?\n",
      "Qs:What happened after the stock market collapsed\n",
      "Qs:How did this lecture begin?\n",
      "Qs:When the stock market collapsed around the world, what happened?\n",
      "Qs:Why was there a pre-break around 2000?\n",
      "Qs:What is the speaker going to talk about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 244])\n",
      "Qs:Who built models that are built on theory fluid dynamics?\n",
      "Qs:When was probability coined?\n",
      "Qs:Why can't we predict hurricanes?\n",
      "Qs:How do we build mathematical models of the outcomes of financial events?\n",
      "Qs:What is the word probability in its present meaning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:26, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:Why can't you lose more than you put into an investment in finance?\n",
      "Qs:Who has a limited liability economy?\n",
      "Qs:Which of these is not a positive number?\n",
      "Qs:How do you calculate the expected value for a random variable?\n",
      "Qs:In finance, what is the most basic concept that returns can be positive or negative?\n",
      "Qs:When you invest in something, what is the increase in the price?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:51, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 467])\n",
      "Qs:Who recommends using geometric mean in evaluating investments, and why\n",
      "Qs:Who recommends using geometric mean in evaluating investments\n",
      "Qs:Does the speaker recommend using gross return in evaluating investments?\n",
      "Qs:Which of these is not a measure of covariance?\n",
      "Qs:In what way does the geometric mean make sense?\n",
      "Qs:How does variance differ from variance and covariance?\n",
      "Qs:What is the square root of the variance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:07, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:When was value at risk calculated?\n",
      "Qs:Why did many people get in trouble dealing with this crises?\n",
      "Qs:Which of these is a core concept in finance?\n",
      "Qs:What is the new idea coming up after this recent crisis?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:20, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 62])\n",
      "Qs:Where did professor brunnermeier work?\n",
      "Qs:In which field does professor brunnermeier teach?\n",
      "Qs:The author of this passage is a professor at what university?\n",
      "Qs:Who emphasized the need for change in analysis of variance?\n",
      "Qs:Why do we need to change analysis of variance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:28, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 488])\n",
      "Qs:In what year was the stock market down by almost half?\n",
      "Qs:Which stocks did joe mcnay invest?\n",
      "Qs:Did apple computer go up 25 times or down?\n",
      "Qs:How many times did apple computer go up in value between 2000 and 2002?\n",
      "Qs:What happened to the stock market between 2000 and 2002?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:37, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 484])\n",
      "Qs:Where does aaron carroll work?\n",
      "Qs:When was walmart going to be such success?\n",
      "Qs:What is the best success of apple?\n",
      "Qs:Who started liquidating in 2000?\n",
      "Qs:How many great men and women of history got squashed?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:49, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 197])\n",
      "Qs:Where does this story take place?\n",
      "Qs:When was next computer founded?\n",
      "Qs:What is the name of the company that jobs founded?\n",
      "Qs:In which month did apple start to really tank?\n",
      "Qs:Who founded apple?\n",
      "Qs:Which company was founded by steve jobs?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:03, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 469])\n",
      "Qs:In which month and year did stock market go up 12.53%?\n",
      "Qs:On what date was stock market up 12.53% on october 30, 1929?\n",
      "Qs:When did stock market go up 12.53%?\n",
      "Qs:What is the bell-shaped curve thought to be?\n",
      "Qs:Which stock market went up 12.53% on october 30, 1929?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:15, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 196])\n",
      "Qs:Where does this story take place?\n",
      "Qs:Who was a student of bob greene?\n",
      "Qs:In what year did the stock market collapse?\n",
      "Qs:How many times has bob greene been teaching this course?\n",
      "Qs:What is the probability of a decline that's that negative?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:31, 13.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1:../outputs/03_01_23_finance_course1/GQ.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2:../outputs/03_01_23_finance_course1/QG+QA.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/60 [00:18<00:54,  1.19s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3:../outputs/03_01_23_finance_course1/GQ+QA+GQ.pickle has been saved\n",
      "Stage 4:../outputs/03_01_23_finance_course1/questions_full.txt has been saved\n",
      "Stage 5:../outputs/03_01_23_finance_course1/questions.txt has been saved\n"
     ]
    }
   ],
   "source": [
    "from MCQ import flanT5MCQ\n",
    "from datetime import datetime\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "dir_path = '../outputs/'+datetime.now().strftime(\"%d_%m_%y\")+'_'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "if sections_num_max!='all':\n",
    "  sections = sections[:sections_num_max]\n",
    "print(f'Limit questions to be from the {sections_num_max} first sections')\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "if 'mcq' in locals():\n",
    "  del mcq\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
    "'''\n",
    "cpu_model = mcq.model.to('cpu')\n",
    "del mcq.model\n",
    "mcq.model = cpu_model\n",
    "mcq.push_model_to_GPU(mcq.model)\n",
    "'''\n",
    "with torch.no_grad():\n",
    "  questions_df = mcq.generate_questions(sections=sections,org_sections=org_text,sections_ranks=np.ones(len(sections)))\n",
    "questions_df.to_pickle(dir_path+'GQ.pickle')\n",
    "print(f\"Stage 1:{dir_path+'GQ.pickle'} has been saved\")\n",
    "torch.cuda.empty_cache()\n",
    "if mcq.COMPUTE_ANSWERS:\n",
    "  mcq.push_model_to_GPU(mcq.QA.model)\n",
    "  questions_df = mcq.QA.select_best_answer(questions_df)\n",
    "  questions_df.to_pickle(dir_path+'QG+QA.pickle')\n",
    "  print(f\"Stage 2:{dir_path+'QG+QA.pickle'} has been saved\")\n",
    "  questions_df = mcq.QG.create_question_MixQG(questions_df)\n",
    "  questions_df.to_pickle(dir_path+'GQ+QA+GQ.pickle')\n",
    "  print(f\"Stage 3:{dir_path+'GQ+QA+GQ.pickle'} has been saved\")\n",
    "  questions_used = 'new_question'\n",
    "  questions_df['RQUGE'] = questions_df.apply(lambda x: mcq.rquge.scorer(x.text, x[questions_used], x.selected_ans)[0]['pred_score'] ,axis='columns')\n",
    "  questions_df = questions_df.sort_values('RQUGE',ascending=False).reset_index()\n",
    "  q_sim_mat,q_ans_sim_mat = mcq.find_similarity(questions_df[questions_used].to_list(),\n",
    "                                            answers = (questions_df[questions_used]+' '+questions_df['selected_ans']).to_list())\n",
    "  filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                  q_sim_mat = q_sim_mat, ans_sim_mat = q_ans_sim_mat,\n",
    "                                  n_thrs=20, return_index=True)\n",
    "  questions_df['use_question']=False\n",
    "  questions_df.loc[filter_idx,'use_question']=True\n",
    "  #questions_df = questions_df.iloc[filter_idx,:]\n",
    "with open(f'{dir_path}questions_full.txt', 'w') as f:\n",
    "    qs_text =[mcq.show_qs(questions_df,i) for i in questions_df.index.values]\n",
    "    text_outuput=''\n",
    "    for i,qs in zip(questions_df.index.values,qs_text):\n",
    "        text_outuput += f\"({i})TAKEN?{questions_df['use_question'][i]} RQUGE:{round(questions_df['RQUGE'][i],4)}\"+'\\n'+ qs+\"\\n\\n\"\n",
    "    f.write(text_outuput)\n",
    "    print(f\"Stage 4:{dir_path}questions_full.txt has been saved\")\n",
    "    \n",
    "with open(f'{dir_path}questions.txt', 'w') as f:\n",
    "    text_outuput=''\n",
    "    for i in np.unique(questions_df.section_n_chunk):\n",
    "      qs_in_sections = questions_df.query(f'section_n_chunk == {i} and use_question == True')\n",
    "      for q,a in zip(qs_in_sections[questions_used],qs_in_sections.selected_ans):\n",
    "        text_outuput+=f'Q:{q}\\nA:{a}\\n'\n",
    "      text_outuput+= '-'*50 + '\\n'\n",
    "    f.write(text_outuput)\n",
    "    print(f\"Stage 5:{dir_path}questions.txt has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "Limit questions to be from the all first sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation \t Definition\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 376])\n",
      "Qs:Who were some of these institutions that collapsed?\n",
      "Qs:The speaker is going to talk about what in the context?\n",
      "Qs:What happened after the stock market collapsed\n",
      "Qs:How did this lecture begin?\n",
      "Qs:When the stock market collapsed around the world, what happened?\n",
      "Qs:Why was there a pre-break around 2000?\n",
      "Qs:What is the speaker going to talk about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 244])\n",
      "Qs:Who built models that are built on theory fluid dynamics?\n",
      "Qs:When was probability coined?\n",
      "Qs:Why can't we predict hurricanes?\n",
      "Qs:How do we build mathematical models of the outcomes of financial events?\n",
      "Qs:What is the word probability in its present meaning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:27, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:Why can't you lose more than you put into an investment in finance?\n",
      "Qs:Who has a limited liability economy?\n",
      "Qs:Which of these is not a positive number?\n",
      "Qs:How do you calculate the expected value for a random variable?\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "try:\n",
    "  from compute_question_and_answer import compute_question_and_answer\n",
    "except:\n",
    "  from compute_question_and_answer import compute_question_and_answer\n",
    "import json\n",
    "name = 'finance_course2'\n",
    "sections_num_max = 'all'\n",
    "JSON_path = '/home/ubuntu/Questions_generation/Financial Markets Course 2.json'\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "if sections_num_max!='all':\n",
    "  sections = sections[:sections_num_max]\n",
    "print(f'Limit questions to be from the {sections_num_max} first sections')\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "out = compute_question_and_answer(sections,org_text,save_file_name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in np.unique(questions_df.section_n):\n",
    "    section_output = {'question':[],'answer':[],'score':[]}\n",
    "    for chunk in np.unique(questions_df.loc[questions_df.section_n==i,'section_n_chunk']):\n",
    "        chunk_questions = questions_df.query(f'section_n_chunk=={chunk}')\n",
    "        for _,q in chunk_questions.iterrows():\n",
    "            if q['take']:\n",
    "                section_output['question'].append(q.new_question)\n",
    "                section_output['answer'].append(q['selected_ans'])\n",
    "                section_output['score'].append(q['RQUGE'])\n",
    "    output.append(section_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections\n",
    "dp = [len(x.split())>350 for x in sections]\n",
    "print(dp)\n",
    "len(list(filter(lambda x: x[1] is not None and dp[x[0]], enumerate(sections))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 ms ± 281 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "121 ms ± 894 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = ['Q:What was the derived uncertainty of the model?',\n",
    "'A:4.1 minutes',\n",
    "'Q:What is the uncertainty of the GBT model?',\n",
    "'A:Gbt is the best performing model. the mean absolute error values in the table suggest that the performance is similar across all three algorithms. the derived uncertainty of the model was 4.1 minutes.',\n",
    "'Q:What type of model is Barket-FM-DOSM?',\n",
    "'A:Duration of surgery model',\n",
    "'Q:What did the comparison lead to?',\n",
    "'A:This comparison led to the conclusion that neither machine learning algorithms nor the dataset are the source of differences in the models performance. the major effector is that the set of features is the source.',\n",
    "'Q:What is the main effector of differences in the model performance?',\n",
    "'A:Dr. barket developed the model. the model is based on the surgery dataset. the main effector of such differences is the set of features.',\n",
    "'Q:What is the most important feature?',\n",
    "'A:Duration of surgery is the most important feature.',\n",
    "'Q:3 out of 8 features selected are the same for which two methods?',\n",
    "'A:Shap and pearson correlation.',\n",
    "'Q:Where is Table 1 located?',\n",
    "'A:Using machine learning techniques to develop supervised models that predict duration of surgery from features related to patients, physicians, and surgeries.',\n",
    "'Q:What can be done with the predictions?',\n",
    "'A:Use the dos value predicted by our model for surgery scheduling can decrease patient waiting time and maximize surgical staff idle time.']\n",
    "\n",
    "%timeit mcq.sentence_model.encode(text)\n",
    "%timeit [mcq.sentence_model.encode(t) for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/generation/beam_search.py:198: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 13])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "def check(args,prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    print(input_ids.shape)\n",
    "    res = model.generate(input_ids, **args)\n",
    "    return res['sequences'].shape\n",
    "    output = tokenizer.batch_decode(res['sequences'], skip_special_tokens=True)\n",
    "\n",
    "    output = [item.split(\"<sep>\") for item in output]\n",
    "    if all([len(cur_sen)==1 for cur_sen in output]):  \n",
    "        output = [cur_sen[0] for cur_sen in output]\n",
    "    if len(output)==1 and isinstance(output[0],list):\n",
    "        output = output[0]\n",
    "    output = [qs.strip() for qs in output]\n",
    "    output = [*Counter(output)] #remove exactly similar questions if exist\n",
    "\n",
    "    # Data originally sorted by probabilities, multiply be the penalities activated (`length_penalty` and/or `diversity_penalty`).\n",
    "    # We want it to be sorted by the perplexity (PPL) score, the coherence of the sentence - probability normalized by the length of the sentence. \n",
    "    ppl = [np.exp(np.array(log_likelihoods.cpu()).mean() / np.array(len(cur_output.split())).mean()) for log_likelihoods,cur_output in zip(res['sequences_scores'],output)]\n",
    "    sorted_idx  = np.argsort(ppl)\n",
    "    return [output[id] for id in sorted_idx], [ppl[id] for id in sorted_idx]\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration\n",
    "checkpoint = \"google/flan-t5-large\"#\"IsaacBot/flan-t5-small-mfaq-finetuned-question-generation-context-only\"# \"google/flan-t5-xl\"#\"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "COMES_sections = ['COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.',\n",
    "                  ' Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.',\n",
    "                  ' For a comprehensive survey on the summary evaluation resources see Koto et al. and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task – based on question answering, similarity between summary and reference embeddings.',\n",
    "                  ' COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.']\n",
    "print(check(generator_args,['generate question:'+i for i in COMES_sections]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love</s>\n"
     ]
    }
   ],
   "source": [
    "h = tokenizer.encode('I love', return_tensors=\"pt\")\n",
    "print(tokenizer.decode(h[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generate question:COMET is a recently proposed trainable neuralbased evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.',\n",
       " 'generate question: Since manual annotation for any generative task is costly and time consuming, automatic metrics are commonly used to measure progress during training and compare output from independent systems. Metrics Shared Task collocated with the WMT workshop since 2008 (Callison-Burch et al., 2008) advances in the MT models performance are accompanied by a continuous development of new automatic metrics. They are robust to both domain shifts and changes in annotation style. One of the issues making research on summary evaluation metrics difficult is lack of standardized framework for collecting human judgments.  We propose a variant of the model – COMES2 – that uses the annotated MT data for pre-training and is capable of predicting several aspects of summary quality. We evaluate our approach (Section 4) on selected datasets with various annotated styles. We examine the applicability of the COMET metric by Rei et al. (2020) that is trained on the data and capable of directly regressing a quality score.',\n",
       " 'generate question: For a comprehensive survey on the summary evaluation resources see Koto et al. and system output (Papineni et al., 2002; Lin, 2004). Over the years, a variety of metrics were proposed for this task – based on question answering, similarity between summary and reference embeddings.',\n",
       " 'generate question: COMET is a trained metric that, based on semantic similarities between the translated and reference texts, learns to output a score that resembles the human perception of translation quality. COMET uses a pre-trained multilingual language model to extract representations for each of the input sequences, which are then pooled and concatenated, before being processed with a stack of feed-forward layers that outputs a single numerical value.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['generate question:'+i for i in COMES_sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>section_n</th>\n",
       "      <th>section_rank</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>question_ppl</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>short_answer_1</th>\n",
       "      <th>short_answer_2</th>\n",
       "      <th>short_answer_3</th>\n",
       "      <th>short_answer_4</th>\n",
       "      <th>generated_selected_ans</th>\n",
       "      <th>selected_ans</th>\n",
       "      <th>new_question</th>\n",
       "      <th>RQUGE</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>What is the purpose of this paper?</td>\n",
       "      <td>0.053599</td>\n",
       "      <td>To evaluate text summarization systems using C...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>An evaluation model for text summarization sys...</td>\n",
       "      <td>This paper introduces a variant of the COMET m...</td>\n",
       "      <td>to assess machine translation quality</td>\n",
       "      <td>Use COMET for text summarization evaluation</td>\n",
       "      <td>evaluate text summarization systems</td>\n",
       "      <td>To evaluate the quality of text summarization ...</td>\n",
       "      <td>to evaluate text summarization systems using c...</td>\n",
       "      <td>To evaluate text summarization systems using C...</td>\n",
       "      <td>What is the purpose of this paper?</td>\n",
       "      <td>4.803611</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>The COMET score is calculated by what method?</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>semantic similarities between the translated a...</td>\n",
       "      <td>The COMET score is calculated by a stack of fe...</td>\n",
       "      <td>pre-trained multilingual language model to ext...</td>\n",
       "      <td>stack of feed-forward layers</td>\n",
       "      <td>human perception of translation quality</td>\n",
       "      <td>pre-trained multilingual language model</td>\n",
       "      <td>semantic similarities between the translated a...</td>\n",
       "      <td>pre-trained multilingual language model to ext...</td>\n",
       "      <td>pre-trained multilingual language model to ext...</td>\n",
       "      <td>What does COMET use?</td>\n",
       "      <td>4.583428</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Annotation-based generative models: a new appr...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Using annotated datasets, we propose an automa...</td>\n",
       "      <td>In this paper we propose an annotated version ...</td>\n",
       "      <td>In this paper we propose an annotated version ...</td>\n",
       "      <td>COMES2 is a variant of the Metric Shared Task ...</td>\n",
       "      <td>an annotation-based generative model</td>\n",
       "      <td>Using annotated dataset to evaluate quality of...</td>\n",
       "      <td>Computational Evaluation of Summary Quality of...</td>\n",
       "      <td>Annotation-based generative models: a new appr...</td>\n",
       "      <td>in this paper we propose an annotated version ...</td>\n",
       "      <td>In this paper we propose an annotated version ...</td>\n",
       "      <td>How is the model evaluated?</td>\n",
       "      <td>4.370208</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>In this paper we propose an automatic method f...</td>\n",
       "      <td>In this paper we propose an automatic method f...</td>\n",
       "      <td>Using annotated MT data, we propose a variant ...</td>\n",
       "      <td>We propose a variant of the model – COMES2 – t...</td>\n",
       "      <td>Computational Modeling for Summary Evaluation</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>Using annotated MT data to predict summary eva...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>in this paper we propose an automatic method f...</td>\n",
       "      <td>In this paper we propose an automatic method f...</td>\n",
       "      <td>How is the COMES2 model evaluated?</td>\n",
       "      <td>4.240835</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>How to evaluate the quality of a generative mo...</td>\n",
       "      <td>0.007374</td>\n",
       "      <td>This paper presents an approach to evaluate th...</td>\n",
       "      <td>Using annotated datasets, we propose COMES2 to...</td>\n",
       "      <td>In this paper, we propose COMES2 – a variant o...</td>\n",
       "      <td>COMES2 is a variant of the model that uses the...</td>\n",
       "      <td>CoMES2</td>\n",
       "      <td>the use of the COMET metrics</td>\n",
       "      <td>we introduce COMET2 and evaluate its performan...</td>\n",
       "      <td>COMET metric by Rei et al. (2020)</td>\n",
       "      <td>comes2 is a variant of the model that uses the...</td>\n",
       "      <td>COMES2 is a variant of the model that uses the...</td>\n",
       "      <td>How is COMES2 evaluated?</td>\n",
       "      <td>4.126691</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Using annotated WMT data for pre-training and ...</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>This paper presents an approach to automatical...</td>\n",
       "      <td>Using annotated WMT data for pre-training and ...</td>\n",
       "      <td>The COMET metric is a framework for evaluating...</td>\n",
       "      <td>COMES2 is a variant of the Metrics Shared Task...</td>\n",
       "      <td>Computational Evaluation of Summary Models usi...</td>\n",
       "      <td>Computational Evaluation of Summary Models</td>\n",
       "      <td>Using annotated WMT data for pre-training and ...</td>\n",
       "      <td>COMET2: Using annotated WMT data for pre-train...</td>\n",
       "      <td>comes2 is a variant of the metrics shared task...</td>\n",
       "      <td>COMES2 is a variant of the Metrics Shared Task...</td>\n",
       "      <td>How is COMES2 evaluated?</td>\n",
       "      <td>4.089975</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>In what way does COMET compare to human percep...</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>based on semantic similarities between the tra...</td>\n",
       "      <td>based on semantic similarities between the tra...</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>trained</td>\n",
       "      <td>input sequences</td>\n",
       "      <td>scores</td>\n",
       "      <td>semantic similarities</td>\n",
       "      <td>based on semantic similarities between the tra...</td>\n",
       "      <td>Based on semantic similarities between the tra...</td>\n",
       "      <td>What kind of model does COMET use?</td>\n",
       "      <td>4.015011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>COMET is a trained method that learns to outpu...</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>Score</td>\n",
       "      <td>an output</td>\n",
       "      <td>score that resembles the human perception of t...</td>\n",
       "      <td>The human perception of translation quality. C...</td>\n",
       "      <td>scores</td>\n",
       "      <td>numerical value</td>\n",
       "      <td>translation quality</td>\n",
       "      <td>score</td>\n",
       "      <td>score that resembles the human perception of t...</td>\n",
       "      <td>score that resembles the human perception of t...</td>\n",
       "      <td>What does COMET learn to output?</td>\n",
       "      <td>4.006101</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>In this paper, we explore using the COMET eval...</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>This paper explores the usage of COMET for eva...</td>\n",
       "      <td>A variant of the model, COMET is trained on th...</td>\n",
       "      <td>The use COMET for text summarization systems.</td>\n",
       "      <td>The use COMET for text summarization</td>\n",
       "      <td>The use COMET for text summarization systems</td>\n",
       "      <td>we introduce a variant of the model</td>\n",
       "      <td>a variant of the model, comet is trained on th...</td>\n",
       "      <td>A variant of the model, COMET is trained on th...</td>\n",
       "      <td>What datasets are used to examine COMET's perf...</td>\n",
       "      <td>3.862159</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>Which machine translation metrics can be used ...</td>\n",
       "      <td>0.040325</td>\n",
       "      <td>A new training method for COMET is proposed to...</td>\n",
       "      <td>MT outputs can be used to evaluate text summar...</td>\n",
       "      <td>We introduce a variant of the model – COMET – ...</td>\n",
       "      <td>We introduce a variant of the model – COMET – ...</td>\n",
       "      <td>Comet</td>\n",
       "      <td>Human judgement</td>\n",
       "      <td>human judgments</td>\n",
       "      <td>summarization output quality</td>\n",
       "      <td>we introduce a variant of the model – comet – ...</td>\n",
       "      <td>We introduce a variant of the model – COMET – ...</td>\n",
       "      <td>What is COMET?</td>\n",
       "      <td>3.602739</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>Using multilingual MT outputs, we introduce a ...</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>We introduce a variant of the COMET model – CO...</td>\n",
       "      <td>COMET: A Neural Evaluation Model of Text Summa...</td>\n",
       "      <td>COMET: A Neural Evaluation Model of Text Summa...</td>\n",
       "      <td>The paper introduces a novel training method f...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>the comet model is trained on multilingual mt ...</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>What datasets are used to examine COMET's perf...</td>\n",
       "      <td>3.302905</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>The COMES2 model is a variant of the COMET mod...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>COMES2 is a variant of the COMET model that us...</td>\n",
       "      <td>A variant of the COMES2 model is proposed that...</td>\n",
       "      <td>Annotation-based summary evaluation metrics fo...</td>\n",
       "      <td>Computational Modeling for Summary Evaluation</td>\n",
       "      <td>Using annotated MT data to predict summary eva...</td>\n",
       "      <td>COMES2: Predicting summary evaluation metrics ...</td>\n",
       "      <td>predicting summary evaluation metrics using an...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>What model do we propose?</td>\n",
       "      <td>3.224199</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>What is the purpose of the COMET2 model?</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>To evaluate the quality of summary output of a...</td>\n",
       "      <td>Using annotated MT data for pre-training and p...</td>\n",
       "      <td>Using annotated MT data for pre-training and p...</td>\n",
       "      <td>A variant of the WMT model – COMES2 that uses ...</td>\n",
       "      <td>To evaluate summary quality</td>\n",
       "      <td>evaluate summary quality of generative models</td>\n",
       "      <td>evaluate summary quality</td>\n",
       "      <td>summary evaluation metrics</td>\n",
       "      <td>to evaluate the quality of summary output of a...</td>\n",
       "      <td>To evaluate the quality of summary output of a...</td>\n",
       "      <td>What is the purpose of metrics?</td>\n",
       "      <td>2.896609</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>How does COMET compare to the human perception...</td>\n",
       "      <td>0.081186</td>\n",
       "      <td>Learning to output scores resembles the human ...</td>\n",
       "      <td>An output score that is based on semantic simi...</td>\n",
       "      <td>An output score that is based on semantic simi...</td>\n",
       "      <td></td>\n",
       "      <td>outputs</td>\n",
       "      <td>that,</td>\n",
       "      <td>trained</td>\n",
       "      <td>score</td>\n",
       "      <td>an output score that is based on semantic simi...</td>\n",
       "      <td>An output score that is based on semantic simi...</td>\n",
       "      <td>What is COMET trained to output?</td>\n",
       "      <td>2.841806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>This paper presents an approach to automatical...</td>\n",
       "      <td>Using annotated MT data, we propose an automat...</td>\n",
       "      <td>A variant of the Metrics Shared Task model – C...</td>\n",
       "      <td>We propose a variant of the Metrics Shared Tas...</td>\n",
       "      <td>Computational Modeling for Summary Evaluation</td>\n",
       "      <td>Using annotated MT data to predict summary eva...</td>\n",
       "      <td>Predicting summary evaluation metrics using an...</td>\n",
       "      <td>COMES2: Predicting summary evaluation metrics ...</td>\n",
       "      <td>a variant of the metrics shared task model – c...</td>\n",
       "      <td>A variant of the Metrics Shared Task model – C...</td>\n",
       "      <td>What do we propose?</td>\n",
       "      <td>2.820311</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Since manual annotation for any generative ta...</td>\n",
       "      <td>What is the purpose of the COMET2 model? How i...</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>Predicting several aspects a quality score for...</td>\n",
       "      <td>To evaluate the quality of summary output from...</td>\n",
       "      <td>This paper presents a variant of the COMET mod...</td>\n",
       "      <td>This paper presents a variant of the COMET mod...</td>\n",
       "      <td>summary evaluation metrics</td>\n",
       "      <td>Predicting several aspects of summary quality</td>\n",
       "      <td>to measure progress during training and compar...</td>\n",
       "      <td>predicting several aspects of summary quality</td>\n",
       "      <td>to evaluate the quality of summary output from...</td>\n",
       "      <td>This paper presents a variant of the COMET mod...</td>\n",
       "      <td>What datasets are used to evaluate the COMET2 ...</td>\n",
       "      <td>2.503120</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>Why do you need to train your COMET model?</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>Multilingual language models are used in trans...</td>\n",
       "      <td>To use COMET, you need to train your model.</td>\n",
       "      <td>To use COMET, you need to train your model. Mu...</td>\n",
       "      <td>Pre-trained multilingual language models are u...</td>\n",
       "      <td>input sequence</td>\n",
       "      <td>interprets the output</td>\n",
       "      <td>pre-trained multilingual language model</td>\n",
       "      <td>human perception of translation quality</td>\n",
       "      <td>to use comet, you need to train your model. mu...</td>\n",
       "      <td>To use COMET, you need to train your model. Mu...</td>\n",
       "      <td>How is COMET trained?</td>\n",
       "      <td>2.296661</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>0.005796</td>\n",
       "      <td>This paper introduces a variant of the model –...</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>A variant of the COMET model – COMES – trained...</td>\n",
       "      <td>COMET: A Neural Evaluation Metric for Text Sum...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>An evaluation metric for text summarization sy...</td>\n",
       "      <td>Using COMET to evaluate text summarization sys...</td>\n",
       "      <td>a variant of the comet model – comes – trained...</td>\n",
       "      <td>A variant of the COMET model – COMES – trained...</td>\n",
       "      <td>What is COMET?</td>\n",
       "      <td>1.652104</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>How can we use COMET to evaluate text summariz...</td>\n",
       "      <td>0.041881</td>\n",
       "      <td>Using the COMET model, we evaluate the quality...</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>This paper introduces a variant of the COMET m...</td>\n",
       "      <td>A variant of the model – COMET trained on the ...</td>\n",
       "      <td>Annotated summarization outputs that uses mult...</td>\n",
       "      <td>Annotated summarization outputs</td>\n",
       "      <td>The paper explores the usage of COMET for eval...</td>\n",
       "      <td>we explore the usage of COMET for text summari...</td>\n",
       "      <td>a variant of the model – comet trained on the ...</td>\n",
       "      <td>A variant of the model – COMET trained on the ...</td>\n",
       "      <td>What is COMET?</td>\n",
       "      <td>1.644758</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a recently proposed trainable neuralb...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>Annotated summarization outputs are used for p...</td>\n",
       "      <td>Using annotated summarization outputs, we intr...</td>\n",
       "      <td>The COMET model is trained on multilingual MT ...</td>\n",
       "      <td>Using annotated summarization outputs, we intr...</td>\n",
       "      <td>This work presents a novel training method to ...</td>\n",
       "      <td>COMET: A Neural Evaluation Model for Text Summ...</td>\n",
       "      <td>COMET: A Neural Evaluation Model for Text Summ...</td>\n",
       "      <td>A novel training method for evaluating text su...</td>\n",
       "      <td>using annotated summarization outputs, we intr...</td>\n",
       "      <td>Using annotated summarization outputs, we intr...</td>\n",
       "      <td>What datasets are used to evaluate COMET?</td>\n",
       "      <td>1.592180</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMET is a trained metric that, based on sema...</td>\n",
       "      <td>what are some examples that use COMET?</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>CoMet scores were created by the University of...</td>\n",
       "      <td>Translation quality metrics are used to evalua...</td>\n",
       "      <td>An example of this is the translation of a Wik...</td>\n",
       "      <td>An example of this is the translation of a Wik...</td>\n",
       "      <td>Translate</td>\n",
       "      <td>Translation Quality</td>\n",
       "      <td>Google Translate</td>\n",
       "      <td>Machine translation</td>\n",
       "      <td>an example of this is the translation of a wik...</td>\n",
       "      <td>An example of this is the translation of a Wik...</td>\n",
       "      <td>What is an example of a COMET score?</td>\n",
       "      <td>1.102774</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  section_n  section_rank  \\\n",
       "0       6        0.0           1.0   \n",
       "1      18        2.0           1.0   \n",
       "2       7        1.0           1.0   \n",
       "3       9        1.0           1.0   \n",
       "4      14        1.0           1.0   \n",
       "5      12        1.0           1.0   \n",
       "6      19        2.0           1.0   \n",
       "7      17        2.0           1.0   \n",
       "8       0        0.0           1.0   \n",
       "9       4        0.0           1.0   \n",
       "10      3        0.0           1.0   \n",
       "11      8        1.0           1.0   \n",
       "12     13        1.0           1.0   \n",
       "13     20        2.0           1.0   \n",
       "14     11        1.0           1.0   \n",
       "15     10        1.0           1.0   \n",
       "16     16        2.0           1.0   \n",
       "17      1        0.0           1.0   \n",
       "18      5        0.0           1.0   \n",
       "19      2        0.0           1.0   \n",
       "20     15        2.0           1.0   \n",
       "\n",
       "                                                 text  \\\n",
       "0   COMET is a recently proposed trainable neuralb...   \n",
       "1    COMET is a trained metric that, based on sema...   \n",
       "2    Since manual annotation for any generative ta...   \n",
       "3    Since manual annotation for any generative ta...   \n",
       "4    Since manual annotation for any generative ta...   \n",
       "5    Since manual annotation for any generative ta...   \n",
       "6    COMET is a trained metric that, based on sema...   \n",
       "7    COMET is a trained metric that, based on sema...   \n",
       "8   COMET is a recently proposed trainable neuralb...   \n",
       "9   COMET is a recently proposed trainable neuralb...   \n",
       "10  COMET is a recently proposed trainable neuralb...   \n",
       "11   Since manual annotation for any generative ta...   \n",
       "12   Since manual annotation for any generative ta...   \n",
       "13   COMET is a trained metric that, based on sema...   \n",
       "14   Since manual annotation for any generative ta...   \n",
       "15   Since manual annotation for any generative ta...   \n",
       "16   COMET is a trained metric that, based on sema...   \n",
       "17  COMET is a recently proposed trainable neuralb...   \n",
       "18  COMET is a recently proposed trainable neuralb...   \n",
       "19  COMET is a recently proposed trainable neuralb...   \n",
       "20   COMET is a trained metric that, based on sema...   \n",
       "\n",
       "                                             question  question_ppl  \\\n",
       "0                  What is the purpose of this paper?      0.053599   \n",
       "1       The COMET score is calculated by what method?      0.002099   \n",
       "2   Annotation-based generative models: a new appr...      0.000009   \n",
       "3   Predicting summary evaluation metrics using an...      0.000261   \n",
       "4   How to evaluate the quality of a generative mo...      0.007374   \n",
       "5   Using annotated WMT data for pre-training and ...      0.000979   \n",
       "6   In what way does COMET compare to human percep...      0.006779   \n",
       "7   COMET is a trained method that learns to outpu...      0.000992   \n",
       "8   Using COMET to evaluate text summarization sys...      0.002084   \n",
       "9   Which machine translation metrics can be used ...      0.040325   \n",
       "10  A novel training method for evaluating text su...      0.018356   \n",
       "11  Predicting summary evaluation metrics using an...      0.000033   \n",
       "12           What is the purpose of the COMET2 model?      0.003426   \n",
       "13  How does COMET compare to the human perception...      0.081186   \n",
       "14  Predicting summary evaluation metrics using an...      0.000755   \n",
       "15  What is the purpose of the COMET2 model? How i...      0.000289   \n",
       "16         Why do you need to train your COMET model?      0.000634   \n",
       "17  Using COMET to evaluate text summarization sys...      0.005796   \n",
       "18  How can we use COMET to evaluate text summariz...      0.041881   \n",
       "19  A novel training method for evaluating text su...      0.007119   \n",
       "20             what are some examples that use COMET?      0.000057   \n",
       "\n",
       "                                             answer_1  \\\n",
       "0   To evaluate text summarization systems using C...   \n",
       "1   Based on semantic similarities between the tra...   \n",
       "2   Using annotated datasets, we propose an automa...   \n",
       "3   In this paper we propose an automatic method f...   \n",
       "4   This paper presents an approach to evaluate th...   \n",
       "5   This paper presents an approach to automatical...   \n",
       "6   based on semantic similarities between the tra...   \n",
       "7                                               Score   \n",
       "8   In this paper, we explore using the COMET eval...   \n",
       "9   A new training method for COMET is proposed to...   \n",
       "10  The COMET model is trained on multilingual MT ...   \n",
       "11  The COMES2 model is a variant of the COMET mod...   \n",
       "12  To evaluate the quality of summary output of a...   \n",
       "13  Learning to output scores resembles the human ...   \n",
       "14  This paper presents an approach to automatical...   \n",
       "15  Predicting several aspects a quality score for...   \n",
       "16  Multilingual language models are used in trans...   \n",
       "17  This paper introduces a variant of the model –...   \n",
       "18  Using the COMET model, we evaluate the quality...   \n",
       "19  Annotated summarization outputs are used for p...   \n",
       "20  CoMet scores were created by the University of...   \n",
       "\n",
       "                                             answer_2  \\\n",
       "0   Using COMET to evaluate text summarization sys...   \n",
       "1   semantic similarities between the translated a...   \n",
       "2   In this paper we propose an annotated version ...   \n",
       "3   In this paper we propose an automatic method f...   \n",
       "4   Using annotated datasets, we propose COMES2 to...   \n",
       "5   Using annotated WMT data for pre-training and ...   \n",
       "6   based on semantic similarities between the tra...   \n",
       "7                                           an output   \n",
       "8   Annotated summarization outputs are used for p...   \n",
       "9   MT outputs can be used to evaluate text summar...   \n",
       "10  Using multilingual MT outputs, we introduce a ...   \n",
       "11  Predicting summary evaluation metrics using an...   \n",
       "12  Using annotated MT data for pre-training and p...   \n",
       "13  An output score that is based on semantic simi...   \n",
       "14  Using annotated MT data, we propose an automat...   \n",
       "15  To evaluate the quality of summary output from...   \n",
       "16        To use COMET, you need to train your model.   \n",
       "17  Annotated summarization outputs are used for p...   \n",
       "18  Annotated summarization outputs are used for p...   \n",
       "19  Using annotated summarization outputs, we intr...   \n",
       "20  Translation quality metrics are used to evalua...   \n",
       "\n",
       "                                             answer_3  \\\n",
       "0   An evaluation model for text summarization sys...   \n",
       "1   The COMET score is calculated by a stack of fe...   \n",
       "2   In this paper we propose an annotated version ...   \n",
       "3   Using annotated MT data, we propose a variant ...   \n",
       "4   In this paper, we propose COMES2 – a variant o...   \n",
       "5   The COMET metric is a framework for evaluating...   \n",
       "6   Based on semantic similarities between the tra...   \n",
       "7   score that resembles the human perception of t...   \n",
       "8   This paper explores the usage of COMET for eva...   \n",
       "9   We introduce a variant of the model – COMET – ...   \n",
       "10  The COMET model is trained on multilingual MT ...   \n",
       "11  COMES2 is a variant of the COMET model that us...   \n",
       "12  Using annotated MT data for pre-training and p...   \n",
       "13  An output score that is based on semantic simi...   \n",
       "14  A variant of the Metrics Shared Task model – C...   \n",
       "15  This paper presents a variant of the COMET mod...   \n",
       "16  To use COMET, you need to train your model. Mu...   \n",
       "17  Using COMET to evaluate text summarization sys...   \n",
       "18  This paper introduces a variant of the COMET m...   \n",
       "19  The COMET model is trained on multilingual MT ...   \n",
       "20  An example of this is the translation of a Wik...   \n",
       "\n",
       "                                             answer_4  \\\n",
       "0   This paper introduces a variant of the COMET m...   \n",
       "1   pre-trained multilingual language model to ext...   \n",
       "2   COMES2 is a variant of the Metric Shared Task ...   \n",
       "3   We propose a variant of the model – COMES2 – t...   \n",
       "4   COMES2 is a variant of the model that uses the...   \n",
       "5   COMES2 is a variant of the Metrics Shared Task...   \n",
       "6   Based on semantic similarities between the tra...   \n",
       "7   The human perception of translation quality. C...   \n",
       "8   A variant of the model, COMET is trained on th...   \n",
       "9   We introduce a variant of the model – COMET – ...   \n",
       "10  We introduce a variant of the COMET model – CO...   \n",
       "11  A variant of the COMES2 model is proposed that...   \n",
       "12  A variant of the WMT model – COMES2 that uses ...   \n",
       "13                                                      \n",
       "14  We propose a variant of the Metrics Shared Tas...   \n",
       "15  This paper presents a variant of the COMET mod...   \n",
       "16  Pre-trained multilingual language models are u...   \n",
       "17  A variant of the COMET model – COMES – trained...   \n",
       "18  A variant of the model – COMET trained on the ...   \n",
       "19  Using annotated summarization outputs, we intr...   \n",
       "20  An example of this is the translation of a Wik...   \n",
       "\n",
       "                                       short_answer_1  \\\n",
       "0               to assess machine translation quality   \n",
       "1                        stack of feed-forward layers   \n",
       "2                an annotation-based generative model   \n",
       "3       Computational Modeling for Summary Evaluation   \n",
       "4                                              CoMES2   \n",
       "5   Computational Evaluation of Summary Models usi...   \n",
       "6                                             trained   \n",
       "7                                              scores   \n",
       "8       The use COMET for text summarization systems.   \n",
       "9                                               Comet   \n",
       "10  COMET: A Neural Evaluation Model of Text Summa...   \n",
       "11  Annotation-based summary evaluation metrics fo...   \n",
       "12                        To evaluate summary quality   \n",
       "13                                            outputs   \n",
       "14      Computational Modeling for Summary Evaluation   \n",
       "15                         summary evaluation metrics   \n",
       "16                                     input sequence   \n",
       "17  COMET: A Neural Evaluation Metric for Text Sum...   \n",
       "18  Annotated summarization outputs that uses mult...   \n",
       "19  This work presents a novel training method to ...   \n",
       "20                                          Translate   \n",
       "\n",
       "                                       short_answer_2  \\\n",
       "0         Use COMET for text summarization evaluation   \n",
       "1             human perception of translation quality   \n",
       "2   Using annotated dataset to evaluate quality of...   \n",
       "3   Predicting summary evaluation metrics using an...   \n",
       "4                        the use of the COMET metrics   \n",
       "5          Computational Evaluation of Summary Models   \n",
       "6                                     input sequences   \n",
       "7                                     numerical value   \n",
       "8                The use COMET for text summarization   \n",
       "9                                     Human judgement   \n",
       "10  COMET: A Neural Evaluation Model of Text Summa...   \n",
       "11      Computational Modeling for Summary Evaluation   \n",
       "12      evaluate summary quality of generative models   \n",
       "13                                              that,   \n",
       "14  Using annotated MT data to predict summary eva...   \n",
       "15      Predicting several aspects of summary quality   \n",
       "16                              interprets the output   \n",
       "17  Using COMET to evaluate text summarization sys...   \n",
       "18                    Annotated summarization outputs   \n",
       "19  COMET: A Neural Evaluation Model for Text Summ...   \n",
       "20                                Translation Quality   \n",
       "\n",
       "                                       short_answer_3  \\\n",
       "0                 evaluate text summarization systems   \n",
       "1             pre-trained multilingual language model   \n",
       "2   Computational Evaluation of Summary Quality of...   \n",
       "3   Using annotated MT data to predict summary eva...   \n",
       "4   we introduce COMET2 and evaluate its performan...   \n",
       "5   Using annotated WMT data for pre-training and ...   \n",
       "6                                              scores   \n",
       "7                                 translation quality   \n",
       "8        The use COMET for text summarization systems   \n",
       "9                                     human judgments   \n",
       "10  The paper introduces a novel training method f...   \n",
       "11  Using annotated MT data to predict summary eva...   \n",
       "12                           evaluate summary quality   \n",
       "13                                            trained   \n",
       "14  Predicting summary evaluation metrics using an...   \n",
       "15  to measure progress during training and compar...   \n",
       "16            pre-trained multilingual language model   \n",
       "17  An evaluation metric for text summarization sy...   \n",
       "18  The paper explores the usage of COMET for eval...   \n",
       "19  COMET: A Neural Evaluation Model for Text Summ...   \n",
       "20                                   Google Translate   \n",
       "\n",
       "                                       short_answer_4  \\\n",
       "0   To evaluate the quality of text summarization ...   \n",
       "1   semantic similarities between the translated a...   \n",
       "2   Annotation-based generative models: a new appr...   \n",
       "3   Predicting summary evaluation metrics using an...   \n",
       "4                   COMET metric by Rei et al. (2020)   \n",
       "5   COMET2: Using annotated WMT data for pre-train...   \n",
       "6                               semantic similarities   \n",
       "7                                               score   \n",
       "8                 we introduce a variant of the model   \n",
       "9                        summarization output quality   \n",
       "10  A novel training method for evaluating text su...   \n",
       "11  COMES2: Predicting summary evaluation metrics ...   \n",
       "12                         summary evaluation metrics   \n",
       "13                                              score   \n",
       "14  COMES2: Predicting summary evaluation metrics ...   \n",
       "15      predicting several aspects of summary quality   \n",
       "16            human perception of translation quality   \n",
       "17  Using COMET to evaluate text summarization sys...   \n",
       "18  we explore the usage of COMET for text summari...   \n",
       "19  A novel training method for evaluating text su...   \n",
       "20                                Machine translation   \n",
       "\n",
       "                               generated_selected_ans  \\\n",
       "0   to evaluate text summarization systems using c...   \n",
       "1   pre-trained multilingual language model to ext...   \n",
       "2   in this paper we propose an annotated version ...   \n",
       "3   in this paper we propose an automatic method f...   \n",
       "4   comes2 is a variant of the model that uses the...   \n",
       "5   comes2 is a variant of the metrics shared task...   \n",
       "6   based on semantic similarities between the tra...   \n",
       "7   score that resembles the human perception of t...   \n",
       "8   a variant of the model, comet is trained on th...   \n",
       "9   we introduce a variant of the model – comet – ...   \n",
       "10  the comet model is trained on multilingual mt ...   \n",
       "11  predicting summary evaluation metrics using an...   \n",
       "12  to evaluate the quality of summary output of a...   \n",
       "13  an output score that is based on semantic simi...   \n",
       "14  a variant of the metrics shared task model – c...   \n",
       "15  to evaluate the quality of summary output from...   \n",
       "16  to use comet, you need to train your model. mu...   \n",
       "17  a variant of the comet model – comes – trained...   \n",
       "18  a variant of the model – comet trained on the ...   \n",
       "19  using annotated summarization outputs, we intr...   \n",
       "20  an example of this is the translation of a wik...   \n",
       "\n",
       "                                         selected_ans  \\\n",
       "0   To evaluate text summarization systems using C...   \n",
       "1   pre-trained multilingual language model to ext...   \n",
       "2   In this paper we propose an annotated version ...   \n",
       "3   In this paper we propose an automatic method f...   \n",
       "4   COMES2 is a variant of the model that uses the...   \n",
       "5   COMES2 is a variant of the Metrics Shared Task...   \n",
       "6   Based on semantic similarities between the tra...   \n",
       "7   score that resembles the human perception of t...   \n",
       "8   A variant of the model, COMET is trained on th...   \n",
       "9   We introduce a variant of the model – COMET – ...   \n",
       "10  The COMET model is trained on multilingual MT ...   \n",
       "11  Predicting summary evaluation metrics using an...   \n",
       "12  To evaluate the quality of summary output of a...   \n",
       "13  An output score that is based on semantic simi...   \n",
       "14  A variant of the Metrics Shared Task model – C...   \n",
       "15  This paper presents a variant of the COMET mod...   \n",
       "16  To use COMET, you need to train your model. Mu...   \n",
       "17  A variant of the COMET model – COMES – trained...   \n",
       "18  A variant of the model – COMET trained on the ...   \n",
       "19  Using annotated summarization outputs, we intr...   \n",
       "20  An example of this is the translation of a Wik...   \n",
       "\n",
       "                                         new_question     RQUGE   take  \n",
       "0                  What is the purpose of this paper?  4.803611   True  \n",
       "1                                What does COMET use?  4.583428   True  \n",
       "2                         How is the model evaluated?  4.370208   True  \n",
       "3                  How is the COMES2 model evaluated?  4.240835   True  \n",
       "4                            How is COMES2 evaluated?  4.126691  False  \n",
       "5                            How is COMES2 evaluated?  4.089975  False  \n",
       "6                  What kind of model does COMET use?  4.015011  False  \n",
       "7                    What does COMET learn to output?  4.006101  False  \n",
       "8   What datasets are used to examine COMET's perf...  3.862159   True  \n",
       "9                                      What is COMET?  3.602739  False  \n",
       "10  What datasets are used to examine COMET's perf...  3.302905  False  \n",
       "11                          What model do we propose?  3.224199   True  \n",
       "12                    What is the purpose of metrics?  2.896609   True  \n",
       "13                   What is COMET trained to output?  2.841806  False  \n",
       "14                                What do we propose?  2.820311   True  \n",
       "15  What datasets are used to evaluate the COMET2 ...  2.503120  False  \n",
       "16                              How is COMET trained?  2.296661  False  \n",
       "17                                     What is COMET?  1.652104  False  \n",
       "18                                     What is COMET?  1.644758  False  \n",
       "19          What datasets are used to evaluate COMET?  1.592180  False  \n",
       "20               What is an example of a COMET score?  1.102774   True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from MCQ import flanT5MCQ\n",
    "from datetime import datetime\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "dir_path = '../outputs/'+datetime.now().strftime(\"%d_%m_%y\")+'_'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open(JSON_path) as f:\n",
    "  result = json.load(f)\n",
    "\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "sections = sections[:sections_num_max]\n",
    "print(f'Limit questions to be from the {sections_num_max} first sections')\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
    "'''\n",
    "import pandas as pd\n",
    "questions_df = pd.read_pickle('/home/ubuntu/Questions_generation/outputs/01_01_23_COMET2/GQ+QA+GQ.pickle')\n",
    "\n",
    "questions_used = 'new_question'\n",
    "questions_df['RQUGE'] = questions_df.apply(lambda x: mcq.rquge.scorer(x.text, x[questions_used], x.selected_ans)[0]['pred_score'] ,axis='columns')\n",
    "questions_df = questions_df.sort_values('RQUGE',ascending=False).reset_index()\n",
    "q_sim_mat,ans_sim_mat = mcq.find_similarity(questions_df[questions_used].to_list(),\n",
    "                                            answers = questions_df['selected_ans'].to_list())\n",
    "filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                  q_sim_mat=q_sim_mat,ans_sim_mat=ans_sim_mat,\n",
    "                                 similarity_thrs=0.75, n_thrs=20, return_index=True)\n",
    "questions_df['use_question']=False\n",
    "questions_df.loc[filter_idx,'use_question']=True\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# initialize model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mdel\u001b[39;00m model\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m TransformersQG(language\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlmqg/t5-large-squad-qg-ae\u001b[39;49m\u001b[39m'\u001b[39;49m,device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# paragraph to generate pairs of question and answer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#context = \"William Turner was an English painter who specialised in watercolour landscapes. He is often known as William Turner of Oxford or just Turner of Oxford to distinguish him from his contemporary, J. M. W. Turner. Many of Turner's paintings depicted the countryside around Oxford. One of his best known pictures is a view of the city of Oxford from Hinksey Hill.\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# model prediction\u001b[39;00m\n\u001b[1;32m     10\u001b[0m question_answer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate_qa(sections[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_39270/3981189669.py\u001b[0m(6)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0;31m\u001b[0;31m# initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m\u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformersQG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lmqg/t5-large-squad-qg-ae'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0;31m# paragraph to generate pairs of question and answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0;31m#context = \"William Turner was an English painter who specialised in watercolour landscapes. He is often known as William Turner of Oxford or just Turner of Oxford to distinguish him from his contemporary, J. M. W. Turner. Many of Turner's paintings depicted the countryside around Oxford. One of his best known pictures is a view of the city of Oxford from Hinksey Hill.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install pytextrank\n",
    "#!pip install lmqg\n",
    "from lmqg import TransformersQG\n",
    "# initialize model\n",
    "del model\n",
    "model = TransformersQG(language='en', model='lmqg/t5-large-squad-qg-ae')\n",
    "# paragraph to generate pairs of question and answer\n",
    "#context = \"William Turner was an English painter who specialised in watercolour landscapes. He is often known as William Turner of Oxford or just Turner of Oxford to distinguish him from his contemporary, J. M. W. Turner. Many of Turner's paintings depicted the countryside around Oxford. One of his best known pictures is a view of the city of Oxford from Hinksey Hill.\"\n",
    "# model prediction\n",
    "question_answer = model.generate_qa(sections[0])\n",
    "# the output is a list of tuple (question, answer)\n",
    "print(question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 10, 13, 14, 15, 21, 22]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                 mcq.find_similarity(questions_df[questions_used].to_list()),\n",
    "                                 similarity_thrs=0.75, n_thrs=20, return_index=True)\n",
    "filter_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'name', 'neg_termset', 'ent_types', 'extension_name', and 'chunk_prefix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_sci_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m'\u001b[39m\u001b[39msentencizer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(Negex(nlp))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'name', 'neg_termset', 'ent_types', 'extension_name', and 'chunk_prefix'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_3955/103320592.py\u001b[0m(5)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mfrom\u001b[0m \u001b[0mnegspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNegex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_sci_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentencizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 5 \u001b[0;31m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNegex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from negspacy.negation import Negex\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.add_pipe(Negex(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/spacy/util.py:885: UserWarning: [W094] Model 'en_core_web_sm' (2.2.0) specifies an under-constrained spaCy version requirement: >=2.2.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.4.3,<3.5.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /home/ubuntu/.local/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnegspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnegation\u001b[39;00m \u001b[39mimport\u001b[39;00m Negex\n\u001b[1;32m      3\u001b[0m \u001b[39m#!pip install spacy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#!python3 -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m\"\u001b[39m\u001b[39mnegex\u001b[39m\u001b[39m\"\u001b[39m, config\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39ment_types\u001b[39m\u001b[39m\"\u001b[39m:[\u001b[39m\"\u001b[39m\u001b[39mPERSON\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mORG\u001b[39m\u001b[39m\"\u001b[39m]})\n\u001b[1;32m      8\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39m\"\u001b[39m\u001b[39mShe does not like Steve Jobs but likes Apple products.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     55\u001b[0m         name,\n\u001b[1;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:432\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:468\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[1;32m    453\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 468\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/en_core_web_sm/__init__.py:12\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:649\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    648\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    650\u001b[0m     data_path,\n\u001b[1;32m    651\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    652\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    653\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    654\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    655\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    656\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    657\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:505\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    503\u001b[0m config_path \u001b[39m=\u001b[39m model_path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconfig.cfg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m--> 505\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39;49moverrides)\n\u001b[1;32m    506\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    507\u001b[0m     config,\n\u001b[1;32m    508\u001b[0m     vocab\u001b[39m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     meta\u001b[39m=\u001b[39mmeta,\n\u001b[1;32m    513\u001b[0m )\n\u001b[1;32m    514\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py:681\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config_path \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m config_path\u001b[39m.\u001b[39mis_file():\n\u001b[0;32m--> 681\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE053\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mconfig_path, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig file\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    682\u001b[0m     \u001b[39mreturn\u001b[39;00m config\u001b[39m.\u001b[39mfrom_disk(\n\u001b[1;32m    683\u001b[0m         config_path, overrides\u001b[39m=\u001b[39moverrides, interpolate\u001b[39m=\u001b[39minterpolate\n\u001b[1;32m    684\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config file from /home/ubuntu/.local/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from negspacy.negation import Negex\n",
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"negex\", config={\"ent_types\":[\"PERSON\",\"ORG\"]})\n",
    "\n",
    "doc = nlp(\"She does not like Steve Jobs but likes Apple products.\")\n",
    "for e in doc.ents:\n",
    "    print(e.text, e._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy-3.0.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.64.1)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.9)\n",
      "Collecting thinc<8.1.0,>=8.0.3\n",
      "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "\u001b[K     |████████████████████████████████| 671 kB 72.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 75.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (22.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (45.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.22.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.4.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.0.0-py3-none-any.whl size=13704312 sha256=9005cfa5fcba0510ddcb2d3635fcb34905ac67596c410e72d67b8cc361e6d053\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/8b/21/c1/257748af7399fdaf1b2afc39c92fb839c436f42e67b656ff7e\n",
      "Successfully built en-core-web-sm\n",
      "\u001b[31mERROR: scispacy 0.5.1 has requirement spacy<3.5.0,>=3.4.0, but you'll have spacy 3.0.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: en-core-sci-sm 0.5.1 has requirement spacy<3.5.0,>=3.4.1, but you'll have spacy 3.0.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: click, typer, pydantic, thinc, spacy, en-core-web-sm\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.7.0\n",
      "    Uninstalling typer-0.7.0:\n",
      "      Successfully uninstalled typer-0.7.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.5\n",
      "    Uninstalling thinc-8.1.5:\n",
      "      Successfully uninstalled thinc-8.1.5\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.3\n",
      "    Uninstalling spacy-3.4.3:\n",
      "      Successfully uninstalled spacy-3.4.3\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "Successfully installed click-7.1.2 en-core-web-sm-3.0.0 pydantic-1.8.2 spacy-3.0.9 thinc-8.0.17 typer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Aren’t you coming? Doesn’t he understand? Are you not coming? Does he not understand? Which of the following did not occur? Does Jeff is a real person? Name something David hadn't reveal in his vacation?\")\n",
    "any([token.dep_=='neg' for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 144, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/spacy/__init__.py\", line 15, in <module>\n",
      "    from .cli.info import info  # noqa: F401\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/spacy/cli/__init__.py\", line 17, in <module>\n",
      "    from .debug_diff import debug_diff  # noqa: F401\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/spacy/cli/debug_diff.py\", line 10, in <module>\n",
      "    from .init_config import init_config, Optimizations\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/spacy/cli/init_config.py\", line 8, in <module>\n",
      "    from jinja2 import Template\n",
      "  File \"/usr/lib/python3/dist-packages/jinja2/__init__.py\", line 33, in <module>\n",
      "    from jinja2.environment import Environment, Template\n",
      "  File \"/usr/lib/python3/dist-packages/jinja2/environment.py\", line 15, in <module>\n",
      "    from jinja2 import nodes\n",
      "  File \"/usr/lib/python3/dist-packages/jinja2/nodes.py\", line 23, in <module>\n",
      "    from jinja2.utils import Markup\n",
      "  File \"/usr/lib/python3/dist-packages/jinja2/utils.py\", line 656, in <module>\n",
      "    from markupsafe import Markup, escape, soft_unicode\n",
      "ImportError: cannot import name 'soft_unicode' from 'markupsafe' (/home/ubuntu/.local/lib/python3.8/site-packages/markupsafe/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "from MCQ import flanT5MCQ\n",
    "\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def is_similar(string, patterns):\n",
    "    # compile the regex pattern to remove spaces and punctuation marks\n",
    "    string = string.replace('_','')\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    # remove spaces and punctuation marks from the input string and the patterns\n",
    "    string = regex.sub('', string)\n",
    "    patterns = [regex.sub('', p) for p in patterns]\n",
    "\n",
    "    # check if the input string is exactly the same as one of the patterns\n",
    "    return string.lower().strip() in patterns\n",
    "answers_black_list =['we','they','the authors','authors','author','the author','you','you are','we are',\n",
    "                                'the speaker','speaker','the lecturer','lecturer',\n",
    "                                'he','he is','she','she is',\n",
    "                                  'I','these','those','they are','we do','it','is is']\n",
    "inputs = ['we','We','we...','They!','  Go','THEY  _ ']\n",
    "[is_similar(ak,answers_black_list) for ak in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n",
      "244\n",
      "963\n",
      "534\n",
      "1167\n",
      "664\n",
      "====================================================================================================\n",
      "376\n",
      "244\n",
      "512\n",
      "452\n",
      "473\n",
      "62\n",
      "510\n",
      "485\n",
      "174\n",
      "489\n",
      "176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\" The crisis began with bubbles in the stock market, housing market, and also in the commodities market. It's a financial crisis that's bigger than any since the Great Depression of the 1930's. There's many different ways of thinking about a crisis like this. And I wanted to focus on one way that people think about it in terms of probability models. So, that's not the only way, it's not necessarily my favorite way. Excuse my cold. I didn't bring any water. I hope I make it through this lecture.  There was a pre-break around 2000 when the stock market collapsed around the world. But then they came back again after 2003 and they were on another boom, like a roller coaster ride. That's the narrative story. And then, what happened is, we see a bunch of institutional collapses. We saw bank failures in the U.S. and then, we saw international cooperation to prevent this from spreading like a disease. So, we had governments all over the world bailing out their banks and other companies. That's what financial theorists will think about is that actually it's not just those few big events. It's the accumulation of a lot of little events.  I'm going to talk today about probability, variance, and covariance, and regression, and idiosyncratic risk, and systematic risk. But I'm also going to, in the context of the crisis, emphasize in this lecture, breakdowns of some of the most popular assumptions that underlie financial theory. And I'm thinking particularly of two breakdowns. One is the failure of independence. And another one is a tendency for outliers or fat-tailed distributions.\"],\n",
       " [\" The word probability in its present meaning wasn't even coined until the 1600's. We do it by dealing with all of these little incremental shocks that affect our lives in a mathematical way. We have mathematical laws of how they accumulate. And once we understand those laws, we can we can build mathematical models of the outcomes. And then we can ask whether we should be surprised by the financial events that we've seen. It's a little bit like science, real hard science. So, for example, weather forecasters build models that are built on the theory of fluid dynamics.  People who are steeped in this tradition in finance think that what we're doing is very much like what we do when we do financial forecasts. We have a statistical model, we see all of the shocks coming in, and of course there will be hurricanes. And we can only forecast them -- you know there's a limit to how far out we can forecast them. Weather forecasters can't do that. Same thing with financial crises. We understand the probability laws, there's only a certain time horizon before which we can.\"],\n",
       " [\"In finance, the basic, the most basic concept that -- in finance -- is that when you invest in something, you have to do it for a time interval. And so, what is your return to investing in something? It's the increase in the price. That's p t plus 1, minus p t. Returns can be positive or negative. They can never be more than -- never be less than minus 100%. In a limited liability economy that we live in, the law says that you cannot lose more than the money you put in. This is the mathematical expectation of a random variable x, which could be the return, or the gross return, but we're going to substitute something else. The expectation of x is the weighted sum of all possible values of x weighted by their probabilities. And the probabilities have to sum to 1. They're positive numbers, or zero, reflecting the likelihood of that random variable occurring, of that value of the random variable. This is for a discrete random variable that takes on only a finite, only a countable number of values. Gross return is always positive. It's between zero and infinity. If you have n observations on a random variable x, you can take the sum of the x observations, summation over i equals 1 to n, and then divide that by n. That's called the average, or the mean, or sample mean, when you have a sample of n observations, which is an estimate of the expected value of x. This is called the mean or average, which you've learned long ago, OK. So, for example, if we're evaluating an investor who has invested money, you could get n observations and take an average of them. The geometric mean makes sense only when all the x's are non-negative. If you put in a negative value, you might get a negative product, and then, if you took the nth root of that, it could be an imaginary number, so let's forget that. We're not going to apply this formula if there are any negative numbers. But it's often used, and I recommend its use, in evaluating investments. Because if you use gross return, it gives a better measure of the outcome of the investments.\"],\n",
       " [\"If there's ever a year in which the return is minus 100%, then the geometric mean is 0. That's a good discipline. This obviously doesn't make sense as a way to evaluate investment success. But we care about more than just about central tendency when evaluating risk. We have to do other things as well, including the geometric return, variance, variance and variance. And so, you want to talk about risk, this is very fundamental to finance. What could be more fundamental than risk for finance? If x tends to be plus or minus 1% from the mean return, the variance would probably be 1. The standard deviation is the square root of the variance. Covariance is a measure of how two different random variables move together. When IBM goes up, does General Motors go up or not? We're getting through these concepts, but I'm not going to get into these ideas here, so I'm just trying to be very basic and simple here, but they're very basic. A measure of the co-movement of the two would be to take deviation of x from its mean times the deviation of y from it's mean, and take the average product of those. It's a positive number if, when x is high relative to its mean, y is. And it's a negative number if they tend to go in opposite directions. If GM tends to do well when IBM does poorly, then we have a negative covariance. And this is the core concept that I was talking about. Some idea of unrelatedness underlies a lot of our thinking in risk. If two variables have a plus 1 correlation, that means they move exactly together. If they are independent, then their correlation should be zero. That's true if the random variables are independent of each other. But we're going to see that breakdown of independence is the story of this lecture. We want to think about independence as mattering a lot. And it's a model, or a core idea, but when do we know that things are independent?\"],\n",
       " [\"The crisis that we've seen here in the stock market is the accumulation of -- you see all these ups and downs. There were relatively more downs in the period from 2000 and 2002. But how do we understand the cumulative effect of it, which is what matters? So, we have to have some kind of probability Model. And that is a core question that made it so difficult for us to understand how to deal with such a crisis, and why so many people got in trouble dealing with this crisis. After the 1987 crash, companies started to compute a measure of the risk to their company, called Value at Risk. Many companies had calculated numbers like this, and told their investors, we can't do too badly because there's no way that we could lose. But they were implicitly making assumptions about independence, or at least relative independence. And so, you need a probability Model to make these calculations, which is based on probability theory in order to do that. And it's not one that is easy to be precise about. It's a core concept in finance. Companies all over the world were estimating very small numbers here, relative to what actually happened. The law of large numbers says that if I have a lot of independent shocks, and average them out, on average there's not going to be much uncertainty. It says that the variance of the average of n random variables that are all independent and identically distributed goes to 0 as the number of elements in the average goes to infinity. And so, that's a fundamental concept that underlies both finance and insurance. The law of large numbers has to do with the idea that if I have a large number of random variables, what is the variance of -- the square root of the variance. If they're all independent, then all of the covariances are 0. So, as n goes large, you can see that the standard deviation of the mean goes to 0. The mean is divided by n. The standard deviation is equal to the squareroot of n times the squared root of one of the variables. There's a new idea coming up now, after this recent crisis, and it's called CoVaR.\"],\n",
       " [\"It's a concept emphasized by Professor Brunnermeier at Princeton and some of his colleagues, that we have to change analysis of variance to recognize that portfolios can sometimes co-vary more than we thought. In the present environment, I think, we recognize the need for that.\"],\n",
       " ['The stock market lost something like almost half of its value between 2000 and 2002. But when I put Apple on the same plot, the computer had to, because Apple did such amazing things, it had to compress. Apple computer is the one of the breakout cases of dramatic success in investing. It went up 25 times. This incidentally is the adjusted price for Apple, because in 2005 Apple did a 2-for-1 split. You know what that means? You can see this. You might be surprised to say, wait a minute, did I hear you right? A lot of companies, when the price hits $60 or something like that, they say, well let\\'s just split all the shares in two. An investment in Apple went up 25 times, whereas an investment in the S & P 500 went up only -- well, it didn\\'t go up, actually, it\\'s down. Now, this is a plot showing the monthly returns on Apple. You can\\'t tell from this plot that Apple went. up 25-fold. That matters a lot to an investor. Buy Apple and your money will go up 25-fold, says Warren Buffett. Buffett: \"It wasn\\'t an even ride. It\\'s a scary ride\" Buffett: Buy Apple in one month, you lose 30% in another month, but you can\\'t tell what\\'s driving it up and down. He says the ride, as you\\'re observing this happen, every month it goes opposite. I just goes big swings. Buffett: The ride is not so obvious because it\\'s a rollercoaster ride. You can\\'t see it happening unless you look at your portfolio. In 1979, the Yale class of 1954 had a 25th reunion, and asked an investor to take a risky portfolio investment for Yale and let\\'s give it to Yale on our 50th anniversary, all right? So, they got a portfolio manager, his name was Joe McNay, and they said -- they put together -- it was $375,000. It\\'s like one house, you know, for all the whole class, no big deal. So, McNay decided to invest in Home Depot, Walmart, and internet stocks. On their 50th reunion in 2004, they presented Yale University with $90 million dollars. He started liquidating in 2000, right the peak of the market. So, it must be partly luck.'],\n",
       " ['No one could have known that Walmart was going to be such a success. For every one of the great men and women of history, there\\'s 1,000 of them that got squashed. And I think that history is like that. The people you read about in history are often just phenomenal risk takers like Joe McNey. But maybe they\\'re just lucky, maybe they are just lucky. Apple lost about a third of its value in one month in 2008. The company\\'s founder Steve Jobs had pancreatic cancer in 2004, but the doctors said it\\'s curable, no problem, so the stock didn\\'t do anything. So, it quickly rebounded because he wasn\\'t, and the company\\'s stock went up because he was not cancer-stricken. Bob Greene: Maybe it\\'s all those poor, all those ordinary people, living the little house, the $400,000 house, they don\\'t risk it. Maybe they\\'re the smart ones. Aaron Carroll: I\\'ve just told you about one blip here, but they were so many of these blips on the way, and they all have some story about the success of some Apple product, or people aren\\'t buying some product. Each point represents one of the points that we saw on the market, Carroll says. Carroll: \"It looks totally different, and it shows such complexity that I can\\'t tell a simple narrative. Every month looks different. The best success was in December, January of 2001, where the stock price went up 50% in one month. The reason why it looks kind of compressed on this way is, because the stock market doesn\\'t move as much as Apple. The return for a stock, for the i-th stock, is equal to the market return, which is represented here by the S & P 500, plus idiosyncratic return. The variance of the stock returns is the variance -- the variance of a stock return is the sum of the market. Apple shows a magnified response to the stock market. It goes up and down approximately one and a half times as much as stock market does on any day. Apple has a lot of idiosyncratic risk, but the aggregate economy matters, right?'],\n",
       " [\"If you think that maybe because Apple is kind of a vulnerable company, that if the economy tanks, Apple will tank even more than the economy. If the market goes up, then it's even better news for Apple, even though it's a volatile, dangerous strategy company. He founded Apple and Apple prospered, then had a falling out with the management, and got kind of kicked out of his own company. And then he founded Next Computer. But meanwhile, Apple started to really tank, and they finally realized they needed Steve Jobs, so they brought him back. And it turned out to be the same month that's the Lehman Brothers collapse occurred. This line, I thought it would have an even higher beta, but I think it's this point which is bringing the beta down.\"],\n",
       " [\"A lot of probability theory works on the assumption that variables are normally distributed. But random variables have a habit of not behaving that way, especially in finance it seems. Benoit Mandelbrot was the discoverer of this concept, and I think the most important figure in it. Pierre Paul Levy invented the concept, as discussed in the next lecture in this week's Lecture on the idea of the 'normal' distribution of random shocks to the financial economy. The bell-shaped curve is thought to be a parabola, a mathematical function. In nature the normal distribution is not the only distribution that occurs, and that especially in certain kinds of circumstances we have more fat-tailed distributions. The way you find out that they're not the same, is that in extremely rare circumstances there'll be a sudden major jump in the variable that you might have thought couldn't happen. Whether it's Cauchy or normal, they look about the same; they look pretty much the same. But the pink line has tremendously large probability of being far out. These are the tails of the distribution. Stock market went up 12.53% on October 30, 1929. That's the biggest one-day increase in the history of the stock market. But there were maybe like 20 days, I can't read off the chart when it did this since 1928. You can go through ten years on Wall Street and never see a drop of that magnitude. So, eventually you get kind of assured. It can't happen. What about an 8% drop? Well, I look at this, I say, I've never seen that. It just doesn't happen. The stock market crash of 1929 had two consecutive days. It went down about 12% on October 28, and then the next day it did it again. That's way off the charts, and if you compute the normal distribution, what's the probability of that? If it's a normal distribution and it fits the central portion, it would say it's virtually zero. It couldn't happen. Anyone have any idea what happened on October 30, 1929? It's obvious to me, but it's not obvious to you. If you believe in normality, October 19, 1987 couldn't happen, Bob Greene says.\"],\n",
       " ['He says a student raised his hand and said the stock market is \"totally falling apart\" Greene: The probability of a decline that\\'s that negative? It\\'s 10 to the minus 71 power. 1 over 10 power. That\\'s an awfully small number. But there it is. It happened, Greene says, and in fact, I\\'ve been teaching this course for 25 years. It just came as a complete surprise to me. I went downtown to Merrill Lynch. The two themes are that independence leads to the law of large numbers, and it leads to some sort of stability. But that\\'s not what happened in this crisis and that\\'s the big question. You get big incredible shocks that you thought couldn\\'t happen, and they just come up with a certain low probability.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_slicer( text):\n",
    "        print(len(mcq.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]))\n",
    "        if len(mcq.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]) <= mcq.tokenizer.model_max_length :\n",
    "            return [text]\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        length = 0\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            _len = len(mcq.tokenizer.tokenize(sentence))\n",
    "            if length + _len <= mcq.tokenizer.model_max_length :\n",
    "                length += _len\n",
    "                chunk.append(sentence)\n",
    "            elif not chunk:\n",
    "                # Can a sentence be applicable for splitting on to chunks?\n",
    "                chunks.append(sentence.strip())\n",
    "                length = 0\n",
    "            else:\n",
    "                chunks.append(' '.join(chunk).strip())\n",
    "                chunk = [sentence]\n",
    "                length = _len\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk).strip())\n",
    "        return chunks\n",
    "sections_chunks = []\n",
    "sections_n = []\n",
    "for i,cur_section in enumerate(sections):\n",
    "    cur_section_chunks = text_slicer(cur_section)\n",
    "    n_chunks = len(cur_section_chunks)\n",
    "    sections_chunks.extend(cur_section_chunks)\n",
    "    if n_chunks==1:\n",
    "        sections_n.append(float(i))\n",
    "    else:\n",
    "        sections_n.extend([i+0.1*k for k in range(n_chunks)])\n",
    "print('='*100)\n",
    "[text_slicer(cur_section) for cur_section in sections_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    719\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 376 at dim 1 (got 244)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(mcq\u001b[39m.\u001b[39;49mtokenizer(sections, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]) \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2519\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2520\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2521\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2601\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2602\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2603\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2604\u001b[0m         )\n\u001b[1;32m   2605\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2607\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2608\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2609\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2610\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2611\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2612\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2613\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2614\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2615\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2616\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2617\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2618\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2619\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2620\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2621\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2622\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2623\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2624\u001b[0m     )\n\u001b[1;32m   2625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2626\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2627\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2628\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2645\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2797\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2788\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2789\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2790\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2795\u001b[0m )\n\u001b[0;32m-> 2797\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2798\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2799\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2800\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2801\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2802\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2803\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2804\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2805\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2806\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2807\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2808\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2809\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2810\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2811\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2812\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2813\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2814\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2815\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:737\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 737\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[1;32m    738\u001b[0m     input_ids,\n\u001b[1;32m    739\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    740\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    741\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    742\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    743\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    744\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    745\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    746\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    747\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    748\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    749\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    751\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    752\u001b[0m )\n\u001b[1;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:817\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    807\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m    809\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    810\u001b[0m     batch_outputs,\n\u001b[1;32m    811\u001b[0m     padding\u001b[39m=\u001b[39mpadding_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    815\u001b[0m )\n\u001b[0;32m--> 817\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             )\n\u001b[0;32m--> 733\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "len(mcq.tokenizer(sections, return_tensors=\"pt\")[\"input_ids\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
