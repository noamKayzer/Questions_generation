{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation \t Definition\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 376])\n",
      "Qs:Why was there a pre-break around 2000?\n",
      "Qs:Who were some of these institutions that collapsed?\n",
      "Qs:The speaker is going to talk about what in the context?\n",
      "Qs:How did this lecture begin?\n",
      "Qs:When the stock market collapsed around the world, what happened?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:11, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 244])\n",
      "Qs:When was probability coined?\n",
      "Qs:What is the word probability in its present meaning?\n",
      "Qs:Who built models that are built on theory fluid dynamics?\n",
      "Qs:Why can't we predict hurricanes?\n",
      "Qs:How do we build mathematical models of the outcomes of financial events?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:22, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:Which of these is not a positive number?\n",
      "Qs:In finance, what is the most basic concept that returns can be positive or negative?\n",
      "Qs:Why can't you lose more than you put into an investment in finance?\n",
      "Qs:Who has a limited liability economy?\n",
      "Qs:How do you calculate the expected value for a random variable?\n",
      "Qs:When you invest in something, what is the increase in the price?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:49, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 467])\n",
      "Qs:In what way does the geometric mean make sense?\n",
      "Qs:Does the speaker recommend using gross return in evaluating investments?\n",
      "Qs:Which of these is not a measure of covariance?\n",
      "Qs:How does variance differ from variance and covariance?\n",
      "Qs:What is the square root of the variance?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:02, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 25])\n",
      "Qs:Where does this idea come from?\n",
      "Qs:what are independent ideas? pick from the following.\n",
      "Qs:How can you tell if something is independent?\n",
      "Qs:Which of these is not a model, or a core idea?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:09, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 473])\n",
      "Qs:When was Value at Risk calculated?\n",
      "Qs:Which of these is a core concept in finance?\n",
      "Qs:What is the new idea coming up after this recent crisis?\n",
      "Qs:Why did many people get in trouble dealing with this crises?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:23, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 62])\n",
      "Qs:Where did Professor Brunnermeier work?\n",
      "Qs:Why do we need to change analysis of variance?\n",
      "Qs:The author of this passage is a professor at what university?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:28, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 488])\n",
      "Qs:Which stocks did Joe McNay invest?\n",
      "Qs:What happened to the stock market between 2000 and 2002?\n",
      "Qs:Did Apple Computer go up 25 times or down?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:34,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 484])\n",
      "Qs:Who started liquidating in 2000?\n",
      "Qs:When was Walmart going to be such success?\n",
      "Qs:Where does Aaron Carroll work?\n",
      "Qs:What is the best success of Apple?\n",
      "Qs:How many great men and women of history got squashed?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:46, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 197])\n",
      "Qs:Who founded Apple?\n",
      "Qs:When was Next Computer founded?\n",
      "Qs:Where does this story take place?\n",
      "Qs:What is the name of the company that Jobs founded?\n",
      "Qs:In which month did Apple start to really tank?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:57, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 469])\n",
      "Qs:What is the bell-shaped curve thought to be?\n",
      "Qs:Which stock market went up 12.53% on October 30, 1929?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:04,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text shape is torch.Size([1, 196])\n",
      "Qs:Where does this story take place?\n",
      "Qs:In what year did the stock market collapse?\n",
      "Qs:Who was a student of Bob Greene?\n",
      "Qs:What is the probability of a decline that's that negative?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:16, 11.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1:../outputs/Financial_Markets_Course2_flanT5L/GQ.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2:../outputs/Financial_Markets_Course2_flanT5L/QG+QA.pickle has been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 14/51 [00:19<01:00,  1.64s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 51/51 [01:07<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3:../outputs/Financial_Markets_Course2_flanT5L/GQ+QA+GQ.pickle has been saved\n",
      "Stage 3:../outputs/Financial_Markets_Course2_flanT5L/questions.txt has been saved\n",
      "Stage 4:../outputs/Financial_Markets_Course2_flanT5L/questions.txt has been saved\n"
     ]
    }
   ],
   "source": [
    "from MCQ import flanT5MCQ\n",
    "\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)\n",
    "\n",
    "cpu_model = mcq.model.to('cpu')\n",
    "del mcq.model\n",
    "mcq.model = cpu_model\n",
    "mcq.push_model_to_GPU(mcq.model)\n",
    "\n",
    "with torch.no_grad():\n",
    "  questions_df = mcq.compute_questions(sections=sections,org_sections=org_text,sections_ranks=np.ones(len(sections)))\n",
    "questions_df.to_pickle(dir_path+'GQ.pickle')\n",
    "print(f\"Stage 1:{dir_path+'GQ.pickle'} has been saved\")\n",
    "torch.cuda.empty_cache()\n",
    "if mcq.COMPUTE_ANSWERS:\n",
    "  mcq.push_model_to_GPU(mcq.QA.model)\n",
    "  questions_df = mcq.QA.select_best_answer(questions_df)\n",
    "  questions_df.to_pickle(dir_path+'QG+QA.pickle')\n",
    "  print(f\"Stage 2:{dir_path+'QG+QA.pickle'} has been saved\")\n",
    "  questions_df = mcq.QG.create_question_MixQG(questions_df)\n",
    "  questions_df.to_pickle(dir_path+'GQ+QA+GQ.pickle')\n",
    "  print(f\"Stage 3:{dir_path+'GQ+QA+GQ.pickle'} has been saved\")\n",
    "  \n",
    "  questions_used = 'new_question'\n",
    "  questions_df['RQUGE'] = questions_df.apply(lambda x: mcq.rquge.scorer(x.text, x[questions_used], x.selected_ans)[0]['pred_score'] ,axis='columns')\n",
    "  questions_df = questions_df.sort_values('RQUGE',ascending=False).reset_index()\n",
    "  filter_idx = mcq.filter_questions(questions_df[questions_used].to_list(),\n",
    "                                 mcq.find_similarity(questions_df[questions_used].to_list()),\n",
    "                                 similarity_thrs=0.5, n_thrs=20, return_index=True)\n",
    "  questions_df = questions_df.iloc[filter_idx,:]\n",
    "with open(f'{dir_path}questions_full.txt', 'w') as f:\n",
    "    qs_text =[mcq.show_qs(questions_df,i) for i in questions_df.index.values]\n",
    "    text_outuput=''\n",
    "    for i,qs in zip(questions_df.index.values,qs_text):\n",
    "        text_outuput += f\"({i}) RQUGE:{round(questions_df['RQUGE'][i],4)}\"+'\\n'+ qs+\"\\n\\n\"\n",
    "    f.write(text_outuput)\n",
    "    print(f\"Stage 3:{dir_path}questions.txt has been saved\")\n",
    "    \n",
    "with open(f'{dir_path}questions.txt', 'w') as f:\n",
    "    text_outuput=''\n",
    "    for i in np.unique(questions_df.section_n):\n",
    "      qs_in_sections = questions_df.loc[questions_df.section_n==i,:]\n",
    "      for q,a in zip(qs_in_sections[questions_used],qs_in_sections.selected_ans):\n",
    "        text_outuput+=f'Q:{q}\\nA:{a}\\n'\n",
    "      text_outuput+= '-'*50 + '\\n'\n",
    "    f.write(text_outuput)\n",
    "    print(f\"Stage 4:{dir_path}questions.txt has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "from MCQ import flanT5MCQ\n",
    "\n",
    "generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "#\"max_length\": 256,\n",
    "\"num_beams\": 10, #20\n",
    "\"length_penalty\":-0.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "\"no_repeat_ngram_size\": 3,\n",
    "#'force_words_ids':[[58]],#token of `?` -cannot use constrained beam search with grouped beam search, while `diversity_penalty` can be used only with group beam search.\n",
    "'top_p' :0.955,\n",
    "#'do_sample':True,\n",
    "'diversity_penalty':float(10), #note diversity is calculated between groups, the final scores are across all outputs, therfore the results with highest scores may be from one group and the diversity calcultion won't be effective for large groups\n",
    "'num_beam_groups':10,#20 \n",
    "\"return_dict_in_generate\" :True,\n",
    "'output_scores':True,\n",
    "\"early_stopping\": True, \n",
    "'num_return_sequences':8\n",
    "}\n",
    "\n",
    "answers_generator_args = {\n",
    "    \"max_new_tokens\":150,\n",
    "    #\"max_length\": 256,\n",
    "    \"num_beams\": 8,#10\n",
    "    \"length_penalty\":0.2,\n",
    "    #\"length_penalty\": 1.5, #Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences.\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    #'force_words_ids':[tokenizer.encode(['.'])],\n",
    "    'top_p' :0.97,\n",
    "    'diversity_penalty':float(8),\n",
    "    'num_beam_groups':8,#10,\n",
    "    \"return_dict_in_generate\" :True,\n",
    "    'output_scores':True,\n",
    "    \"early_stopping\": True,\n",
    "    'num_return_sequences':5\n",
    "}\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import torch\n",
    "# Disable all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "name = 'Financial_Markets_Course2_flanT5L'\n",
    "\n",
    "dir_path = '../outputs/'+name+'/'\n",
    "if not os.path.isdir(dir_path):\n",
    "  os.mkdir(dir_path)\n",
    "\n",
    "with open('/home/ubuntu/Questions_generation/Financial Markets Course 2.json') as f:\n",
    "  result = json.load(f)\n",
    "sections = [s['summary']['text'] if 'summary' in s.keys() and 'text' in s['summary'].keys() else None for k,s in result['sections'].items() ]\n",
    "org_text = [s['original']['text'] if 'original' in s.keys() and 'text' in s['original'].keys() else None  for k,s in result['sections'].items() ]\n",
    "\n",
    "min_words_in_section=60\n",
    "sections = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, sections))\n",
    "#org_text = list(filter(lambda x: x is not None and len(x.split())>min_words_in_section, org_text))\n",
    "short_answers_generator_args = copy.deepcopy(answers_generator_args)\n",
    "short_answers_generator_args[\"length_penalty\"]=-0.6\n",
    "mcq = flanT5MCQ(generator_args=generator_args,answers_generator_args=answers_generator_args,short_answers_generator_args=short_answers_generator_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def is_similar(string, patterns):\n",
    "    # compile the regex pattern to remove spaces and punctuation marks\n",
    "    string = string.replace('_','')\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    # remove spaces and punctuation marks from the input string and the patterns\n",
    "    string = regex.sub('', string)\n",
    "    patterns = [regex.sub('', p) for p in patterns]\n",
    "\n",
    "    # check if the input string is exactly the same as one of the patterns\n",
    "    return string.lower().strip() in patterns\n",
    "answers_black_list =['we','they','the authors','authors','author','the author','you','you are','we are',\n",
    "                                'the speaker','speaker','the lecturer','lecturer',\n",
    "                                'he','he is','she','she is',\n",
    "                                  'I','these','those','they are','we do','it','is is']\n",
    "inputs = ['we','We','we...','They!','  Go','THEY  _ ']\n",
    "[is_similar(ak,answers_black_list) for ak in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n",
      "244\n",
      "963\n",
      "534\n",
      "1167\n",
      "664\n",
      "====================================================================================================\n",
      "376\n",
      "244\n",
      "512\n",
      "452\n",
      "473\n",
      "62\n",
      "510\n",
      "485\n",
      "174\n",
      "489\n",
      "176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\" The crisis began with bubbles in the stock market, housing market, and also in the commodities market. It's a financial crisis that's bigger than any since the Great Depression of the 1930's. There's many different ways of thinking about a crisis like this. And I wanted to focus on one way that people think about it in terms of probability models. So, that's not the only way, it's not necessarily my favorite way. Excuse my cold. I didn't bring any water. I hope I make it through this lecture.  There was a pre-break around 2000 when the stock market collapsed around the world. But then they came back again after 2003 and they were on another boom, like a roller coaster ride. That's the narrative story. And then, what happened is, we see a bunch of institutional collapses. We saw bank failures in the U.S. and then, we saw international cooperation to prevent this from spreading like a disease. So, we had governments all over the world bailing out their banks and other companies. That's what financial theorists will think about is that actually it's not just those few big events. It's the accumulation of a lot of little events.  I'm going to talk today about probability, variance, and covariance, and regression, and idiosyncratic risk, and systematic risk. But I'm also going to, in the context of the crisis, emphasize in this lecture, breakdowns of some of the most popular assumptions that underlie financial theory. And I'm thinking particularly of two breakdowns. One is the failure of independence. And another one is a tendency for outliers or fat-tailed distributions.\"],\n",
       " [\" The word probability in its present meaning wasn't even coined until the 1600's. We do it by dealing with all of these little incremental shocks that affect our lives in a mathematical way. We have mathematical laws of how they accumulate. And once we understand those laws, we can we can build mathematical models of the outcomes. And then we can ask whether we should be surprised by the financial events that we've seen. It's a little bit like science, real hard science. So, for example, weather forecasters build models that are built on the theory of fluid dynamics.  People who are steeped in this tradition in finance think that what we're doing is very much like what we do when we do financial forecasts. We have a statistical model, we see all of the shocks coming in, and of course there will be hurricanes. And we can only forecast them -- you know there's a limit to how far out we can forecast them. Weather forecasters can't do that. Same thing with financial crises. We understand the probability laws, there's only a certain time horizon before which we can.\"],\n",
       " [\"In finance, the basic, the most basic concept that -- in finance -- is that when you invest in something, you have to do it for a time interval. And so, what is your return to investing in something? It's the increase in the price. That's p t plus 1, minus p t. Returns can be positive or negative. They can never be more than -- never be less than minus 100%. In a limited liability economy that we live in, the law says that you cannot lose more than the money you put in. This is the mathematical expectation of a random variable x, which could be the return, or the gross return, but we're going to substitute something else. The expectation of x is the weighted sum of all possible values of x weighted by their probabilities. And the probabilities have to sum to 1. They're positive numbers, or zero, reflecting the likelihood of that random variable occurring, of that value of the random variable. This is for a discrete random variable that takes on only a finite, only a countable number of values. Gross return is always positive. It's between zero and infinity. If you have n observations on a random variable x, you can take the sum of the x observations, summation over i equals 1 to n, and then divide that by n. That's called the average, or the mean, or sample mean, when you have a sample of n observations, which is an estimate of the expected value of x. This is called the mean or average, which you've learned long ago, OK. So, for example, if we're evaluating an investor who has invested money, you could get n observations and take an average of them. The geometric mean makes sense only when all the x's are non-negative. If you put in a negative value, you might get a negative product, and then, if you took the nth root of that, it could be an imaginary number, so let's forget that. We're not going to apply this formula if there are any negative numbers. But it's often used, and I recommend its use, in evaluating investments. Because if you use gross return, it gives a better measure of the outcome of the investments.\"],\n",
       " [\"If there's ever a year in which the return is minus 100%, then the geometric mean is 0. That's a good discipline. This obviously doesn't make sense as a way to evaluate investment success. But we care about more than just about central tendency when evaluating risk. We have to do other things as well, including the geometric return, variance, variance and variance. And so, you want to talk about risk, this is very fundamental to finance. What could be more fundamental than risk for finance? If x tends to be plus or minus 1% from the mean return, the variance would probably be 1. The standard deviation is the square root of the variance. Covariance is a measure of how two different random variables move together. When IBM goes up, does General Motors go up or not? We're getting through these concepts, but I'm not going to get into these ideas here, so I'm just trying to be very basic and simple here, but they're very basic. A measure of the co-movement of the two would be to take deviation of x from its mean times the deviation of y from it's mean, and take the average product of those. It's a positive number if, when x is high relative to its mean, y is. And it's a negative number if they tend to go in opposite directions. If GM tends to do well when IBM does poorly, then we have a negative covariance. And this is the core concept that I was talking about. Some idea of unrelatedness underlies a lot of our thinking in risk. If two variables have a plus 1 correlation, that means they move exactly together. If they are independent, then their correlation should be zero. That's true if the random variables are independent of each other. But we're going to see that breakdown of independence is the story of this lecture. We want to think about independence as mattering a lot. And it's a model, or a core idea, but when do we know that things are independent?\"],\n",
       " [\"The crisis that we've seen here in the stock market is the accumulation of -- you see all these ups and downs. There were relatively more downs in the period from 2000 and 2002. But how do we understand the cumulative effect of it, which is what matters? So, we have to have some kind of probability Model. And that is a core question that made it so difficult for us to understand how to deal with such a crisis, and why so many people got in trouble dealing with this crisis. After the 1987 crash, companies started to compute a measure of the risk to their company, called Value at Risk. Many companies had calculated numbers like this, and told their investors, we can't do too badly because there's no way that we could lose. But they were implicitly making assumptions about independence, or at least relative independence. And so, you need a probability Model to make these calculations, which is based on probability theory in order to do that. And it's not one that is easy to be precise about. It's a core concept in finance. Companies all over the world were estimating very small numbers here, relative to what actually happened. The law of large numbers says that if I have a lot of independent shocks, and average them out, on average there's not going to be much uncertainty. It says that the variance of the average of n random variables that are all independent and identically distributed goes to 0 as the number of elements in the average goes to infinity. And so, that's a fundamental concept that underlies both finance and insurance. The law of large numbers has to do with the idea that if I have a large number of random variables, what is the variance of -- the square root of the variance. If they're all independent, then all of the covariances are 0. So, as n goes large, you can see that the standard deviation of the mean goes to 0. The mean is divided by n. The standard deviation is equal to the squareroot of n times the squared root of one of the variables. There's a new idea coming up now, after this recent crisis, and it's called CoVaR.\"],\n",
       " [\"It's a concept emphasized by Professor Brunnermeier at Princeton and some of his colleagues, that we have to change analysis of variance to recognize that portfolios can sometimes co-vary more than we thought. In the present environment, I think, we recognize the need for that.\"],\n",
       " ['The stock market lost something like almost half of its value between 2000 and 2002. But when I put Apple on the same plot, the computer had to, because Apple did such amazing things, it had to compress. Apple computer is the one of the breakout cases of dramatic success in investing. It went up 25 times. This incidentally is the adjusted price for Apple, because in 2005 Apple did a 2-for-1 split. You know what that means? You can see this. You might be surprised to say, wait a minute, did I hear you right? A lot of companies, when the price hits $60 or something like that, they say, well let\\'s just split all the shares in two. An investment in Apple went up 25 times, whereas an investment in the S & P 500 went up only -- well, it didn\\'t go up, actually, it\\'s down. Now, this is a plot showing the monthly returns on Apple. You can\\'t tell from this plot that Apple went. up 25-fold. That matters a lot to an investor. Buy Apple and your money will go up 25-fold, says Warren Buffett. Buffett: \"It wasn\\'t an even ride. It\\'s a scary ride\" Buffett: Buy Apple in one month, you lose 30% in another month, but you can\\'t tell what\\'s driving it up and down. He says the ride, as you\\'re observing this happen, every month it goes opposite. I just goes big swings. Buffett: The ride is not so obvious because it\\'s a rollercoaster ride. You can\\'t see it happening unless you look at your portfolio. In 1979, the Yale class of 1954 had a 25th reunion, and asked an investor to take a risky portfolio investment for Yale and let\\'s give it to Yale on our 50th anniversary, all right? So, they got a portfolio manager, his name was Joe McNay, and they said -- they put together -- it was $375,000. It\\'s like one house, you know, for all the whole class, no big deal. So, McNay decided to invest in Home Depot, Walmart, and internet stocks. On their 50th reunion in 2004, they presented Yale University with $90 million dollars. He started liquidating in 2000, right the peak of the market. So, it must be partly luck.'],\n",
       " ['No one could have known that Walmart was going to be such a success. For every one of the great men and women of history, there\\'s 1,000 of them that got squashed. And I think that history is like that. The people you read about in history are often just phenomenal risk takers like Joe McNey. But maybe they\\'re just lucky, maybe they are just lucky. Apple lost about a third of its value in one month in 2008. The company\\'s founder Steve Jobs had pancreatic cancer in 2004, but the doctors said it\\'s curable, no problem, so the stock didn\\'t do anything. So, it quickly rebounded because he wasn\\'t, and the company\\'s stock went up because he was not cancer-stricken. Bob Greene: Maybe it\\'s all those poor, all those ordinary people, living the little house, the $400,000 house, they don\\'t risk it. Maybe they\\'re the smart ones. Aaron Carroll: I\\'ve just told you about one blip here, but they were so many of these blips on the way, and they all have some story about the success of some Apple product, or people aren\\'t buying some product. Each point represents one of the points that we saw on the market, Carroll says. Carroll: \"It looks totally different, and it shows such complexity that I can\\'t tell a simple narrative. Every month looks different. The best success was in December, January of 2001, where the stock price went up 50% in one month. The reason why it looks kind of compressed on this way is, because the stock market doesn\\'t move as much as Apple. The return for a stock, for the i-th stock, is equal to the market return, which is represented here by the S & P 500, plus idiosyncratic return. The variance of the stock returns is the variance -- the variance of a stock return is the sum of the market. Apple shows a magnified response to the stock market. It goes up and down approximately one and a half times as much as stock market does on any day. Apple has a lot of idiosyncratic risk, but the aggregate economy matters, right?'],\n",
       " [\"If you think that maybe because Apple is kind of a vulnerable company, that if the economy tanks, Apple will tank even more than the economy. If the market goes up, then it's even better news for Apple, even though it's a volatile, dangerous strategy company. He founded Apple and Apple prospered, then had a falling out with the management, and got kind of kicked out of his own company. And then he founded Next Computer. But meanwhile, Apple started to really tank, and they finally realized they needed Steve Jobs, so they brought him back. And it turned out to be the same month that's the Lehman Brothers collapse occurred. This line, I thought it would have an even higher beta, but I think it's this point which is bringing the beta down.\"],\n",
       " [\"A lot of probability theory works on the assumption that variables are normally distributed. But random variables have a habit of not behaving that way, especially in finance it seems. Benoit Mandelbrot was the discoverer of this concept, and I think the most important figure in it. Pierre Paul Levy invented the concept, as discussed in the next lecture in this week's Lecture on the idea of the 'normal' distribution of random shocks to the financial economy. The bell-shaped curve is thought to be a parabola, a mathematical function. In nature the normal distribution is not the only distribution that occurs, and that especially in certain kinds of circumstances we have more fat-tailed distributions. The way you find out that they're not the same, is that in extremely rare circumstances there'll be a sudden major jump in the variable that you might have thought couldn't happen. Whether it's Cauchy or normal, they look about the same; they look pretty much the same. But the pink line has tremendously large probability of being far out. These are the tails of the distribution. Stock market went up 12.53% on October 30, 1929. That's the biggest one-day increase in the history of the stock market. But there were maybe like 20 days, I can't read off the chart when it did this since 1928. You can go through ten years on Wall Street and never see a drop of that magnitude. So, eventually you get kind of assured. It can't happen. What about an 8% drop? Well, I look at this, I say, I've never seen that. It just doesn't happen. The stock market crash of 1929 had two consecutive days. It went down about 12% on October 28, and then the next day it did it again. That's way off the charts, and if you compute the normal distribution, what's the probability of that? If it's a normal distribution and it fits the central portion, it would say it's virtually zero. It couldn't happen. Anyone have any idea what happened on October 30, 1929? It's obvious to me, but it's not obvious to you. If you believe in normality, October 19, 1987 couldn't happen, Bob Greene says.\"],\n",
       " ['He says a student raised his hand and said the stock market is \"totally falling apart\" Greene: The probability of a decline that\\'s that negative? It\\'s 10 to the minus 71 power. 1 over 10 power. That\\'s an awfully small number. But there it is. It happened, Greene says, and in fact, I\\'ve been teaching this course for 25 years. It just came as a complete surprise to me. I went downtown to Merrill Lynch. The two themes are that independence leads to the law of large numbers, and it leads to some sort of stability. But that\\'s not what happened in this crisis and that\\'s the big question. You get big incredible shocks that you thought couldn\\'t happen, and they just come up with a certain low probability.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_slicer( text):\n",
    "        print(len(mcq.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]))\n",
    "        if len(mcq.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]) <= mcq.tokenizer.model_max_length :\n",
    "            return [text]\n",
    "        chunks = []\n",
    "        chunk = []\n",
    "        length = 0\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            _len = len(mcq.tokenizer.tokenize(sentence))\n",
    "            if length + _len <= mcq.tokenizer.model_max_length :\n",
    "                length += _len\n",
    "                chunk.append(sentence)\n",
    "            elif not chunk:\n",
    "                # Can a sentence be applicable for splitting on to chunks?\n",
    "                chunks.append(sentence.strip())\n",
    "                length = 0\n",
    "            else:\n",
    "                chunks.append(' '.join(chunk).strip())\n",
    "                chunk = [sentence]\n",
    "                length = _len\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk).strip())\n",
    "        return chunks\n",
    "sections_chunks = []\n",
    "sections_n = []\n",
    "for i,cur_section in enumerate(sections):\n",
    "    cur_section_chunks = text_slicer(cur_section)\n",
    "    n_chunks = len(cur_section_chunks)\n",
    "    sections_chunks.extend(cur_section_chunks)\n",
    "    if n_chunks==1:\n",
    "        sections_n.append(float(i))\n",
    "    else:\n",
    "        sections_n.extend([i+0.1*k for k in range(n_chunks)])\n",
    "print('='*100)\n",
    "[text_slicer(cur_section) for cur_section in sections_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    719\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 376 at dim 1 (got 244)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(mcq\u001b[39m.\u001b[39;49mtokenizer(sections, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]) \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2519\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2520\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2521\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2601\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2602\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2603\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2604\u001b[0m         )\n\u001b[1;32m   2605\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2607\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2608\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2609\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2610\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2611\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2612\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2613\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2614\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2615\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2616\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2617\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2618\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2619\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2620\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2621\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2622\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2623\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2624\u001b[0m     )\n\u001b[1;32m   2625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2626\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2627\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2628\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2645\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2797\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2788\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2789\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2790\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2795\u001b[0m )\n\u001b[0;32m-> 2797\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2798\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2799\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2800\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2801\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2802\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2803\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2804\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2805\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2806\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2807\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2808\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2809\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2810\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2811\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2812\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2813\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2814\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2815\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:737\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 737\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[1;32m    738\u001b[0m     input_ids,\n\u001b[1;32m    739\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    740\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    741\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    742\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    743\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    744\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    745\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    746\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    747\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    748\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    749\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    751\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    752\u001b[0m )\n\u001b[1;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:817\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    807\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m    809\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    810\u001b[0m     batch_outputs,\n\u001b[1;32m    811\u001b[0m     padding\u001b[39m=\u001b[39mpadding_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    815\u001b[0m )\n\u001b[0;32m--> 817\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             )\n\u001b[0;32m--> 733\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "len(mcq.tokenizer(sections, return_tensors=\"pt\")[\"input_ids\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
