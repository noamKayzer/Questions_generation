{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/miniforge3/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1177: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/ubuntu/miniforge3/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1177: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "pre_loaded = {}\n",
    "\n",
    "from compute_answer import get_answer\n",
    "\n",
    "def model_loader(kind, name, load):\n",
    "    name = kind + \":\" + name\n",
    "    if name not in pre_loaded:\n",
    "      pre_loaded[name] = load()\n",
    "    return pre_loaded[name]\n",
    "    #\n",
    "    \n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "checkpoint = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "\n",
    "model_question_generation = model_loader(\n",
    "  'model',\n",
    "  checkpoint,\n",
    "  lambda : AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    ")\n",
    "tokenizer_question_generation = model_loader(\n",
    "  'tokenizer',\n",
    "  checkpoint,\n",
    "  lambda : AutoTokenizer.from_pretrained(checkpoint)\n",
    ")\n",
    "\n",
    "model_loader(\n",
    "    'nltk',\n",
    "    \"punkt\",\n",
    "    lambda : nltk.download('punkt')\n",
    ")\n",
    "# global variables:\n",
    "# NER_LABELS = ('GPE', 'WORK_OF_ART', 'PERSON', 'NORP', 'EVENT', 'LOC', 'ORG','PRODUCT','LANGUAGE','QUANTITY')\n",
    "NER_LABELS = (\"NP\",)\n",
    "MIN_TOKENS = 10 \n",
    "MAX_TOKENS = 45\n",
    "\n",
    "# ------------------------ MAIN ------------------------ #\n",
    "\n",
    "def generate_mcq(sections, num_questions=10, sparse_mode=True, use_cache=False):\n",
    "    sentences = []\n",
    "    for section in sections.values():\n",
    "        for sentence in section[\"sentences\"]:\n",
    "            if not sentence.get('tokens') or not (MIN_TOKENS <= sentence['tokens'] <= MAX_TOKENS):\n",
    "                continue\n",
    "            sentences.append(sentence)\n",
    "    sentences.sort(key=lambda x:x[\"rank\"], reverse=True)\n",
    "    cache = []\n",
    "    output = []\n",
    "    for sentence in sentences:\n",
    "        _output, cache = generate_mcq_one(\n",
    "            sentence,\n",
    "            cache,\n",
    "            1 if sparse_mode else (num_questions - len(output))\n",
    "        )\n",
    "        output += _output\n",
    "        if num_questions and len(output) >= num_questions:\n",
    "            return {'questions': output}\n",
    "    return {'questions': output}\n",
    "    #\n",
    "\n",
    "def generate_mcq_one(sent, cache, num_questions):\n",
    "    sent['text'] = sent.get(\"resolved\", sent['original'])\n",
    "    _output = []\n",
    "    for phrase, label, rank in sent['ents']:\n",
    "        if rank == 0:\n",
    "            continue\n",
    "        if label not in NER_LABELS:\n",
    "            continue\n",
    "        if phrase in cache:\n",
    "            continue\n",
    "        if in_parentheses_or_brackets(phrase, sent['text']): \n",
    "            continue\n",
    "\n",
    "        # try to generate a question:\n",
    "        question = generate_question(phrase, sent['text'], max_length=64)\n",
    "\n",
    "        # filter bad output:\n",
    "        if not question.endswith('?') or phrase.lower() in question.lower():\n",
    "            continue\n",
    "        # experimental:\n",
    "        if not any(phrase in question for phrase, _, rank in sent['ents'] if rank != 0):\n",
    "            continue          \n",
    "        answer = get_answer(question, sent['text'], nbest=10, null_threshold=-3.76, for_mcq=True)\n",
    "        if not answer:\n",
    "            continue\n",
    "        if answer not in phrase and phrase not in answer:\n",
    "            continue\n",
    "\n",
    "        final_answer = phrase if len(phrase) >= len(answer) else answer\n",
    "        if len(final_answer.replace(\"-\",\" \").split()) < 3:\n",
    "            continue\n",
    "        \n",
    "        # append MCQ:\n",
    "        _output.append({\n",
    "            \"context_missing\": sent['text'].replace(phrase, \"___???___\"),\n",
    "            \"question\": question,\n",
    "            \"answer\": final_answer\n",
    "        })\n",
    "        cache.append(phrase) # optional\n",
    "        num_questions -= 1\n",
    "        if num_questions == 0:\n",
    "            break\n",
    "    return _output,cache\n",
    "    #\n",
    "\n",
    "import re\n",
    "\n",
    "# ------------------------ HELPER ------------------------ #\n",
    "\n",
    "def in_parentheses_or_brackets(span, sentence):\n",
    "    matches_parentheses = re.findall('\\(.*?\\)',sentence)\n",
    "    matches_brackets = re.findall('\\[.*?\\]',sentence)\n",
    "    # print(matches_parentheses, matches_brackets)\n",
    "    for match in matches_parentheses + matches_brackets:\n",
    "        if span in match:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_question(answer, context, max_length=64):\n",
    "    input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "    features = tokenizer_question_generation([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model_question_generation.generate(input_ids=features['input_ids'], \n",
    "                attention_mask=features['attention_mask'],\n",
    "                max_length=max_length)\n",
    "\n",
    "    return tokenizer_question_generation.decode(output[0], skip_special_tokens=True).replace(\"question:\",\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [\" The Internet has become a forum for terrorist groups and individual terrorists to spread their messages of hate and violence. Cyberterrorism is certainly on the terrorists’ agenda and is likely to become their new mode of operation. The various uses of the online platforms by terrorists can be grouped into communicative uses and instrumental uses. Terrorists use the cyberspace for instrumental purposes that include the teaching and training of terrorists online, and establishing ‘virtual training camps” for future assailants online, among other uses. The use of cyberterrorism has become known as cyberterrorism or cyberwarfare. Terrorism has often been conceptualized as a form of psychological warfare. Terrorism has certainly sought to wage such a campaign through the Internet. Terrorists can use the Internet to spread threats intended to distill fear and helplessness. Al-Qaeda has consistently claimed on its websites that the 9/11 destruction of the World Trade Center has inflicted psychological damage, as well as concrete damage, on the U.S. economy. The Internet is particularly well suited to allowing even a small group to amplify its message and exaggerate its importance and threat it poses. One of the primary uses of online communication by terrorists is for the dissemination of propaganda. This generally takes the form of multimedia communications providing ideological, political or religious explanations, justifications, or promotion of terrorist activities. These may include online messages, streaming videos of preaching, social media messages, and even video games developed by terrorist organizations. Terrorist propaganda distributed via the Internet covers a range of objectives and audiences. The fact that many terrorists now have direct control over the content of their message offers further opportunities to shape how they are perceived by different target audiences. Most terrorist online propaganda does not celebrate their violent activities. Terrorist messages emphasize restrictions placed on freedom of expression and plight of comrades who are now political prisoners. These issues resonate powerfully with their own supporters and are also calculated to elicit sympathy from Western audiences. Enemy publics may be targets for these complaints, by emphasizing the antidemocratic nature of the steps taken against them, try to create feelings of unease and shame among their foes.\"]\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "generate_mcq([nlp(sections[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'descriptor',\n",
       " 'descriptor_database',\n",
       " 'descriptor_pool',\n",
       " 'internal',\n",
       " 'message',\n",
       " 'message_factory',\n",
       " 'pyext',\n",
       " 'reflection',\n",
       " 'symbol_database',\n",
       " 'text_encoding']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.protobuf  as fff\n",
    "dir(fff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.21.12'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb5dccc8e3c7e786daf1eda69742c93f8becc3cb00853abb54bb5063887fb73d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
